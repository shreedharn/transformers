
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../transformers_math2/">
      
      
        <link rel="next" href="../history_quick_ref/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Mathematical Quick Reference - Introduction to Transformers</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../styles.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#mathematical-quick-reference-for-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Introduction to Transformers" class="md-header__button md-logo" aria-label="Introduction to Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Introduction to Transformers
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Mathematical Quick Reference
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Introduction to Transformers" class="md-nav__button md-logo" aria-label="Introduction to Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Introduction to Transformers
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../nn_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Network Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../mlp_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Building Networks with MLP
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rnn_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Modeling with RNN
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers Fundamentals
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers Advanced
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../knowledge_store/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Knowledge Store
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyTorch Primer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_math1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_math2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Mathematical Quick Reference
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Mathematical Quick Reference
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#linear-algebra" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Algebra
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Algebra">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Multiplication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-transpose" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Transpose
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-inverse" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Inverse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eigenvalues--eigenvectors" class="md-nav__link">
    <span class="md-ellipsis">
      Eigenvalues &amp; Eigenvectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singular-value-decomposition-svd" class="md-nav__link">
    <span class="md-ellipsis">
      Singular Value Decomposition (SVD)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vectors--geometry" class="md-nav__link">
    <span class="md-ellipsis">
      Vectors &amp; Geometry
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vectors &amp; Geometry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      Dot Product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine Similarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#euclidean-distance" class="md-nav__link">
    <span class="md-ellipsis">
      Euclidean Distance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lp-norms" class="md-nav__link">
    <span class="md-ellipsis">
      Lp Norms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculus" class="md-nav__link">
    <span class="md-ellipsis">
      Calculus
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Calculus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#derivatives" class="md-nav__link">
    <span class="md-ellipsis">
      Derivatives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partial-derivatives" class="md-nav__link">
    <span class="md-ellipsis">
      Partial Derivatives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chain-rule" class="md-nav__link">
    <span class="md-ellipsis">
      Chain Rule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hessian" class="md-nav__link">
    <span class="md-ellipsis">
      Hessian
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jacobian" class="md-nav__link">
    <span class="md-ellipsis">
      Jacobian
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#differential-equations" class="md-nav__link">
    <span class="md-ellipsis">
      Differential Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Differential Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ordinary-differential-equations-odes" class="md-nav__link">
    <span class="md-ellipsis">
      Ordinary Differential Equations (ODEs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partial-differential-equations-pdes" class="md-nav__link">
    <span class="md-ellipsis">
      Partial Differential Equations (PDEs)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nonlinear-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Nonlinear Functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Nonlinear Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperbolic-tangent-tanh" class="md-nav__link">
    <span class="md-ellipsis">
      Hyperbolic Tangent (tanh)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      Sigmoid
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu" class="md-nav__link">
    <span class="md-ellipsis">
      ReLU
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probability--statistics" class="md-nav__link">
    <span class="md-ellipsis">
      Probability &amp; Statistics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Probability &amp; Statistics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expectation" class="md-nav__link">
    <span class="md-ellipsis">
      Expectation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance" class="md-nav__link">
    <span class="md-ellipsis">
      Variance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Entropy Loss
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Adam Optimizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Mechanisms
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention Mechanisms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scaled-dot-product-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Scaled Dot-Product Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-components" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer Components
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Layer Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-connections" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Connections
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Concepts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Concepts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#temperature-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Temperature Scaling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../history_quick_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequence Modeling History
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    README
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LICENSE
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#linear-algebra" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Algebra
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Algebra">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#matrix-multiplication" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Multiplication
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-transpose" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Transpose
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-inverse" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Inverse
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#eigenvalues--eigenvectors" class="md-nav__link">
    <span class="md-ellipsis">
      Eigenvalues &amp; Eigenvectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singular-value-decomposition-svd" class="md-nav__link">
    <span class="md-ellipsis">
      Singular Value Decomposition (SVD)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vectors--geometry" class="md-nav__link">
    <span class="md-ellipsis">
      Vectors &amp; Geometry
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vectors &amp; Geometry">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dot-product" class="md-nav__link">
    <span class="md-ellipsis">
      Dot Product
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine Similarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#euclidean-distance" class="md-nav__link">
    <span class="md-ellipsis">
      Euclidean Distance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lp-norms" class="md-nav__link">
    <span class="md-ellipsis">
      Lp Norms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#calculus" class="md-nav__link">
    <span class="md-ellipsis">
      Calculus
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Calculus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#derivatives" class="md-nav__link">
    <span class="md-ellipsis">
      Derivatives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partial-derivatives" class="md-nav__link">
    <span class="md-ellipsis">
      Partial Derivatives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#chain-rule" class="md-nav__link">
    <span class="md-ellipsis">
      Chain Rule
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hessian" class="md-nav__link">
    <span class="md-ellipsis">
      Hessian
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jacobian" class="md-nav__link">
    <span class="md-ellipsis">
      Jacobian
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#differential-equations" class="md-nav__link">
    <span class="md-ellipsis">
      Differential Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Differential Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ordinary-differential-equations-odes" class="md-nav__link">
    <span class="md-ellipsis">
      Ordinary Differential Equations (ODEs)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partial-differential-equations-pdes" class="md-nav__link">
    <span class="md-ellipsis">
      Partial Differential Equations (PDEs)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nonlinear-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Nonlinear Functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Nonlinear Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperbolic-tangent-tanh" class="md-nav__link">
    <span class="md-ellipsis">
      Hyperbolic Tangent (tanh)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sigmoid" class="md-nav__link">
    <span class="md-ellipsis">
      Sigmoid
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#relu" class="md-nav__link">
    <span class="md-ellipsis">
      ReLU
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#probability--statistics" class="md-nav__link">
    <span class="md-ellipsis">
      Probability &amp; Statistics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Probability &amp; Statistics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#expectation" class="md-nav__link">
    <span class="md-ellipsis">
      Expectation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variance" class="md-nav__link">
    <span class="md-ellipsis">
      Variance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-entropy-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Entropy Loss
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam-optimizer" class="md-nav__link">
    <span class="md-ellipsis">
      Adam Optimizer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-mechanisms" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Mechanisms
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention Mechanisms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scaled-dot-product-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Scaled Dot-Product Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-components" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer Components
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer Components">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Layer Normalization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-connections" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Connections
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Concepts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Concepts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#temperature-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Temperature Scaling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="mathematical-quick-reference-for-neural-networks">Mathematical Quick Reference for Neural Networks<a class="headerlink" href="#mathematical-quick-reference-for-neural-networks" title="Permanent link">&para;</a></h1>
<p>A comprehensive reference of core mathematical concepts used in neural networks and deep learning, with detailed intuitions and practical PyTorch examples.</p>
<h2 id="linear-algebra">Linear Algebra<a class="headerlink" href="#linear-algebra" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/linear_algebra.ipynb">pynb/math_ref/linear_algebra.ipynb</a></p>
</blockquote>
<h3 id="matrix-multiplication">Matrix Multiplication<a class="headerlink" href="#matrix-multiplication" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{C} &amp;= \mathbf{A}\mathbf{B} \newline
C_{ij} &amp;= \sum_k A_{ik}B_{kj}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> The fundamental operation of neural networks. Each row of the weight matrix represents neuron weights, each column of the input matrix represents input vectors. The result computes weighted sums for all neurons simultaneously, enabling parallel computation. This is why a single matrix multiplication can represent an entire layer's forward pass.</p>
<p><strong>How it affects output:</strong> Matrix multiplication transforms input vectors from one feature space to another. Small changes in weights create proportional changes in outputs, making gradient-based learning possible.</p>
<h3 id="matrix-transpose">Matrix Transpose<a class="headerlink" href="#matrix-transpose" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
(\mathbf{A})^T_{ij} = \mathbf{A}_{ji}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Essential for backpropagation. When gradients flow backward through a layer with weights, we need the transpose to properly route the gradient signals back to the previous layer. The transpose "reverses" the forward direction of information flow.</p>
<p><strong>How it affects output:</strong> Transpose changes how information flows. In forward pass, weights map inputs to outputs. In backward pass, transpose maps output gradients back to input gradients.</p>
<h3 id="matrix-inverse">Matrix Inverse<a class="headerlink" href="#matrix-inverse" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Used in analytical solutions for least squares (normal equations) and understanding linear transformations. In neural networks, helps analyze layer transformations and appears in second-order optimization methods like Newton's method.</p>
<p><strong>How it affects output:</strong> Matrix inverse provides exact solutions when they exist. However, neural networks rarely use inverses directly due to computational cost and numerical instability.</p>
<h3 id="eigenvalues--eigenvectors">Eigenvalues &amp; Eigenvectors<a class="headerlink" href="#eigenvalues--eigenvectors" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{A}\mathbf{v} = \lambda\mathbf{v}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Reveals principal directions of data variation (PCA), helps analyze gradient flow and conditioning of weight matrices. Large eigenvalues can indicate exploding gradients, while small ones suggest vanishing gradients. Critical for understanding optimization landscapes.</p>
<p><strong>How it affects output:</strong> Eigenvalues indicate how much each direction gets amplified. Large eigenvalues can cause exploding gradients, small ones cause vanishing gradients.</p>
<h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)<a class="headerlink" href="#singular-value-decomposition-svd" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Decomposes any matrix into orthogonal transformations and scaling. Used in dimensionality reduction, weight initialization, and analyzing the effective rank of learned representations. Helps understand what transformations neural network layers are actually learning.</p>
<p><strong>How it affects output:</strong> SVD reveals the intrinsic dimensionality and structure of transformations. It's used for initialization to maintain gradient flow and for analyzing what neural networks learn.</p>
<hr />
<h2 id="vectors--geometry">Vectors &amp; Geometry<a class="headerlink" href="#vectors--geometry" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/vectors_geometry.ipynb">pynb/math_ref/vectors_geometry.ipynb</a></p>
</blockquote>
<h3 id="dot-product">Dot Product<a class="headerlink" href="#dot-product" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{a} \cdot \mathbf{b} = \sum_i a_i b_i = |\mathbf{a}||\mathbf{b}|\cos(\theta)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures how "aligned" two vectors are. In neurons, the dot product between input vectors and weight vectors gives the raw activation strengthâ€”high when input pattern matches what the neuron is looking for. This is the core operation that determines neuron firing.</p>
<p><strong>How it affects output:</strong> High dot product means input and weight vectors point in similar directions, creating strong positive activation. Orthogonal vectors produce zero activation.</p>
<h3 id="cosine-similarity">Cosine Similarity<a class="headerlink" href="#cosine-similarity" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\cos(\theta) = \frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}||\mathbf{b}|}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures similarity independent of magnitude. Used in attention mechanisms, word embeddings, and similarity-based learning. Helps neural networks focus on directional patterns rather than absolute magnitudes, making them more robust to scaling variations.</p>
<p><strong>How it affects output:</strong> Cosine similarity ranges from -1 to 1, making it scale-invariant. Similar directions get high scores regardless of vector magnitude.</p>
<h3 id="euclidean-distance">Euclidean Distance<a class="headerlink" href="#euclidean-distance" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
d(\mathbf{a}, \mathbf{b}) = |\mathbf{a} - \mathbf{b}|_2 = \sqrt{\sum_i (a_i - b_i)^2}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures how "far apart" two points are in feature space. Used in loss functions (MSE), clustering, and nearest neighbor methods. In neural networks, helps measure prediction errors and similarity between representations.</p>
<p><strong>How it affects output:</strong> Smaller distances indicate more similar points. MSE loss penalizes large errors quadratically, making the model focus on reducing big mistakes first.</p>
<h3 id="lp-norms">Lp Norms<a class="headerlink" href="#lp-norms" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
|\mathbf{x}|_p = \left(\sum_i |x_i|^p\right)^{1/p}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures vector "size" in different ways. L1 norm promotes sparsity (many weights become zero), L2 norm promotes smoothness (weights stay small). Used in regularization to control model complexity and prevent overfitting by penalizing large weights.</p>
<p><strong>How it affects output:</strong> L1 norm creates sparse solutions (many zeros), L2 norm creates smooth solutions (small values). Higher p values focus more on the largest elements.</p>
<hr />
<h2 id="calculus">Calculus<a class="headerlink" href="#calculus" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/calculus.ipynb">pynb/math_ref/calculus.ipynb</a></p>
</blockquote>
<h3 id="derivatives">Derivatives<a class="headerlink" href="#derivatives" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures how fast a function changes. In neural networks, tells us how much the loss changes when we tweak a parameter. This is the foundation of gradient-based learningâ€”we follow the derivative to find better parameter values.</p>
<p><strong>How it affects output:</strong> Derivatives tell us sensitivity - how much output changes for small input changes. Essential for parameter updates.</p>
<h3 id="partial-derivatives">Partial Derivatives<a class="headerlink" href="#partial-derivatives" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial x_i}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Derivative with respect to one variable while holding others constant. Neural networks have millions of parameters, so we need partial derivatives to see how the loss changes with respect to each individual weight or bias.</p>
<p><strong>How it affects output:</strong> Each parameter gets its own gradient, allowing independent optimization of millions of parameters simultaneously.</p>
<h3 id="chain-rule">Chain Rule<a class="headerlink" href="#chain-rule" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\frac{d}{dx}f(g(x)) = f'(g(x)) \cdot g'(x)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> The mathematical foundation of backpropagation. Neural networks are compositions of functions (layer after layer), so to compute gradients we multiply derivatives along the chain from output back to input. This is why it's called "backpropagation"â€”propagating derivatives backward through the composition.</p>
<p><strong>How it affects output:</strong> Chain rule enables automatic differentiation through arbitrarily deep networks by systematically applying the composition rule.</p>
<h3 id="gradient">Gradient<a class="headerlink" href="#gradient" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\nabla f = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right]
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Points in the direction of steepest increase. In neural network training, we move parameters in the negative gradient direction (steepest decrease) to minimize the loss function. The gradient tells us both direction and magnitude of the best parameter update.</p>
<p><strong>How it affects output:</strong> Gradient provides both direction (sign) and magnitude for optimal parameter updates. Larger gradients indicate more sensitive parameters.</p>
<h3 id="hessian">Hessian<a class="headerlink" href="#hessian" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{H}_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Matrix of second derivatives that describes the curvature of the loss surface. Helps understand convergence behavior and is used in advanced optimization methods like Newton's method and natural gradients. High curvature areas require smaller learning rates.</p>
<p><strong>How it affects output:</strong> Hessian reveals optimization landscape curvature. High condition numbers indicate difficult optimization requiring careful learning rates.</p>
<h3 id="jacobian">Jacobian<a class="headerlink" href="#jacobian" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{J}_{ij} = \frac{\partial f_i}{\partial x_j}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Matrix of first derivatives for vector-valued functions. Essential for backpropagation through layers that output vectors (like hidden layers). Each element shows how one output component changes with respect to one input component, enabling gradient flow through complex architectures.</p>
<p><strong>How it affects output:</strong> Jacobian enables gradient flow through vector outputs, crucial for multi-output layers and complex architectures.</p>
<hr />
<h2 id="differential-equations">Differential Equations<a class="headerlink" href="#differential-equations" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/differential_equations.ipynb">pynb/math_ref/differential_equations.ipynb</a></p>
</blockquote>
<h3 id="ordinary-differential-equations-odes">Ordinary Differential Equations (ODEs)<a class="headerlink" href="#ordinary-differential-equations-odes" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\frac{dy}{dt} = f(y, t)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Models how quantities change over time. Neural ODEs treat layer depth as continuous time, allowing adaptive depth and memory-efficient training. ResNets approximate the solution to ODEs, explaining why skip connections work so well for deep networks.</p>
<p><strong>How it affects output:</strong> ODEs provide continuous depth, memory efficiency, and theoretical foundations for understanding deep networks as dynamical systems.</p>
<h3 id="partial-differential-equations-pdes">Partial Differential Equations (PDEs)<a class="headerlink" href="#partial-differential-equations-pdes" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\frac{\partial u}{\partial t} = f\left(u, \frac{\partial u}{\partial x}, \frac{\partial^2 u}{\partial x^2}, \ldots\right)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Models complex spatiotemporal phenomena. Physics-informed neural networks (PINNs) embed PDE constraints directly into the loss function, allowing neural networks to solve scientific computing problems while respecting physical laws.</p>
<p><strong>How it affects output:</strong> PINNs ensure solutions satisfy physical laws, enabling neural networks to solve scientific problems with built-in physical constraints.</p>
<hr />
<h2 id="nonlinear-functions">Nonlinear Functions<a class="headerlink" href="#nonlinear-functions" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/nonlinear_functions.ipynb">pynb/math_ref/nonlinear_functions.ipynb</a></p>
</blockquote>
<h3 id="hyperbolic-tangent-tanh">Hyperbolic Tangent (tanh)<a class="headerlink" href="#hyperbolic-tangent-tanh" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Activation function that squashes inputs to (-1, 1). Provides nonlinearity needed for complex patterns while keeping outputs bounded. The S-shape introduces smooth nonlinear decision boundaries, and its zero-centered output helps with gradient flow compared to sigmoid.</p>
<p><strong>How it affects output:</strong> Tanh produces zero-centered outputs helping gradient flow, but can suffer from vanishing gradients when inputs are large.</p>
<h3 id="sigmoid">Sigmoid<a class="headerlink" href="#sigmoid" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\sigma(x) = \frac{1}{1 + e^{-x}}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Squashes inputs to (0, 1), naturally interpreted as probabilities. Used in binary classification and gating mechanisms (LSTM gates). However, suffers from vanishing gradients at extremes, which is why ReLU became more popular in deep networks.</p>
<p><strong>How it affects output:</strong> Sigmoid outputs natural probabilities but suffers from vanishing gradients, making it problematic for deep networks but perfect for gates.</p>
<h3 id="relu">ReLU<a class="headerlink" href="#relu" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\text{ReLU}(x) = \max(0, x)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Simple nonlinearity that sets negative values to zero. Solves vanishing gradient problem because gradient is either 0 or 1. Promotes sparsity (many neurons inactive) which makes networks more interpretable and efficient. Biologically inspired by neuron firing thresholds.</p>
<p><strong>How it affects output:</strong> ReLU enables deep networks by preventing vanishing gradients and promoting sparsity, but can suffer from dying neurons.</p>
<hr />
<h2 id="probability--statistics">Probability &amp; Statistics<a class="headerlink" href="#probability--statistics" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/probability_statistics.ipynb">pynb/math_ref/probability_statistics.ipynb</a></p>
</blockquote>
<h3 id="expectation">Expectation<a class="headerlink" href="#expectation" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbb{E}[X] = \sum_x x \cdot P(X = x)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Average value of a random variable. In neural networks, we often work with expected loss over data distributions. Batch statistics, dropout, and stochastic optimization all rely on expectation to handle randomness in training and make models robust to unseen data.</p>
<p><strong>How it affects output:</strong> Expectation provides theoretical foundation for loss functions, batch statistics, and stochastic optimization in neural networks.</p>
<h3 id="variance">Variance<a class="headerlink" href="#variance" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures spread of a distribution. Critical for weight initialization (to prevent vanishing/exploding gradients) and batch normalization (to stabilize training). Understanding variance helps design networks that maintain good signal propagation through many layers.</p>
<p><strong>How it affects output:</strong> Proper variance control through initialization and normalization prevents vanishing/exploding gradients and stabilizes training.</p>
<h3 id="softmax">Softmax<a class="headerlink" href="#softmax" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Converts real values to probability distribution. Essential for multi-class classification and attention mechanisms. The exponential amplifies differences while ensuring outputs sum to 1, creating a "soft" version of selecting the maximum value that's differentiable for gradient-based learning.</p>
<p><strong>How it affects output:</strong> Softmax creates valid probability distributions and enables differentiable "argmax" operations for classification and attention.</p>
<h3 id="cross-entropy-loss">Cross-Entropy Loss<a class="headerlink" href="#cross-entropy-loss" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathcal{L} = -\sum_i y_i \log(\hat{y}_i)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Measures difference between predicted and true probability distributions. Natural loss function for classification because it heavily penalizes confident wrong predictions. Mathematically connected to maximum likelihood estimation and information theoryâ€”minimizing cross-entropy maximizes the likelihood of correct predictions.</p>
<p><strong>How it affects output:</strong> Cross-entropy encourages confident correct predictions while heavily penalizing confident mistakes, leading to well-calibrated classifiers.</p>
<hr />
<h2 id="optimization">Optimization<a class="headerlink" href="#optimization" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/optimization.ipynb">pynb/math_ref/optimization.ipynb</a></p>
</blockquote>
<h3 id="gradient-descent">Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\theta_{t+1} = \theta_t - \alpha \nabla_\theta \mathcal{L}(\theta_t)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Iteratively moves parameters in direction of steepest loss decrease. The fundamental learning algorithm for neural networks. The learning rate controls step sizeâ€”too large causes instability, too small causes slow convergence. Modern variants (Adam, RMSprop) adapt the learning rate automatically.</p>
<p><strong>How it affects output:</strong> Gradient descent provides the fundamental mechanism for learning by iteratively improving parameters based on loss gradients.</p>
<h3 id="adam-optimizer">Adam Optimizer<a class="headerlink" href="#adam-optimizer" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
m_t &amp;= \beta_1 m_{t-1} + (1-\beta_1)g_t \newline
v_t &amp;= \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \newline
\theta_t &amp;= \theta_{t-1} - \frac{\alpha}{\sqrt{v_t} + \epsilon}\hat{m}_t
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Adaptive learning rate optimizer that maintains running averages of gradients (momentum) and squared gradients (variance). Automatically adjusts learning rates per parameter based on historical gradients. Like cruise control for optimization - speeds up in flat areas, slows down in steep areas.</p>
<p><strong>How it affects output:</strong> Adam adapts learning rates per parameter, leading to faster convergence and better handling of sparse gradients compared to SGD.</p>
<hr />
<h2 id="attention-mechanisms">Attention Mechanisms<a class="headerlink" href="#attention-mechanisms" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/attention_mechanisms.ipynb">pynb/math_ref/attention_mechanisms.ipynb</a></p>
</blockquote>
<h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention<a class="headerlink" href="#scaled-dot-product-attention" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> The core operation of transformers. Queries (Q) search through keys (K) to find relevant information, then retrieve corresponding values (V). The scaling prevents attention from becoming too sharp/peaked as dimensions grow, maintaining good gradient flow and distributed attention weights.</p>
<p><strong>How it affects output:</strong> Attention allows models to dynamically focus on relevant parts of the input, enabling long-range dependencies and context-aware representations.</p>
<hr />
<h2 id="transformer-components">Transformer Components<a class="headerlink" href="#transformer-components" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/transformer_components.ipynb">pynb/math_ref/transformer_components.ipynb</a></p>
</blockquote>
<h3 id="layer-normalization">Layer Normalization<a class="headerlink" href="#layer-normalization" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma} \odot \gamma + \beta \quad \text{where } \mu, \sigma \text{ computed per sample}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Normalizes activations within each sample to have zero mean and unit variance. Unlike batch normalization, works independently for each sample, making it stable for variable sequence lengths and small batch sizes. Helps with gradient flow and training stability.</p>
<p><strong>How it affects output:</strong> Layer normalization stabilizes training by normalizing layer inputs, reducing internal covariate shift and enabling higher learning rates.</p>
<h3 id="residual-connections">Residual Connections<a class="headerlink" href="#residual-connections" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\mathbf{h}_{l+1} = \mathbf{h}_l + F(\mathbf{h}_l)
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Creates "gradient highways" that allow information to flow directly through the network. Essential for training very deep networks (like transformers with many layers) by preventing vanishing gradients. Acts like a safety net - even if some layers learn poorly, information can still reach the output.</p>
<p><strong>How it affects output:</strong> Residual connections enable training of much deeper networks by providing gradient highways and allowing layers to learn incremental changes.</p>
<hr />
<h2 id="advanced-concepts">Advanced Concepts<a class="headerlink" href="#advanced-concepts" title="Permanent link">&para;</a></h2>
<blockquote>
<p>ðŸ““ <strong>Jupyter Notebook:</strong> <a href="pynb/math_ref/advanced_concepts.ipynb">pynb/math_ref/advanced_concepts.ipynb</a></p>
</blockquote>
<h3 id="temperature-scaling">Temperature Scaling<a class="headerlink" href="#temperature-scaling" title="Permanent link">&para;</a></h3>
<p>$$
\begin{aligned}
\text{softmax}(z/\tau) \quad \text{where } \tau \text{ is temperature}
\end{aligned}
$$</p>
<p><strong>Intuition:</strong> Controls the "sharpness" of probability distributions. Lower temperature makes the distribution more peaked (confident), higher temperature makes it more uniform (uncertain). Used in generation for creativity control and in calibration to match predicted confidence with actual accuracy.</p>
<p><strong>How it affects output:</strong> Temperature scaling controls the confidence/uncertainty trade-off in model outputs, enabling calibrated predictions and controllable generation creativity.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.top", "navigation.instant", "content.code.copy", "content.tabs.link", "toc.follow"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
    
  </body>
</html>