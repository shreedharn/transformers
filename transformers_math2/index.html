
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../transformers_math1/">
      
      
        <link rel="next" href="../math_quick_ref/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Mathematical Foundations 2 - Introduction to Transformers</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../styles.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-mathematics-of-transformers-from-first-principles-to-practice" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Introduction to Transformers" class="md-header__button md-logo" aria-label="Introduction to Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Introduction to Transformers
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Mathematical Foundations 2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Introduction to Transformers" class="md-nav__button md-logo" aria-label="Introduction to Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Introduction to Transformers
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../nn_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Network Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../mlp_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Building Networks with MLP
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rnn_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Modeling with RNN
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers Fundamentals
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers Advanced
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../knowledge_store/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Knowledge Store
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyTorch Primer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_math1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 1
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 2
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 2
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#part-2-advanced-concepts-and-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Part 2: Advanced Concepts and Scaling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-practical-numerics--implementation-notes" class="md-nav__link">
    <span class="md-ellipsis">
      8. Practical Numerics &amp; Implementation Notes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Practical Numerics &amp; Implementation Notes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-initialization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Initialization Strategies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Mixed Precision Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Gradient Clipping
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-optimization-for-deep-networks" class="md-nav__link">
    <span class="md-ellipsis">
      9. Optimization for Deep Networks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Optimization for Deep Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-from-sgd-to-adam" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 From SGD to Adam
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-advanced-optimizers" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 Advanced Optimizers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-learning-rate-schedules" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 Learning Rate Schedules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      9.4 Gradient Clipping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#95-numerical-stability" class="md-nav__link">
    <span class="md-ellipsis">
      9.5 Numerical Stability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-efficient-attention--scaling" class="md-nav__link">
    <span class="md-ellipsis">
      10. Efficient Attention &amp; Scaling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10. Efficient Attention &amp; Scaling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-complexity-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 Complexity Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-flashattention-memory-efficient-attention" class="md-nav__link">
    <span class="md-ellipsis">
      10.2 FlashAttention: Memory-Efficient Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#103-multi-query-and-grouped-query-attention" class="md-nav__link">
    <span class="md-ellipsis">
      10.3 Multi-Query and Grouped-Query Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#104-kv-caching-for-autoregressive-generation" class="md-nav__link">
    <span class="md-ellipsis">
      10.4 KV Caching for Autoregressive Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#105-linear-attention-approximations" class="md-nav__link">
    <span class="md-ellipsis">
      10.5 Linear Attention Approximations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-regularization-generalization-and-calibration" class="md-nav__link">
    <span class="md-ellipsis">
      11. Regularization, Generalization, and Calibration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. Regularization, Generalization, and Calibration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-dropout-in-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 Dropout in Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-evaluation-and-calibration" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 Evaluation and Calibration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-advanced-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      11.3 Advanced Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#114-label-smoothing" class="md-nav__link">
    <span class="md-ellipsis">
      11.4 Label Smoothing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-common-pitfalls--misconceptions" class="md-nav__link">
    <span class="md-ellipsis">
      14. Common Pitfalls &amp; Misconceptions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="14. Common Pitfalls &amp; Misconceptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#141-high-dimensional-distance-misconceptions" class="md-nav__link">
    <span class="md-ellipsis">
      14.1 High-Dimensional Distance Misconceptions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#142-attention-scaling-mistakes" class="md-nav__link">
    <span class="md-ellipsis">
      14.2 Attention Scaling Mistakes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#143-layernorm-placement" class="md-nav__link">
    <span class="md-ellipsis">
      14.3 LayerNorm Placement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#144-softmax-temperature-misuse" class="md-nav__link">
    <span class="md-ellipsis">
      14.4 Softmax Temperature Misuse
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-summary--what-to-learn-next" class="md-nav__link">
    <span class="md-ellipsis">
      15. Summary &amp; What to Learn Next
    </span>
  </a>
  
    <nav class="md-nav" aria-label="15. Summary &amp; What to Learn Next">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#151-key-mathematical-insights" class="md-nav__link">
    <span class="md-ellipsis">
      15.1 Key Mathematical Insights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#152-advanced-techniques-covered" class="md-nav__link">
    <span class="md-ellipsis">
      15.2 Advanced Techniques Covered
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#153-next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      15.3 Next Steps
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#connection-to-part-1" class="md-nav__link">
    <span class="md-ellipsis">
      Connection to Part 1
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-a-symbolshape-reference" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix A: Symbol/Shape Reference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Appendix A: Symbol/Shape Reference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-head-attention-shapes" class="md-nav__link">
    <span class="md-ellipsis">
      Single-Head Attention Shapes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head--batched-shapes" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Head &amp; Batched Shapes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-b-key-derivations" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix B: Key Derivations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Appendix B: Key Derivations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#b1-softmax-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      B.1 Softmax Gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b2-matrix-calculus-identities" class="md-nav__link">
    <span class="md-ellipsis">
      B.2 Matrix Calculus Identities
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../math_quick_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematical Quick Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../history_quick_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequence Modeling History
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    README
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LICENSE
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#part-2-advanced-concepts-and-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Part 2: Advanced Concepts and Scaling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-practical-numerics--implementation-notes" class="md-nav__link">
    <span class="md-ellipsis">
      8. Practical Numerics &amp; Implementation Notes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Practical Numerics &amp; Implementation Notes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-initialization-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Initialization Strategies
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Mixed Precision Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      8.3 Gradient Clipping
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-optimization-for-deep-networks" class="md-nav__link">
    <span class="md-ellipsis">
      9. Optimization for Deep Networks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Optimization for Deep Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-from-sgd-to-adam" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 From SGD to Adam
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-advanced-optimizers" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 Advanced Optimizers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-learning-rate-schedules" class="md-nav__link">
    <span class="md-ellipsis">
      9.3 Learning Rate Schedules
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#94-gradient-clipping" class="md-nav__link">
    <span class="md-ellipsis">
      9.4 Gradient Clipping
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#95-numerical-stability" class="md-nav__link">
    <span class="md-ellipsis">
      9.5 Numerical Stability
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-efficient-attention--scaling" class="md-nav__link">
    <span class="md-ellipsis">
      10. Efficient Attention &amp; Scaling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10. Efficient Attention &amp; Scaling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-complexity-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      10.1 Complexity Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-flashattention-memory-efficient-attention" class="md-nav__link">
    <span class="md-ellipsis">
      10.2 FlashAttention: Memory-Efficient Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#103-multi-query-and-grouped-query-attention" class="md-nav__link">
    <span class="md-ellipsis">
      10.3 Multi-Query and Grouped-Query Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#104-kv-caching-for-autoregressive-generation" class="md-nav__link">
    <span class="md-ellipsis">
      10.4 KV Caching for Autoregressive Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#105-linear-attention-approximations" class="md-nav__link">
    <span class="md-ellipsis">
      10.5 Linear Attention Approximations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-regularization-generalization-and-calibration" class="md-nav__link">
    <span class="md-ellipsis">
      11. Regularization, Generalization, and Calibration
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. Regularization, Generalization, and Calibration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-dropout-in-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      11.1 Dropout in Transformers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-evaluation-and-calibration" class="md-nav__link">
    <span class="md-ellipsis">
      11.2 Evaluation and Calibration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-advanced-tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      11.3 Advanced Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#114-label-smoothing" class="md-nav__link">
    <span class="md-ellipsis">
      11.4 Label Smoothing
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-common-pitfalls--misconceptions" class="md-nav__link">
    <span class="md-ellipsis">
      14. Common Pitfalls &amp; Misconceptions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="14. Common Pitfalls &amp; Misconceptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#141-high-dimensional-distance-misconceptions" class="md-nav__link">
    <span class="md-ellipsis">
      14.1 High-Dimensional Distance Misconceptions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#142-attention-scaling-mistakes" class="md-nav__link">
    <span class="md-ellipsis">
      14.2 Attention Scaling Mistakes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#143-layernorm-placement" class="md-nav__link">
    <span class="md-ellipsis">
      14.3 LayerNorm Placement
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#144-softmax-temperature-misuse" class="md-nav__link">
    <span class="md-ellipsis">
      14.4 Softmax Temperature Misuse
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-summary--what-to-learn-next" class="md-nav__link">
    <span class="md-ellipsis">
      15. Summary &amp; What to Learn Next
    </span>
  </a>
  
    <nav class="md-nav" aria-label="15. Summary &amp; What to Learn Next">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#151-key-mathematical-insights" class="md-nav__link">
    <span class="md-ellipsis">
      15.1 Key Mathematical Insights
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#152-advanced-techniques-covered" class="md-nav__link">
    <span class="md-ellipsis">
      15.2 Advanced Techniques Covered
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#153-next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      15.3 Next Steps
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#connection-to-part-1" class="md-nav__link">
    <span class="md-ellipsis">
      Connection to Part 1
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    <span class="md-ellipsis">
      Further Reading
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-a-symbolshape-reference" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix A: Symbol/Shape Reference
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Appendix A: Symbol/Shape Reference">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#single-head-attention-shapes" class="md-nav__link">
    <span class="md-ellipsis">
      Single-Head Attention Shapes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head--batched-shapes" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Head &amp; Batched Shapes
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-b-key-derivations" class="md-nav__link">
    <span class="md-ellipsis">
      Appendix B: Key Derivations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Appendix B: Key Derivations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#b1-softmax-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      B.1 Softmax Gradient
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b2-matrix-calculus-identities" class="md-nav__link">
    <span class="md-ellipsis">
      B.2 Matrix Calculus Identities
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="the-mathematics-of-transformers-from-first-principles-to-practice">The Mathematics of Transformers: From First Principles to Practice<a class="headerlink" href="#the-mathematics-of-transformers-from-first-principles-to-practice" title="Permanent link">&para;</a></h1>
<h2 id="part-2-advanced-concepts-and-scaling">Part 2: Advanced Concepts and Scaling<a class="headerlink" href="#part-2-advanced-concepts-and-scaling" title="Permanent link">&para;</a></h2>
<h2 id="overview">Overview<a class="headerlink" href="#overview" title="Permanent link">&para;</a></h2>
<p>This second part builds upon the foundational concepts from <a href="../transformers_math1/">Part 1: Building Intuition and Core Concepts</a> to cover advanced topics essential for implementing and scaling Transformer models in practice. Here we focus on optimization techniques, training stability, efficient attention implementations, and the mathematical considerations needed for real-world large models.</p>
<p><strong>Prerequisites:</strong> We assume you've completed Part 1, which covers mathematical preliminaries, basic neural networks, attention mechanisms, multi-head attention, and Transformer blocks. If you haven't read Part 1 yet, please start there for the foundational understanding.</p>
<p><strong>What You'll Learn:</strong>
- Advanced optimization algorithms (SGD momentum, Adam, AdamW) and their mathematical foundations
- Learning rate schedules and gradient clipping techniques
- Efficient attention implementations for scaling to long sequences
- Regularization and calibration techniques for better generalization
- Common pitfalls and how to avoid them
- Implementation best practices for numerical stability</p>
<p><strong>Appendices:</strong>
- <a href="#appendix-a-symbolshape-reference">A. Symbol/Shape Reference</a>
- <a href="#appendix-b-key-derivations">B. Key Derivations</a></p>
<p><strong>Additional Resources:</strong>
- <a href="../transformers_math1/">Part 1: Building Intuition and Core Concepts</a>
- <a href="../glossary/">Glossary</a> - Comprehensive terms and definitions</p>
<h2 id="8-practical-numerics--implementation-notes">8. Practical Numerics &amp; Implementation Notes<a class="headerlink" href="#8-practical-numerics--implementation-notes" title="Permanent link">&para;</a></h2>
<h3 id="81-initialization-strategies">8.1 Initialization Strategies<a class="headerlink" href="#81-initialization-strategies" title="Permanent link">&para;</a></h3>
<p><strong>Xavier/Glorot for Linear Layers:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right) \quad (49)
</code></pre></div></p>
<p><strong>Attention-Specific:</strong> Initialize query/key projections with smaller variance to prevent attention collapse (overly peaked attention distributions).</p>
<h3 id="82-mixed-precision-training">8.2 Mixed Precision Training<a class="headerlink" href="#82-mixed-precision-training" title="Permanent link">&para;</a></h3>
<p><strong>FP16 Forward, FP32 Gradients:</strong> Use half precision for speed, full precision for numerical stability:
ðŸ’» <strong>Implementation Example</strong>: For Automatic Mixed Precision implementation, see <a href="./pynb/math_ref/optimization.ipynb">Optimization Notebook</a></p>
<h3 id="83-gradient-clipping">8.3 Gradient Clipping<a class="headerlink" href="#83-gradient-clipping" title="Permanent link">&para;</a></h3>
<p><strong>Global Norm Clipping:</strong> As detailed in equation (11), we clip gradients to prevent explosive updates.</p>
<h2 id="9-optimization-for-deep-networks">9. Optimization for Deep Networks<a class="headerlink" href="#9-optimization-for-deep-networks" title="Permanent link">&para;</a></h2>
<h3 id="91-from-sgd-to-adam">9.1 From SGD to Adam<a class="headerlink" href="#91-from-sgd-to-adam" title="Permanent link">&para;</a></h3>
<p>ðŸ“š <strong>Quick Reference</strong>: See <a href="../math_quick_ref/#mathematical-quick-reference-for-neural-networks">Adam Optimizer</a> and <a href="../math_quick_ref/#mathematical-quick-reference-for-neural-networks">Gradient Descent</a> in the mathematical reference table.</p>
<p><strong>SGD with Momentum:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>\begin{align}
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>\mathbf{v}_t &amp;= \beta \mathbf{v}_{t-1} + (1-\beta) \nabla_\theta \mathcal{L} \quad (5)\\
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>\theta_t &amp;= \theta_{t-1} - \eta \mathbf{v}_t \quad (6)
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>\end{align}
</code></pre></div></p>
<p><strong>What momentum does:</strong> Like a ball rolling down a hill. Instead of just following the current slope (gradient), momentum keeps some memory of where you were going before. This helps you:
- <strong>Roll through small bumps</strong> (escape local minima)
- <strong>Speed up in consistent directions</strong> (valleys)<br />
- <strong>Slow down when direction changes</strong> (near the bottom)</p>
<p><strong>Bowling ball analogy:</strong> A heavy bowling ball doesn't stop immediately when it hits a small bump - it uses its momentum to keep rolling toward the pins (optimal solution).</p>
<p><strong>Understanding the formula:</strong>
- $\mathbf{v}_t$: Current \"velocity\" (combination of current gradient + previous velocity)
- $\beta \approx 0.9$: How much previous velocity to keep (90%)<br />
- $(1-\beta) = 0.1$: How much current gradient to use (10%)
- $\eta$: Learning rate (step size)</p>
<p><strong>Adam Optimizer:</strong> Combines momentum with adaptive learning rates:
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>\begin{align}
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>\mathbf{m}_t &amp;= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1) \nabla_\theta \mathcal{L} \quad (7)\\
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>\mathbf{v}_t &amp;= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2) (\nabla_\theta \mathcal{L})^2 \quad (8)\\
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>\theta_t &amp;= \theta_{t-1} - \eta \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon} \quad (9)
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>\end{align}
</code></pre></div></p>
<p>where $\hat{\mathbf{m}}_t$, $\hat{\mathbf{v}}_t$ are bias-corrected estimates.</p>
<p><strong>What Adam does - explained simply:</strong></p>
<p>Adam is like having a smart GPS that adjusts your driving based on two things:</p>
<ol>
<li><strong>$\mathbf{m}_t$ (momentum):</strong> "Which direction have we been going lately?" - Like momentum, but with exponential averaging</li>
<li><strong>$\mathbf{v}_t$ (second moment):</strong> "How bumpy has the road been?" - Tracks how much the gradients have been changing</li>
</ol>
<p><strong>The key insight:</strong> If the road has been very bumpy (high variance in gradients), take smaller steps. If it's been smooth and consistent, you can take bigger steps.</p>
<p><strong>Breaking down the symbols:</strong>
- $\beta_1 \approx 0.9$: How much to remember from previous direction (90%)
- $\beta_2 \approx 0.999$: How much to remember from previous bumpiness (99.9%) 
- $\epsilon \approx 10^{-8}$: Tiny number to prevent division by zero
- $\hat{\mathbf{m}}_t$, $\hat{\mathbf{v}}_t$: Bias-corrected estimates (explained below)</p>
<p><strong>Bias correction intuition:</strong> At the beginning, $\mathbf{m}_0 = \mathbf{v}_0 = 0$, so the averages are biased toward zero. We correct for this by dividing by $(1-\beta^t)$, which starts small and approaches 1.</p>
<p><strong>Car analogy:</strong> Adam is like cruise control that:
- Remembers which direction you've been driving (momentum)
- Adjusts speed based on road conditions (adaptive learning rate)
- Starts cautiously but gets more confident over time (bias correction)</p>
<h3 id="92-advanced-optimizers">9.2 Advanced Optimizers<a class="headerlink" href="#92-advanced-optimizers" title="Permanent link">&para;</a></h3>
<p><strong>AdamW vs Adam:</strong> AdamW decouples weight decay from gradient-based updates:</p>
<p><strong>Adam with L2 regularization:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \eta \lambda \theta_{t-1}
</code></pre></div></p>
<p><strong>AdamW (decoupled weight decay):</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>\theta_t = (1 - \eta \lambda) \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
</code></pre></div></p>
<p><strong>Why AdamW is better:</strong> Weight decay is applied regardless of gradient magnitude, leading to better generalization.</p>
<p><strong>$\beta_2$ Warmup:</strong> Start with high $\beta_2$ (e.g., 0.99) and gradually decrease to final value (e.g., 0.999) over first few thousand steps. Helps with training stability.</p>
<p><strong>Gradient Accumulation:</strong> Simulate larger batch sizes:
ðŸ’» <strong>Implementation Example</strong>: For gradient accumulation implementation, see <a href="./pynb/math_ref/optimization.ipynb">Optimization Notebook</a></p>
<h3 id="93-learning-rate-schedules">9.3 Learning Rate Schedules<a class="headerlink" href="#93-learning-rate-schedules" title="Permanent link">&para;</a></h3>
<p><strong>Why do we need schedules?</strong> Think of learning to drive: you start slow in the parking lot (warmup), drive at normal speed on the highway (main training), then slow down carefully when approaching your destination (decay).</p>
<p><strong>Warmup:</strong> Gradually increase learning rate to avoid early instability:
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>\eta_t = \eta_{\text{max}} \cdot \min\left(\frac{t}{T_{\text{warmup}}}, 1\right) \quad (10)
</code></pre></div></p>
<p><strong>Why warmup works:</strong>
- <strong>Early training is chaotic:</strong> Random initial weights create wild gradients
- <strong>Start gentle:</strong> Small learning rate prevents the model from making terrible early decisions
- <strong>Build confidence gradually:</strong> As the model learns basic patterns, we can be more aggressive</p>
<p><strong>Driving analogy:</strong> You don't floor the gas pedal the moment you start your car in winter - you let it warm up first.</p>
<p><strong>Cosine Decay:</strong> Smooth reduction following cosine curve prevents abrupt changes.</p>
<p><strong>Why cosine decay?</strong> 
- <strong>Smooth slowdown:</strong> Like gradually applying brakes instead of slamming them
- <strong>Fine-tuning phase:</strong> Later in training, we want to make small adjustments, not big jumps
- <strong>Mathematical smoothness:</strong> Cosine provides a natural, smooth curve from 1 to 0</p>
<p><strong>Formula:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>\eta_t = \eta_{\text{max}} \cdot 0.5 \left(1 + \cos\left(\pi \cdot \frac{t - T_{\text{warmup}}}{T_{\text{total}} - T_{\text{warmup}}}\right)\right)
</code></pre></div></p>
<p><strong>Real-world analogy:</strong> Like landing an airplane - you approach fast, then gradually slow down for a smooth landing, not a crash.</p>
<p><strong>Original Transformer Schedule:</strong> Combines warmup with inverse square root decay:
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>\eta_t = d_{\text{model}}^{-0.5} \cdot \min(t^{-0.5}, t \cdot T_{\text{warmup}}^{-1.5})
</code></pre></div></p>
<p><strong>When to use cosine vs original:</strong> Cosine for fine-tuning and shorter training; original schedule for training from scratch with very large models.</p>
<h3 id="94-gradient-clipping">9.4 Gradient Clipping<a class="headerlink" href="#94-gradient-clipping" title="Permanent link">&para;</a></h3>
<p><strong>The Problem:</strong> Sometimes gradients become extremely large (exploding gradients), causing the model to make huge, destructive updates.</p>
<p><strong>The Solution:</strong> Clip (limit) the gradients to a maximum norm.</p>
<p><strong>Global Norm Clipping:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>\tilde{g} = \min\left(1, \frac{c}{\|\mathbf{g}\|_2}\right) \mathbf{g} \quad (11)
</code></pre></div></p>
<p><strong>What this does intuitively:</strong>
- Calculate the total "size" of all gradients combined: $|\mathbf{g}|_2$
- If this size exceeds our limit $c$, scale all gradients down proportionally
- If it's within the limit, leave gradients unchanged</p>
<p><strong>Speedometer analogy:</strong> Like a speed limiter in a car. If you try to go 120 mph but the limit is 65 mph, it scales your speed down to 65 mph while keeping you in the same direction.</p>
<p><strong>Why proportional scaling?</strong> We want to keep the relative direction of updates the same, just make them smaller. It's like turning down the volume on music - all frequencies get reduced equally.</p>
<p><strong>Example:</strong>
- Your gradients total to norm 50, but your clip value is 5
- Scaling factor: $\min(1, 5/50) = 0.1$<br />
- All gradients get multiplied by 0.1 (reduced to 10% of original size)</p>
<h3 id="95-numerical-stability">9.5 Numerical Stability<a class="headerlink" href="#95-numerical-stability" title="Permanent link">&para;</a></h3>
<p><strong>Log-Sum-Exp Trick:</strong> For numerical stability in softmax:
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>\log\left(\sum_{i=1}^n e^{x_i}\right) = c + \log\left(\sum_{i=1}^n e^{x_i - c}\right) \quad (12)
</code></pre></div></p>
<p>where $c = \max_i x_i$ prevents overflow.</p>
<h2 id="10-efficient-attention--scaling">10. Efficient Attention &amp; Scaling<a class="headerlink" href="#10-efficient-attention--scaling" title="Permanent link">&para;</a></h2>
<h3 id="101-complexity-analysis">10.1 Complexity Analysis<a class="headerlink" href="#101-complexity-analysis" title="Permanent link">&para;</a></h3>
<p><strong>Standard Attention Complexity:</strong>
- Time: $O(n^2 d)$ for sequence length $n$, model dimension $d$
- Space: $O(n^2 + nd)$ for attention matrix and activations</p>
<p><strong>Memory Bottleneck:</strong> Attention matrix $A \in \mathbb{R}^{n \times n}$ dominates memory usage for long sequences.</p>
<p><strong>Detailed Complexity Breakdown:</strong>
1. <strong>QK^T computation</strong>: $O(n^2 d)$ time, $O(n^2)$ space
2. <strong>Softmax normalization</strong>: $O(n^2)$ time and space
3. <strong>Attention-Value multiplication</strong>: $O(n^2 d)$ time, $O(nd)$ space
4. <strong>Total</strong>: $O(n^2 d)$ time, $O(n^2 + nd)$ space</p>
<p><strong>Scaling Challenges:</strong>
- Quadratic scaling limits practical sequence lengths
- Memory requirements grow quadratically with sequence length
- Computational cost increases quadratically even with parallelization</p>
<h3 id="102-flashattention-memory-efficient-attention">10.2 FlashAttention: Memory-Efficient Attention<a class="headerlink" href="#102-flashattention-memory-efficient-attention" title="Permanent link">&para;</a></h3>
<p><strong>Core Idea:</strong> Compute attention without materializing the full $n \times n$ attention matrix.</p>
<p><strong>Tiling Strategy:</strong>
1. Divide $Q$, $K$, $V$ into blocks
2. Compute attention scores block by block
3. Use online softmax to maintain numerical stability
4. Accumulate results without storing intermediate attention weights</p>
<p><strong>Memory Reduction:</strong> From $O(n^2)$ to $O(n)$ memory complexity for the attention computation.</p>
<p><strong>Speed Improvement:</strong> Better GPU utilization through reduced memory bandwidth requirements.</p>
<p><strong>Key Insight:</strong> Trade computational redundancy for memory efficiency - recompute rather than store.</p>
<h3 id="103-multi-query-and-grouped-query-attention">10.3 Multi-Query and Grouped-Query Attention<a class="headerlink" href="#103-multi-query-and-grouped-query-attention" title="Permanent link">&para;</a></h3>
<p><strong>Multi-Query Attention (MQA):</strong> Share key and value projections across heads:
- Queries: $Q \in \mathbb{R}^{B \times H \times n \times d_k}$ (per-head)
- Keys/Values: $K, V \in \mathbb{R}^{B \times 1 \times n \times d_k}$ (shared)</p>
<p><strong>Grouped-Query Attention (GQA):</strong> Intermediate approach - group heads:
- Divide $H$ heads into $G$ groups
- Each group shares K, V projections
- Reduces KV cache size by factor $H/G$</p>
<p><strong>KV Cache Memory Analysis:</strong>
- <strong>Standard MHA:</strong> $2 \cdot B \cdot H \cdot n \cdot d_k$ parameters
- <strong>MQA:</strong> $2 \cdot B \cdot 1 \cdot n \cdot d_k$ parameters (HÃ— reduction)
- <strong>GQA:</strong> $2 \cdot B \cdot G \cdot n \cdot d_k$ parameters</p>
<p><strong>Quantization:</strong> Reduce memory further with int8/fp16 KV cache storage.</p>
<h3 id="104-kv-caching-for-autoregressive-generation">10.4 KV Caching for Autoregressive Generation<a class="headerlink" href="#104-kv-caching-for-autoregressive-generation" title="Permanent link">&para;</a></h3>
<p><strong>Key Insight:</strong> During generation, keys and values for previous tokens don't change.</p>
<p><strong>Cache Update:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>K_{\text{cache}} \gets \mathrm{concat}(K_{\text{cache}},\ k_{\text{new}}) \tag{42}
</code></pre></div>
<ul>
<li><strong>$K_{\text{cache}}$</strong>: Cached keys from previous tokens.</li>
<li><strong>$V_{\text{cache}}$</strong>: Cached values from previous tokens.</li>
<li><strong>$k_{\text{new}}, v_{\text{new}}$</strong>: Key and value for the new token.</li>
<li><strong>$q_{\text{new}}$</strong>: Query for the new token.</li>
</ul>
<p>At each generation step, append the new key and value to the cache, then compute attention using the full cache.</p>
<p><strong>Memory Trade-off:</strong> Cache size grows as $O(nd)$ but eliminates $O(n^2)$ recomputation.</p>
<p>ðŸ’» <strong>Implementation Example</strong>: For KV Cache implementation, see <a href="./pynb/math_ref/advanced_concepts.ipynb">Advanced Concepts Notebook</a></p>
<h3 id="105-linear-attention-approximations">10.5 Linear Attention Approximations<a class="headerlink" href="#105-linear-attention-approximations" title="Permanent link">&para;</a></h3>
<p><strong>Kernel Method View:</strong> Approximate $\text{softmax}(\mathbf{q}^T\mathbf{k})$ with $\phi(\mathbf{q})^T \phi(\mathbf{k})$ for feature map $\phi$.</p>
<p><strong>Linear Attention:</strong>
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>\text{LinAttn}(Q,K,V) = \frac{\phi(Q)(\phi(K)^T V)}{\phi(Q)(\phi(K)^T \mathbf{1})} \quad (45)
</code></pre></div></p>
<p><strong>Complexity Reduction:</strong> Reduces from $O(n^2 d)$ to $O(nd^2)$ when $d &lt; n$.</p>
<h2 id="11-regularization-generalization-and-calibration">11. Regularization, Generalization, and Calibration<a class="headerlink" href="#11-regularization-generalization-and-calibration" title="Permanent link">&para;</a></h2>
<h3 id="111-dropout-in-transformers">11.1 Dropout in Transformers<a class="headerlink" href="#111-dropout-in-transformers" title="Permanent link">&para;</a></h3>
<p><strong>Attention Dropout:</strong> Applied to attention weights:
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>A_{\text{dropped}} = \text{Dropout}(\text{softmax}(QK^T/\sqrt{d_k})) \quad (46)
</code></pre></div></p>
<p><strong>FFN Dropout:</strong> Applied after first linear transformation:
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>\text{FFN}(\mathbf{x}) = W_2 \cdot \text{Dropout}(\text{GELU}(W_1 \mathbf{x})) \quad (47)
</code></pre></div></p>
<h3 id="112-evaluation-and-calibration">11.2 Evaluation and Calibration<a class="headerlink" href="#112-evaluation-and-calibration" title="Permanent link">&para;</a></h3>
<p><strong>Expected Calibration Error (ECE):</strong> Measures how well predicted probabilities match actual outcomes:
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>\text{ECE} = \sum_{m=1}^M \frac{|B_m|}{n} |\text{acc}(B_m) - \text{conf}(B_m)|
</code></pre></div>
where $B_m$ are probability bins, $\text{acc}$ is accuracy, $\text{conf}$ is confidence.</p>
<p><strong>Temperature Scaling:</strong> Post-training calibration method:
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>P_{\text{cal}}(y|x) = \text{softmax}(\mathbf{z}/T)
</code></pre></div>
where $T &gt; 1$ makes predictions less confident, $T &lt; 1$ more confident.</p>
<p><strong>Perplexity Dependence on Tokenizer:</strong> PPL comparisons only valid with same tokenizer. Different tokenizers create different sequence lengths and vocabulary sizes.</p>
<p><strong>Example:</strong> "hello world" might be:
- GPT tokenizer: ["hel", "lo", " wor", "ld"] (4 tokens)
- Character-level: ["h", "e", "l", "l", "o", " ", "w", "o", "r", "l", "d"] (11 tokens)</p>
<h3 id="113-advanced-tokenization">11.3 Advanced Tokenization<a class="headerlink" href="#113-advanced-tokenization" title="Permanent link">&para;</a></h3>
<p><strong>Byte-Level BPE vs Unigram:</strong>
- <strong>BPE:</strong> Greedily merges frequent character pairs, handles any Unicode
- <strong>Unigram:</strong> Probabilistic model, often better for morphologically rich languages</p>
<p><strong>Special Token Handling:</strong>
- <strong>BOS (Beginning of Sequence):</strong> Often used for unconditional generation
- <strong>EOS (End of Sequence):</strong> Signals completion, crucial for proper training
- <strong>PAD:</strong> For batching variable-length sequences</p>
<p><strong>Embedding/LM-Head Tying Caveats:</strong>
When sharing weights, ensure shape compatibility:
- Embedding: $E \in \mathbb{R}^{V \times d_{\text{model}}}$
- LM head: needs $\mathbb{R}^{d_{\text{model}} \times V}$
- Solution: Use $E^T$ for output projection (as shown in equation 40)</p>
<h3 id="114-label-smoothing">11.4 Label Smoothing<a class="headerlink" href="#114-label-smoothing" title="Permanent link">&para;</a></h3>
<p><strong>Smooth Labels:</strong> Replace one-hot targets with:
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>y_{\text{smooth}} = (1-\alpha) y_{\text{true}} + \frac{\alpha}{V} \mathbf{1} \quad (48)
</code></pre></div></p>
<p><strong>Effect on Gradients:</strong> Prevents overconfident predictions and improves calibration.</p>
<h2 id="14-common-pitfalls--misconceptions">14. Common Pitfalls &amp; Misconceptions<a class="headerlink" href="#14-common-pitfalls--misconceptions" title="Permanent link">&para;</a></h2>
<h3 id="141-high-dimensional-distance-misconceptions">14.1 High-Dimensional Distance Misconceptions<a class="headerlink" href="#141-high-dimensional-distance-misconceptions" title="Permanent link">&para;</a></h3>
<p><strong>Pitfall:</strong> Using Euclidean distance instead of cosine similarity in high dimensions.
<strong>Fix:</strong> In $d &gt; 100$, most vectors are approximately orthogonal, making cosine similarity more discriminative.</p>
<h3 id="142-attention-scaling-mistakes">14.2 Attention Scaling Mistakes<a class="headerlink" href="#142-attention-scaling-mistakes" title="Permanent link">&para;</a></h3>
<p><strong>Pitfall:</strong> Forgetting $1/\sqrt{d_k}$ scaling or using wrong dimension.
<strong>Symptom:</strong> Attention weights become too peaked, leading to poor gradients.
<strong>Fix:</strong> Always scale by $\sqrt{d_k}$ where $d_k$ is the key dimension. Note that $d_k=d_{\text{model}}/h$ under common implementations.</p>
<h3 id="143-layernorm-placement">14.3 LayerNorm Placement<a class="headerlink" href="#143-layernorm-placement" title="Permanent link">&para;</a></h3>
<p><strong>Pitfall:</strong> Using post-LayerNorm (original) instead of pre-LayerNorm (modern).
<strong>Issue:</strong> Post-LN can lead to training instability in deep models.
<strong>Modern Practice:</strong> Apply LayerNorm before attention and FFN blocks.</p>
<h3 id="144-softmax-temperature-misuse">14.4 Softmax Temperature Misuse<a class="headerlink" href="#144-softmax-temperature-misuse" title="Permanent link">&para;</a></h3>
<p><strong>Pitfall:</strong> Applying temperature scaling inconsistently.
<strong>Correct Usage:</strong> Temperature $\tau$ in $\text{softmax}(\mathbf{z}/\tau)$ controls sharpness:
- $\tau &gt; 1$: Smoother distribution
- $\tau &lt; 1$: Sharper distribution</p>
<h2 id="15-summary--what-to-learn-next">15. Summary &amp; What to Learn Next<a class="headerlink" href="#15-summary--what-to-learn-next" title="Permanent link">&para;</a></h2>
<h3 id="151-key-mathematical-insights">15.1 Key Mathematical Insights<a class="headerlink" href="#151-key-mathematical-insights" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Attention as Similarity Search:</strong> Q/K/V framework emerges naturally from maximum inner product search</li>
<li><strong>Scaling Laws:</strong> $1/\sqrt{d_k}$ scaling prevents attention collapse (overly peaked distributions) in high dimensions  </li>
<li><strong>Residual Connections:</strong> Enable gradient flow through deep networks via skip connections</li>
<li><strong>Multi-Head Architecture:</strong> Parallel subspace projections enable diverse attention patterns</li>
</ol>
<h3 id="152-advanced-techniques-covered">15.2 Advanced Techniques Covered<a class="headerlink" href="#152-advanced-techniques-covered" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Optimization:</strong> SGD momentum, Adam, AdamW with proper learning rate schedules</li>
<li><strong>Efficiency:</strong> FlashAttention, Multi-Query/Grouped-Query Attention, KV caching</li>
<li><strong>Regularization:</strong> Dropout, label smoothing, calibration techniques</li>
<li><strong>Numerical Stability:</strong> Gradient clipping, mixed precision, proper initialization</li>
</ol>
<h3 id="153-next-steps">15.3 Next Steps<a class="headerlink" href="#153-next-steps" title="Permanent link">&para;</a></h3>
<p><strong>Scaling Laws:</strong> Study how performance scales with model size, data, and compute (Kaplan et al., 2020)</p>
<p><strong>Parameter-Efficient Fine-Tuning:</strong> LoRA, adapters, and other methods for efficient adaptation</p>
<p><strong>Retrieval-Augmented Models:</strong> Combining parametric knowledge with external memory</p>
<p><strong>Advanced Architectures:</strong> Mixture of Experts, sparse attention patterns, and alternative architectures</p>
<hr />
<h2 id="connection-to-part-1">Connection to Part 1<a class="headerlink" href="#connection-to-part-1" title="Permanent link">&para;</a></h2>
<p>This tutorial builds directly on the foundations established in <a href="../transformers_math1/">Part 1: Building Intuition and Core Concepts</a>. Together, these two parts provide a complete mathematical understanding of Transformer architectures, from basic principles through advanced implementation considerations.</p>
<p>If you haven't already, we highly recommend reading Part 1 first to build the necessary intuition before diving into these advanced topics.</p>
<hr />
<h2 id="further-reading">Further Reading<a class="headerlink" href="#further-reading" title="Permanent link">&para;</a></h2>
<p><strong>Core Papers:</strong>
[1] Vaswani, A., et al. "Attention is all you need." <em>Advances in Neural Information Processing Systems</em>, 2017.
[2] Devlin, J., et al. "BERT: Pre-training of deep bidirectional transformers for language understanding." <em>NAACL-HLT</em>, 2019.
[3] Brown, T., et al. "Language models are few-shot learners." <em>Advances in Neural Information Processing Systems</em>, 2020.</p>
<p><strong>Mathematical Foundations:</strong>
[4] Kaplan, J., et al. "Scaling laws for neural language models." <em>arXiv preprint arXiv:2001.08361</em>, 2020.
[5] Su, J., et al. "RoFormer: Enhanced transformer with rotary position embedding." <em>arXiv preprint arXiv:2104.09864</em>, 2021.</p>
<p><strong>Efficiency &amp; Scaling:</strong>
[6] Dao, T., et al. "FlashAttention: Fast and memory-efficient exact attention with IO-awareness." <em>Advances in Neural Information Processing Systems</em>, 2022.
[7] Shazeer, N. "Fast transformer decoding: One write-head is all you need." <em>arXiv preprint arXiv:1911.02150</em>, 2019.</p>
<p><strong>Training &amp; Optimization:</strong>
[8] Loshchilov, I., &amp; Hutter, F. "Decoupled weight decay regularization." <em>ICLR</em>, 2019.
[9] Xiong, R., et al. "On layer normalization in the transformer architecture." <em>ICML</em>, 2020.
[10] Press, O., &amp; Wolf, L. "Using the output embedding to improve language models." <em>EACL</em>, 2017.</p>
<hr />
<h2 id="appendix-a-symbolshape-reference">Appendix A: Symbol/Shape Reference<a class="headerlink" href="#appendix-a-symbolshape-reference" title="Permanent link">&para;</a></h2>
<h3 id="single-head-attention-shapes">Single-Head Attention Shapes<a class="headerlink" href="#single-head-attention-shapes" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Typical Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>$Q, K, V$</td>
<td>Query, Key, Value matrices</td>
<td>$[n \times d_k], [n \times d_k], [n \times d_v]$</td>
</tr>
<tr>
<td>$n$</td>
<td>Sequence length</td>
<td>Scalar</td>
</tr>
<tr>
<td>$d_{\text{model}}$</td>
<td>Model dimension</td>
<td>Scalar (512, 768, 1024, etc.)</td>
</tr>
<tr>
<td>$d_k, d_v$</td>
<td>Key, value dimensions</td>
<td>Usually $d_{\text{model}}/h$</td>
</tr>
<tr>
<td>$h$</td>
<td>Number of attention heads</td>
<td>Scalar (8, 12, 16, etc.)</td>
</tr>
</tbody>
</table>
<h3 id="multi-head--batched-shapes">Multi-Head &amp; Batched Shapes<a class="headerlink" href="#multi-head--batched-shapes" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Symbol</th>
<th>Meaning</th>
<th>Batched Multi-Head Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>$Q, K, V$</td>
<td>Projected queries, keys, values</td>
<td>$[B, H, n, d_k], [B, H, n, d_k], [B, H, n, d_v]$</td>
</tr>
<tr>
<td>$A$</td>
<td>Attention weights matrix</td>
<td>$[B, H, n, n]$</td>
</tr>
<tr>
<td>$O$</td>
<td>Attention output (pre-concat)</td>
<td>$[B, H, n, d_v]$</td>
</tr>
<tr>
<td>$O_{\text{proj}}$</td>
<td>Final output (post-concat)</td>
<td>$[B, n, d_{\text{model}}]$</td>
</tr>
<tr>
<td>$W^Q, W^K, W^V$</td>
<td>Attention projection matrices</td>
<td>$[d_{\text{model}} \times d_k]$ per head</td>
</tr>
<tr>
<td>$W^O$</td>
<td>Output projection</td>
<td>$[d_{\text{model}} \times d_{\text{model}}]$</td>
</tr>
</tbody>
</table>
<p><strong>Convention:</strong> $B$ = batch size, $H$ = number of heads, $n$ = sequence length, $d_k = d_v = d_{\text{model}}/H$</p>
<h2 id="appendix-b-key-derivations">Appendix B: Key Derivations<a class="headerlink" href="#appendix-b-key-derivations" title="Permanent link">&para;</a></h2>
<h3 id="b1-softmax-gradient">B.1 Softmax Gradient<a class="headerlink" href="#b1-softmax-gradient" title="Permanent link">&para;</a></h3>
<p>For $p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>\frac{\partial p_i}{\partial z_j} = \begin{cases}
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>p_i(1 - p_i) &amp; \text{if } i = j \\
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>-p_i p_j &amp; \text{if } i \neq j
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>\end{cases} = p_i(\delta_{ij} - p_j)
</code></pre></div>
<h3 id="b2-matrix-calculus-identities">B.2 Matrix Calculus Identities<a class="headerlink" href="#b2-matrix-calculus-identities" title="Permanent link">&para;</a></h3>
<p><strong>Trace-Vec Identity:</strong> $\text{tr}(AB) = \text{vec}(A^T)^T \text{vec}(B)$</p>
<p><strong>Kronecker Product:</strong> $\text{vec}(AXB) = (B^T \otimes A)\text{vec}(X)$</p>
<p><strong>Chain Rule for Matrices:</strong> $\frac{\partial f}{\partial X} = \sum_Y \frac{\partial f}{\partial Y} \frac{\partial Y}{\partial X}$</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.top", "navigation.instant", "content.code.copy", "content.tabs.link", "toc.follow"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
    
  </body>
</html>