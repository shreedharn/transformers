
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../pytorch_ref/">
      
      
        <link rel="next" href="../transformers_math2/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Mathematical Foundations 1 - Introduction to Transformers</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../styles.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#the-mathematics-of-transformers-from-first-principles-to-practice" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Introduction to Transformers" class="md-header__button md-logo" aria-label="Introduction to Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Introduction to Transformers
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Mathematical Foundations 1
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Introduction to Transformers" class="md-nav__button md-logo" aria-label="Introduction to Transformers" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Introduction to Transformers
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../nn_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Neural Network Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../mlp_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Building Networks with MLP
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../rnn_intro/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Modeling with RNN
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers Fundamentals
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Transformers Advanced
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../knowledge_store/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Knowledge Store
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" checked>
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            References
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PyTorch Primer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 1
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 1
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#part-1-building-intuition-and-core-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      Part 1: Building Intuition and Core Concepts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    <span class="md-ellipsis">
      Abstract
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assumptions--conventions" class="md-nav__link">
    <span class="md-ellipsis">
      Assumptions &amp; Conventions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-roadmap" class="md-nav__link">
    <span class="md-ellipsis">
      1. Roadmap
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-neural-network-training-mathematical-foundations" class="md-nav__link">
    <span class="md-ellipsis">
      2. Neural Network Training: Mathematical Foundations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Neural Network Training: Mathematical Foundations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-from-training-to-inference-the-complete-journey" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 From Training to Inference: The Complete Journey
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 From Training to Inference: The Complete Journey">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-from-line-slopes-to-neural-network-training" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.1 From Line Slopes to Neural Network Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from-2d-slopes-to-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      From 2D Slopes to Gradient Descent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#worked-example-fx--x²" class="md-nav__link">
    <span class="md-ellipsis">
      Worked Example: f(x) = x²
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extending-to-multiple-variables-gradients-as-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Extending to Multiple Variables: Gradients as Vectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-form-for-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Form for Machine Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-hidden-layer-mlp-putting-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Single Hidden Layer MLP: Putting It All Together
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-learning-rate-α-universal-step-size-controller" class="md-nav__link">
    <span class="md-ellipsis">
      The Learning Rate α: Universal Step Size Controller
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-connection-from-slopes-to-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Visual Connection: From Slopes to Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-gradient-fields-and-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Gradient Fields and Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-residual-connections-as-discretized-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 Residual Connections as Discretized Dynamics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-deep-learning-mathematics-in-context" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Deep Learning Mathematics in Context
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Deep Learning Mathematics in Context">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-vectors-as-word-meanings" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.1 Vectors as Word Meanings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-matrices-as-transformations-of-meaning" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 Matrices as Transformations of Meaning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-gradients-as-learning-signals" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.3 Gradients as Learning Signals
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-softmax-and-cross-entropy-from-scores-to-decisions" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 Softmax and Cross-Entropy: From Scores to Decisions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-multilayer-perceptrons-as-a-warm-up" class="md-nav__link">
    <span class="md-ellipsis">
      3. Multilayer Perceptrons as a Warm-Up
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Multilayer Perceptrons as a Warm-Up">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-backpropagation-derivation" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Backpropagation Derivation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-advanced-normalization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Advanced Normalization Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-alternative-normalization-methods" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Alternative Normalization Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-high-dimensional-geometry--similarity" class="md-nav__link">
    <span class="md-ellipsis">
      4. High-Dimensional Geometry &amp; Similarity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. High-Dimensional Geometry &amp; Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-distance-metrics-in-high-dimensions" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Distance Metrics in High Dimensions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-distance-calculations-with-concrete-examples" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Distance Calculations with Concrete Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 Distance Calculations with Concrete Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#421-cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.1 Cosine Similarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#422-euclidean-distance" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.2 Euclidean Distance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-maximum-inner-product-search-mips" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Maximum Inner Product Search (MIPS)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-from-similarity-to-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5. From Similarity to Attention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. From Similarity to Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-deriving-scaled-dot-product-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Deriving Scaled Dot-Product Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-masked-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 Masked Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-why-the-sqrtd_k-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Why the $\sqrt{d_k}$ Scaling?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-backpropagation-through-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Backpropagation Through Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-multi-head-attention--positional-information" class="md-nav__link">
    <span class="md-ellipsis">
      6. Multi-Head Attention &amp; Positional Information
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Multi-Head Attention &amp; Positional Information">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-multi-head-as-subspace-projections" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Multi-Head as Subspace Projections
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-advanced-positional-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 Advanced Positional Encodings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-alternative-position-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      6.3 Alternative Position Encodings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-transformer-block-mathematics" class="md-nav__link">
    <span class="md-ellipsis">
      7. Transformer Block Mathematics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Transformer Block Mathematics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-complete-block-equations" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 Complete Block Equations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-why-gelu-over-relu" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Why GELU over ReLU?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-training-objective--tokenizationembeddings" class="md-nav__link">
    <span class="md-ellipsis">
      8. Training Objective &amp; Tokenization/Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Training Objective &amp; Tokenization/Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-next-token-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Next-Token Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-embedding-mathematics" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Embedding Mathematics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-worked-mini-examples" class="md-nav__link">
    <span class="md-ellipsis">
      9. Worked Mini-Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Worked Mini-Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-tiny-attention-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 Tiny Attention Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-backprop-through-simple-attention" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 Backprop Through Simple Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#continuing-to-advanced-topics" class="md-nav__link">
    <span class="md-ellipsis">
      Continuing to Advanced Topics
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_math2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematical Foundations 2
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../math_quick_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mathematical Quick Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../history_quick_ref/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequence Modeling History
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Glossary
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    README
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LICENSE
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#part-1-building-intuition-and-core-concepts" class="md-nav__link">
    <span class="md-ellipsis">
      Part 1: Building Intuition and Core Concepts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#abstract" class="md-nav__link">
    <span class="md-ellipsis">
      Abstract
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assumptions--conventions" class="md-nav__link">
    <span class="md-ellipsis">
      Assumptions &amp; Conventions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-roadmap" class="md-nav__link">
    <span class="md-ellipsis">
      1. Roadmap
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-neural-network-training-mathematical-foundations" class="md-nav__link">
    <span class="md-ellipsis">
      2. Neural Network Training: Mathematical Foundations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Neural Network Training: Mathematical Foundations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-from-training-to-inference-the-complete-journey" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 From Training to Inference: The Complete Journey
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.1 From Training to Inference: The Complete Journey">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-from-line-slopes-to-neural-network-training" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.1 From Line Slopes to Neural Network Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#from-2d-slopes-to-gradient-descent" class="md-nav__link">
    <span class="md-ellipsis">
      From 2D Slopes to Gradient Descent
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#worked-example-fx--x²" class="md-nav__link">
    <span class="md-ellipsis">
      Worked Example: f(x) = x²
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extending-to-multiple-variables-gradients-as-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      Extending to Multiple Variables: Gradients as Vectors
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#matrix-form-for-machine-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Matrix Form for Machine Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#single-hidden-layer-mlp-putting-it-all-together" class="md-nav__link">
    <span class="md-ellipsis">
      Single Hidden Layer MLP: Putting It All Together
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-learning-rate-α-universal-step-size-controller" class="md-nav__link">
    <span class="md-ellipsis">
      The Learning Rate α: Universal Step Size Controller
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#visual-connection-from-slopes-to-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Visual Connection: From Slopes to Networks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-gradient-fields-and-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.2 Gradient Fields and Optimization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-residual-connections-as-discretized-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      2.1.3 Residual Connections as Discretized Dynamics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-deep-learning-mathematics-in-context" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Deep Learning Mathematics in Context
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.2 Deep Learning Mathematics in Context">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-vectors-as-word-meanings" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.1 Vectors as Word Meanings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-matrices-as-transformations-of-meaning" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.2 Matrices as Transformations of Meaning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-gradients-as-learning-signals" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.3 Gradients as Learning Signals
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#224-softmax-and-cross-entropy-from-scores-to-decisions" class="md-nav__link">
    <span class="md-ellipsis">
      2.2.4 Softmax and Cross-Entropy: From Scores to Decisions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-multilayer-perceptrons-as-a-warm-up" class="md-nav__link">
    <span class="md-ellipsis">
      3. Multilayer Perceptrons as a Warm-Up
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Multilayer Perceptrons as a Warm-Up">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-backpropagation-derivation" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Backpropagation Derivation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-advanced-normalization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Advanced Normalization Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-alternative-normalization-methods" class="md-nav__link">
    <span class="md-ellipsis">
      3.4 Alternative Normalization Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-high-dimensional-geometry--similarity" class="md-nav__link">
    <span class="md-ellipsis">
      4. High-Dimensional Geometry &amp; Similarity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. High-Dimensional Geometry &amp; Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-distance-metrics-in-high-dimensions" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Distance Metrics in High Dimensions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-distance-calculations-with-concrete-examples" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Distance Calculations with Concrete Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4.2 Distance Calculations with Concrete Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#421-cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.1 Cosine Similarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#422-euclidean-distance" class="md-nav__link">
    <span class="md-ellipsis">
      4.2.2 Euclidean Distance
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-maximum-inner-product-search-mips" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Maximum Inner Product Search (MIPS)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-from-similarity-to-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5. From Similarity to Attention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. From Similarity to Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-deriving-scaled-dot-product-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Deriving Scaled Dot-Product Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-masked-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 Masked Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-why-the-sqrtd_k-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Why the $\sqrt{d_k}$ Scaling?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-backpropagation-through-attention" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Backpropagation Through Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-multi-head-attention--positional-information" class="md-nav__link">
    <span class="md-ellipsis">
      6. Multi-Head Attention &amp; Positional Information
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Multi-Head Attention &amp; Positional Information">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-multi-head-as-subspace-projections" class="md-nav__link">
    <span class="md-ellipsis">
      6.1 Multi-Head as Subspace Projections
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-advanced-positional-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      6.2 Advanced Positional Encodings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-alternative-position-encodings" class="md-nav__link">
    <span class="md-ellipsis">
      6.3 Alternative Position Encodings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-transformer-block-mathematics" class="md-nav__link">
    <span class="md-ellipsis">
      7. Transformer Block Mathematics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Transformer Block Mathematics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-complete-block-equations" class="md-nav__link">
    <span class="md-ellipsis">
      7.1 Complete Block Equations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-why-gelu-over-relu" class="md-nav__link">
    <span class="md-ellipsis">
      7.2 Why GELU over ReLU?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-training-objective--tokenizationembeddings" class="md-nav__link">
    <span class="md-ellipsis">
      8. Training Objective &amp; Tokenization/Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Training Objective &amp; Tokenization/Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-next-token-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      8.1 Next-Token Prediction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-embedding-mathematics" class="md-nav__link">
    <span class="md-ellipsis">
      8.2 Embedding Mathematics
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-worked-mini-examples" class="md-nav__link">
    <span class="md-ellipsis">
      9. Worked Mini-Examples
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Worked Mini-Examples">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-tiny-attention-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      9.1 Tiny Attention Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-backprop-through-simple-attention" class="md-nav__link">
    <span class="md-ellipsis">
      9.2 Backprop Through Simple Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#continuing-to-advanced-topics" class="md-nav__link">
    <span class="md-ellipsis">
      Continuing to Advanced Topics
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="the-mathematics-of-transformers-from-first-principles-to-practice">The Mathematics of Transformers: From First Principles to Practice<a class="headerlink" href="#the-mathematics-of-transformers-from-first-principles-to-practice" title="Permanent link">&para;</a></h1>
<h2 id="part-1-building-intuition-and-core-concepts">Part 1: Building Intuition and Core Concepts<a class="headerlink" href="#part-1-building-intuition-and-core-concepts" title="Permanent link">&para;</a></h2>
<h2 id="abstract">Abstract<a class="headerlink" href="#abstract" title="Permanent link">&para;</a></h2>
<p>This tutorial builds the mathematical foundations of Transformer architectures from first principles, targeting motivated high school students with basic algebra and geometry background. This first part focuses on building intuition, covering linear algebra basics, simple networks, and the step-by-step path to attention and the Transformer block. We progress systematically from optimization theory and high-dimensional geometry through attention mechanisms to complete Transformer blocks, emphasizing mathematical intuition, worked derivations, and practical implementation considerations. Every mathematical concept is explained with real-world analogies and intuitive reasoning before diving into the formal mathematics.</p>
<p>For advanced topics including optimization, training stability, scaling laws, and implementation details for real-world large models, see <a href="../transformers_math2/">Part 2: Advanced Concepts and Scaling</a>.</p>
<h2 id="assumptions--conventions">Assumptions &amp; Conventions<a class="headerlink" href="#assumptions--conventions" title="Permanent link">&para;</a></h2>
<p>Mathematical Notation:</p>
<ul>
<li>Vectors are row-major; matrices multiply on the right</li>
<li>Shapes annotated as [seq, dim] or [batch, seq, heads, dim]</li>
<li>Masking uses additive large negative values (-∞) before softmax</li>
<li>Row-wise softmax normalization</li>
<li>Token/position numbering starts from 0</li>
<li>Default dtype is fp32 unless specified</li>
<li>Equations numbered sequentially throughout document</li>
</ul>
<p>For Advanced Topics:</p>
<ul>
<li><a href="../transformers_math2/">Part 2: Advanced Concepts and Scaling</a> covers optimization, efficient attention, regularization, and implementation details</li>
</ul>
<p>Additional Resources:</p>
<ul>
<li><a href="../glossary/">Glossary</a> - Comprehensive terms and definitions</li>
</ul>
<h2 id="1-roadmap">1. Roadmap<a class="headerlink" href="#1-roadmap" title="Permanent link">&para;</a></h2>
<p>We begin with optimization fundamentals and high-dimensional geometry, then build attention as a principled similarity search mechanism. The journey: <strong>gradients → similarity metrics → attention → multi-head attention → full Transformer blocks</strong>. Each step connects mathematical theory to practical implementation, culminating in a complete understanding of how Transformers process sequences through learned representations and attention-based information routing.</p>
<p>For the complete journey including <strong>efficient inference</strong>, <strong>scaling laws</strong>, and <strong>advanced optimization techniques</strong>, continue to <a href="../transformers_math2/">Part 2</a>.</p>
<h2 id="2-neural-network-training-mathematical-foundations">2. Neural Network Training: Mathematical Foundations<a class="headerlink" href="#2-neural-network-training-mathematical-foundations" title="Permanent link">&para;</a></h2>
<p>📚 Quick Reference: For pure mathematical concepts and formulas, see <a href="../math_quick_ref/">Mathematical Quick Reference</a>. This section focuses on mathematical concepts <strong>in the context of deep learning</strong>.</p>
<h3 id="21-from-training-to-inference-the-complete-journey">2.1 From Training to Inference: The Complete Journey<a class="headerlink" href="#21-from-training-to-inference-the-complete-journey" title="Permanent link">&para;</a></h3>
<h4 id="211-from-line-slopes-to-neural-network-training">2.1.1 From Line Slopes to Neural Network Training<a class="headerlink" href="#211-from-line-slopes-to-neural-network-training" title="Permanent link">&para;</a></h4>
<p>Building Intuition: Slope of a Line</p>
<p>Let's start with something familiar - the equation of a straight line:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>y = mx + b
</code></pre></div></p>
<p>where:</p>
<ul>
<li>m is the slope - tells us how steep the line is</li>
<li>b is the y-intercept - where the line crosses the y-axis</li>
</ul>
<p>What does slope mean intuitively? Slope tells us "for every step I move right, how much do I move up or down?" If the slope is 2, then moving 1 step right means moving 2 steps up. If the slope is -0.5, then moving 1 step right means moving 0.5 steps down.</p>
<p>Connecting to Optimization: In machine learning, we want to find the "bottom of a valley" - the point where our error is smallest. To do this, we need to know which direction is "downhill."</p>
<h4 id="from-2d-slopes-to-gradient-descent">From 2D Slopes to Gradient Descent<a class="headerlink" href="#from-2d-slopes-to-gradient-descent" title="Permanent link">&para;</a></h4>
<p>The Derivative as Slope: For any function $f(x)$, the derivative $\frac{df}{dx}$ tells us the slope at any point:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>\frac{df}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}
</code></pre></div>
<p>What this equation means: "If I move a tiny amount h to the right, how much does f change?" The derivative is the limit as that tiny amount approaches zero.</p>
<p>Gradient Descent in One Dimension: If we want to minimize $f(x)$, we update $x$ using:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>x_{\text{new}} = x_{\text{old}} - \alpha \frac{df}{dx}
</code></pre></div>
<p>where $\alpha$ (alpha) is the learning rate - how big steps we take.</p>
<p>The key insight: The negative sign makes us go <em>opposite</em> to the slope direction. If the slope is positive (going uphill to the right), we move left. If the slope is negative (going downhill to the right), we move right.</p>
<h4 id="worked-example-fx--x²">Worked Example: f(x) = x²<a class="headerlink" href="#worked-example-fx--x²" title="Permanent link">&para;</a></h4>
<p>Let's minimize $f(x) = x^2$ starting from $x = 3$:</p>
<p>Step 1: Compute the derivative: $\frac{df}{dx} = 2x$</p>
<p>Step 2: Choose learning rate: $\alpha = 0.1$</p>
<p>Step 3: Update step by step:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>Iteration 0: x = 3.0, f(x) = 9.0, df/dx = 6.0
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>             x_new = 3.0 - 0.1 × 6.0 = 2.4
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>Iteration 1: x = 2.4, f(x) = 5.76, df/dx = 4.8  
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>             x_new = 2.4 - 0.1 × 4.8 = 1.92
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>Iteration 2: x = 1.92, f(x) = 3.69, df/dx = 3.84
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>             x_new = 1.92 - 0.1 × 3.84 = 1.54
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>...continuing...
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>Iteration 10: x ≈ 0.27, f(x) ≈ 0.07
</code></pre></div>
<p>What's happening: We're taking steps toward x = 0 (the minimum of x²), and each step gets smaller as we approach the bottom.</p>
<h4 id="extending-to-multiple-variables-gradients-as-vectors">Extending to Multiple Variables: Gradients as Vectors<a class="headerlink" href="#extending-to-multiple-variables-gradients-as-vectors" title="Permanent link">&para;</a></h4>
<p>From Derivatives to Gradients: When we have multiple variables, like $f(x, y) = x^2 + y^2$, we need partial derivatives:</p>
<ul>
<li>$\frac{\partial f}{\partial x} = 2x$ (rate of change with respect to x, holding y fixed)</li>
<li>$\frac{\partial f}{\partial y} = 2y$ (rate of change with respect to y, holding x fixed)</li>
</ul>
<p>The Gradient Vector: We combine these into a gradient vector:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix} = \begin{bmatrix} 2x \\ 2y \end{bmatrix}
</code></pre></div>
<p>Vector Gradient Descent: Now our update rule becomes:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>\begin{bmatrix} x_{\text{new}} \\ y_{\text{new}} \end{bmatrix} = \begin{bmatrix} x_{\text{old}} \\ y_{\text{old}} \end{bmatrix} - \alpha \begin{bmatrix} 2x_{\text{old}} \\ 2y_{\text{old}} \end{bmatrix}
</code></pre></div>
<p>Intuition: The gradient vector points in the direction of steepest <em>ascent</em>. By moving in the <em>opposite</em> direction (negative gradient), we go downhill most quickly.</p>
<h4 id="matrix-form-for-machine-learning">Matrix Form for Machine Learning<a class="headerlink" href="#matrix-form-for-machine-learning" title="Permanent link">&para;</a></h4>
<p>Setting up the Problem: In machine learning, we have:</p>
<ul>
<li>X: Input matrix with shape (samples × features) - each row is one data point</li>
<li>W: Weight matrix with shape (features × outputs) - the parameters we want to learn  </li>
<li>b: Bias vector with shape (outputs,) - additional adjustable parameters</li>
<li>Y_hat: Predictions with shape (samples × outputs), computed as Y_hat = XW + b</li>
</ul>
<p>Loss Function: We measure how wrong our predictions are using mean squared error:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>L = \frac{1}{N} \sum_{i=1}^N \|Y_{\text{true}}^{(i)} - Y_{\text{hat}}^{(i)}\|^2
</code></pre></div>
<p>Matrix Gradients: To minimize the loss, we need gradients with respect to W and b:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>\frac{\partial L}{\partial W} = \frac{2}{N} X^T (Y_{\text{hat}} - Y_{\text{true}})
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>\frac{\partial L}{\partial b} = \frac{2}{N} \sum_{i=1}^N (Y_{\text{hat}}^{(i)} - Y_{\text{true}}^{(i)})
</code></pre></div>
<p>Matrix Gradient Descent Updates:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>W_{\text{new}} = W_{\text{old}} - \alpha \frac{\partial L}{\partial W}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>b_{\text{new}} = b_{\text{old}} - \alpha \frac{\partial L}{\partial b}
</code></pre></div>
<p>Key Insight: The same learning rate $\alpha$ controls the step size for all parameters, just like in our simple 1D case.</p>
<h4 id="single-hidden-layer-mlp-putting-it-all-together">Single Hidden Layer MLP: Putting It All Together<a class="headerlink" href="#single-hidden-layer-mlp-putting-it-all-together" title="Permanent link">&para;</a></h4>
<p>Forward Pass Equations:</p>
<p><div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>Z_1 = X W_1 + b_1 \quad \text{(shape: samples x hidden units)}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>A_1 = \sigma(Z_1) \quad \text{(apply activation function element-wise)}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>Z_2 = A_1 W_2 + b_2 \quad \text{(shape: samples x outputs)}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>Y_{\text{hat}} = Z_2 \quad \text{(final predictions)}
</code></pre></div></p>
<p>where $\sigma$ is an activation function like ReLU or sigmoid.</p>
<p>Backward Pass (Backpropagation):</p>
<p>Step 1: Compute the error at the output:
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>\delta_2 = Y_{\text{hat}} - Y_{\text{true}} \quad \text{(shape: samples × outputs)}
</code></pre></div></p>
<p>Step 2: Compute gradients for output layer:
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>\frac{\partial L}{\partial W_2} = \frac{1}{N} A_1^T \delta_2
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>\frac{\partial L}{\partial b_2} = \frac{1}{N} \sum_{i=1}^N \delta_2^{(i)}
</code></pre></div></p>
<p>Step 3: Backpropagate error to hidden layer:
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>\delta_1 = (\delta_2 W_2^T) \odot \sigma&#39;(Z_1) \quad \text{(element-wise multiplication)}
</code></pre></div></p>
<p>Step 4: Compute gradients for hidden layer:
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>\frac{\partial L}{\partial W_1} = \frac{1}{N} X^T \delta_1
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>\frac{\partial L}{\partial b_1} = \frac{1}{N} \sum_{i=1}^N \delta_1^{(i)}
</code></pre></div></p>
<p>Step 5: Update all parameters using the same learning rate:
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>W_1 \leftarrow W_1 - \alpha \frac{\partial L}{\partial W_1}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>b_1 \leftarrow b_1 - \alpha \frac{\partial L}{\partial b_1}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>W_2 \leftarrow W_2 - \alpha \frac{\partial L}{\partial W_2}
</code></pre></div>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>b_2 \leftarrow b_2 - \alpha \frac{\partial L}{\partial b_2}
</code></pre></div></p>
<p>Understanding the δ terms:</p>
<ul>
<li>$\delta_2$: "How much does changing each output neuron's value affect the loss?"</li>
<li>$\delta_1$: "How much does changing each hidden neuron's value affect the loss?"</li>
</ul>
<p>The $\delta$ terms flow backwards through the network, carrying error information from the output back to earlier layers.</p>
<h4 id="the-learning-rate-α-universal-step-size-controller">The Learning Rate α: Universal Step Size Controller<a class="headerlink" href="#the-learning-rate-α-universal-step-size-controller" title="Permanent link">&para;</a></h4>
<p>Same Role Everywhere: Notice that $\alpha$ plays the identical role in:</p>
<ul>
<li>1D gradient descent: $x \leftarrow x - \alpha \frac{df}{dx}$</li>
<li>Vector gradient descent: $\mathbf{x} \leftarrow \mathbf{x} - \alpha \nabla f$</li>
<li>Matrix gradient descent: $W \leftarrow W - \alpha \frac{\partial L}{\partial W}$</li>
<li>Neural network training: All parameters use the same $\alpha$</li>
</ul>
<p>Choosing α:</p>
<ul>
<li>Too large: Updates overshoot the minimum, causing oscillation or divergence</li>
<li>Too small: Updates are tiny, causing very slow convergence</li>
<li>Just right: Steady progress toward the minimum without overshooting</li>
</ul>
<h4 id="visual-connection-from-slopes-to-networks">Visual Connection: From Slopes to Networks<a class="headerlink" href="#visual-connection-from-slopes-to-networks" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>1D Slope:          2D Gradient:           Matrix Gradients:        MLP Training:
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>    f(x)             f(x,y)                    L(W,b)                Forward:
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>     /|                 /|\                      /|\                X→Z₁→A₁→Z₂→Ŷ
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>    / |                / | \                    / | \                   
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a>   /  |               /  |  \                  /  |  \               Backward:
<a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a>slope |             ∇f   |  ∇f                ∇L  |   ∇L           δ₂←δ₁←∇W₁,∇b₁
<a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a>   \  |               \  |  /                  \  |  /               
<a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a>    \ |                \ | /                    \ | /                Update:
<a id="__codelineno-25-10" name="__codelineno-25-10" href="#__codelineno-25-10"></a>     \|                 \|/                      \|/               W,b ← W,b-α∇
<a id="__codelineno-25-11" name="__codelineno-25-11" href="#__codelineno-25-11"></a>      x                  x                        θ                 (same α!)
<a id="__codelineno-25-12" name="__codelineno-25-12" href="#__codelineno-25-12"></a>
<a id="__codelineno-25-13" name="__codelineno-25-13" href="#__codelineno-25-13"></a>
<a id="__codelineno-25-14" name="__codelineno-25-14" href="#__codelineno-25-14"></a>Update: x₁ = x₀ - α(df/dx)   [x,y]₁ = [x,y]₀ - α∇f    θ₁ = θ₀ - α∇L    All params use α
</code></pre></div>
<p>The Big Picture: Whether we're finding the bottom of a simple parabola or training a neural network with millions of parameters, we're doing the same fundamental thing:</p>
<ol>
<li><strong>Measure the slope</strong> (derivative, gradient, or backpropagated error)</li>
<li><strong>Take a step in the opposite direction</strong> (negative sign)</li>
<li><strong>Control step size</strong> (learning rate α)</li>
<li><strong>Repeat until we reach the bottom</strong></li>
</ol>
<p>This is why understanding the simple case of line slopes gives us insight into the most sophisticated neural network training algorithms.</p>
<h4 id="212-gradient-fields-and-optimization">2.1.2 Gradient Fields and Optimization<a class="headerlink" href="#212-gradient-fields-and-optimization" title="Permanent link">&para;</a></h4>
<p>Gradient Descent as Continuous Flow: Parameter updates $\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}$ approximate the ODE:
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>\frac{d\theta}{dt} = -\nabla_\theta \mathcal{L}(\theta) \quad (3)
</code></pre></div></p>
<p>Understanding the symbols:</p>
<ul>
<li>$\theta$ (theta): The parameters we want to learn - think of these as the "knobs" we can adjust</li>
<li>$\eta$ (eta): Learning rate - how big steps we take. Like deciding whether to take baby steps or giant leaps when hiking downhill</li>
<li>$\nabla$ (nabla): The gradient symbol - points in the direction of steepest ascent (we go opposite direction to descend)</li>
<li>$\mathcal{L}$ (script L): The loss function - measures how "wrong" our current parameters are</li>
</ul>
<p>What the equation means: "Change the parameters in the opposite direction of the gradient, scaled by the learning rate."</p>
<p>This connects discrete optimization to continuous dynamical systems.</p>
<p>Why This Matters: Understanding optimization as flow helps explain momentum methods, learning rate schedules, and convergence behavior.</p>
<h4 id="213-residual-connections-as-discretized-dynamics">2.1.3 Residual Connections as Discretized Dynamics<a class="headerlink" href="#213-residual-connections-as-discretized-dynamics" title="Permanent link">&para;</a></h4>
<p>Residual Block: $\mathbf{h}_{l+1} = \mathbf{h}_l + F(\mathbf{h}_l)$ approximates:
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a>\frac{d\mathbf{h}}{dt} = F(\mathbf{h}) \quad (4)
</code></pre></div></p>
<p>What residual connections do intuitively: Think of them as "safety nets" for information. Without residual connections, information would have to successfully pass through every layer to reach the output. With residual connections, information can "skip over" layers that might be learning slowly or poorly.</p>
<p>Highway analogy: Imagine driving from city A to city B. Without residual connections, you MUST go through every small town along the way. With residual connections, there's a highway that bypasses some towns - you still visit some towns (transformation), but you're guaranteed to make progress toward your destination even if some towns are roadblocked.</p>
<p>Why this enables deep networks: In very deep networks (50+ layers), gradients tend to vanish as they backpropagate. Residual connections provide a "gradient highway" - gradients can flow directly backward through the skip connections, ensuring that even early layers receive useful training signals.</p>
<p>This enables training very deep networks by maintaining gradient flow.</p>
<p>Stability Consideration: The transformation $F$ should be well-conditioned to avoid exploding/vanishing gradients.</p>
<p>💻 Implementation Example: For a practical implementation of residual connections, see <a href="./pynb/math_ref/advanced_concepts.ipynb">Advanced Concepts Notebook</a></p>
<h3 id="22-deep-learning-mathematics-in-context">2.2 Deep Learning Mathematics in Context<a class="headerlink" href="#22-deep-learning-mathematics-in-context" title="Permanent link">&para;</a></h3>
<h4 id="221-vectors-as-word-meanings">2.2.1 Vectors as Word Meanings<a class="headerlink" href="#221-vectors-as-word-meanings" title="Permanent link">&para;</a></h4>
<p>What vectors represent in transformers: Vectors are not just mathematical objects—they encode semantic meaning. Each word becomes a point in high-dimensional space where:</p>
<ul>
<li>Similar words cluster together: "king" and "queen" vectors point in similar directions</li>
<li>Vector arithmetic captures relationships: "king" - "man" + "woman" ≈ "queen"</li>
<li>Distance measures semantic similarity: Cosine similarity between "cat" and "dog" is higher than between "cat" and "airplane"</li>
</ul>
<p>Why this matters for attention: When transformers compute attention, they're asking "which word meanings are most relevant to understanding this context?" This is fundamentally a similarity search in semantic space.</p>
<h4 id="222-matrices-as-transformations-of-meaning">2.2.2 Matrices as Transformations of Meaning<a class="headerlink" href="#222-matrices-as-transformations-of-meaning" title="Permanent link">&para;</a></h4>
<p>Linear transformations in neural networks:</p>
<ul>
<li>Weight matrices $W$ transform input meanings: $\mathbf{h}<em _text_old="\text{old">{\text{new}} = \mathbf{h}</em> W$}</li>
<li>Multiple transformations compose: $\mathbf{h}_3 = \mathbf{h}_1 W_1 W_2$ applies two sequential meaning transformations</li>
<li>Transpose operations $W^T$ reverse transformations during backpropagation</li>
</ul>
<p>Block matrix operations enable parallel processing:
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>\begin{bmatrix} \mathbf{h}_1 \\ \mathbf{h}_2 \\ \vdots \\ \mathbf{h}_n \end{bmatrix} W = \begin{bmatrix} \mathbf{h}_1 W \\ \mathbf{h}_2 W \\ \vdots \\ \mathbf{h}_n W \end{bmatrix}
</code></pre></div>
This processes entire sequences simultaneously instead of word-by-word.</p>
<h4 id="223-gradients-as-learning-signals">2.2.3 Gradients as Learning Signals<a class="headerlink" href="#223-gradients-as-learning-signals" title="Permanent link">&para;</a></h4>
<p>What gradients mean in neural networks: Gradients tell us "if I adjust this parameter slightly, how much will my prediction error change?" This guides learning:</p>
<ul>
<li>Large gradients: Parameter strongly affects error → make bigger adjustments</li>
<li>Small gradients: Parameter weakly affects error → make smaller adjustments</li>
<li>Zero gradients: Parameter doesn't affect error → don't change it</li>
</ul>
<p>Chain rule enables credit assignment: In deep networks, we need to know how output errors relate to early layer parameters. The chain rule flows error signals backward through the network:
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>\frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_3} \frac{\partial \mathbf{h}_3}{\partial \mathbf{h}_2} \frac{\partial \mathbf{h}_2}{\partial W_1}
</code></pre></div></p>
<h4 id="224-softmax-and-cross-entropy-from-scores-to-decisions">2.2.4 Softmax and Cross-Entropy: From Scores to Decisions<a class="headerlink" href="#224-softmax-and-cross-entropy-from-scores-to-decisions" title="Permanent link">&para;</a></h4>
<p>Softmax converts neural network outputs to probabilities:
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}} \quad (1)
</code></pre></div></p>
<p>Why transformers use this combination:</p>
<ol>
<li><strong>Neural networks output raw scores</strong> (logits) that can be any real number</li>
<li><strong>Softmax normalizes these into probabilities</strong> that sum to 1</li>
<li><strong>Cross-entropy loss measures prediction quality</strong> using these probabilities</li>
</ol>
<p>Cross-Entropy Loss:
<div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a>\mathcal{L} = -\sum_{i=1}^n y_i \log p_i \quad (2)
</code></pre></div></p>
<p>Why cross-entropy is perfect for language modeling:</p>
<ul>
<li>Encourages confident correct predictions: Low loss when p_i ≈ 1 for correct answer</li>
<li>Harshly penalizes confident wrong predictions: High loss when p_i ≈ 0 for correct answer</li>
<li>Provides clean gradients: $\nabla \mathcal{L} = \mathbf{p} - \mathbf{y}$ (predicted - true)</li>
<li>Matches softmax naturally: Both work with probability distributions</li>
</ul>
<p>Example: If the model predicts 90% probability for the correct next word, loss is low. If it predicts 1% probability for the correct next word, loss is very high.</p>
<h2 id="3-multilayer-perceptrons-as-a-warm-up">3. Multilayer Perceptrons as a Warm-Up<a class="headerlink" href="#3-multilayer-perceptrons-as-a-warm-up" title="Permanent link">&para;</a></h2>
<h3 id="31-forward-pass">3.1 Forward Pass<a class="headerlink" href="#31-forward-pass" title="Permanent link">&para;</a></h3>
<p>Two-layer MLP:</p>
<p>Shape Analysis: If input $\mathbf{x} \in \mathbb{R}^{1 \times d_{\text{in}}}$:</p>
<ul>
<li>$W^{(1)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{hidden}}}$</li>
<li>$W^{(2)} \in \mathbb{R}^{d_{\text{hidden}} \times d_{\text{out}}}$</li>
</ul>
<h3 id="32-backpropagation-derivation">3.2 Backpropagation Derivation<a class="headerlink" href="#32-backpropagation-derivation" title="Permanent link">&para;</a></h3>
<p>Loss Gradient w.r.t. Output:
<div class="highlight"><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a>\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(2)}} = \frac{\partial \mathcal{L}}{\partial \hat{\mathbf{y}}} \quad (16)
</code></pre></div></p>
<p>Weight Gradients:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a>\begin{aligned}
<a id="__codelineno-33-2" name="__codelineno-33-2" href="#__codelineno-33-2"></a>\frac{\partial \mathcal{L}}{\partial W^{(2)}} &amp;= (\mathbf{h}^{(1)})^T \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(2)}} \\
<a id="__codelineno-33-3" name="__codelineno-33-3" href="#__codelineno-33-3"></a>\frac{\partial \mathcal{L}}{\partial W^{(1)}} &amp;= \mathbf{x}^T \left[ \left( \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(2)}} W^{(2)T} \right) \odot \sigma&#39;(\mathbf{z}^{(1)}) \right]
<a id="__codelineno-33-4" name="__codelineno-33-4" href="#__codelineno-33-4"></a>\end{aligned}
</code></pre></div>
<ul>
<li>Where $\odot$ denotes element-wise multiplication.</li>
<li>$\sigma'(\mathbf{z}^{(1)})$ is the derivative of the activation function applied elementwise.</li>
<li>$\frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(2)}}$ is the gradient of the loss with respect to the pre-activation output of the second layer.</li>
</ul>
<p>where $\odot$ denotes element-wise multiplication.</p>
<h3 id="33-advanced-normalization-techniques">3.3 Advanced Normalization Techniques<a class="headerlink" href="#33-advanced-normalization-techniques" title="Permanent link">&para;</a></h3>
<p>LayerNorm: Normalizes across features within each sample:
<div class="highlight"><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a>\text{LayerNorm}(\mathbf{x}) = \gamma \odot \frac{\mathbf{x} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta \quad (19)
</code></pre></div></p>
<p>Shape Analysis: For $\mathbf{x} \in \mathbb{R}^{n \times d}$: $\gamma, \beta \in \mathbb{R}^{d}$ (learnable per-feature parameters)</p>
<p>Understanding the Greek letters:</p>
<ul>
<li>$\mu$ (mu): The mean (average) of all the numbers</li>
<li>$\sigma$ (sigma): The standard deviation - how spread out the numbers are from the average</li>
<li>$\gamma$ (gamma): A learnable scale parameter - lets the model decide how much to amplify the result</li>
<li>$\beta$ (beta): A learnable shift parameter - lets the model decide how much to shift the result</li>
<li>$\epsilon$ (epsilon): A tiny number to prevent division by zero</li>
</ul>
<p>What LayerNorm does: "Make all the numbers have zero average and unit variance, then let the model scale and shift them as needed." It's like standardizing test scores so they're all on the same scale.</p>
<p>where $\mu = \frac{1}{d}\sum_{i=1}^d x_i$ and $\sigma^2 = \frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2$.</p>
<p>Why LayerNorm for Sequences: Unlike BatchNorm, it doesn't depend on batch statistics, making it suitable for variable-length sequences.</p>
<h3 id="34-alternative-normalization-methods">3.4 Alternative Normalization Methods<a class="headerlink" href="#34-alternative-normalization-methods" title="Permanent link">&para;</a></h3>
<p>RMSNorm (Root Mean Square Norm): Simplifies LayerNorm by removing the mean:
<div class="highlight"><pre><span></span><code><a id="__codelineno-35-1" name="__codelineno-35-1" href="#__codelineno-35-1"></a>\text{RMSNorm}(\mathbf{x}) = \gamma \odot \frac{\mathbf{x}}{\sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}}
</code></pre></div></p>
<p>Benefits: Faster computation, similar performance to LayerNorm.</p>
<p>Scaled Residuals: In very deep networks, scale residual connections:
<div class="highlight"><pre><span></span><code><a id="__codelineno-36-1" name="__codelineno-36-1" href="#__codelineno-36-1"></a>\mathbf{h}_{l+1} = \mathbf{h}_l + \alpha \cdot F(\mathbf{h}_l)
</code></pre></div>
where $\alpha &lt; 1$ prevents residual explosion.</p>
<p>Pre-LN vs Post-LN:</p>
<ul>
<li>Pre-LN (modern): $\mathbf{h}_{l+1} = \mathbf{h}_l + F(\text{LN}(\mathbf{h}_l))$</li>
<li>Post-LN (original): $\mathbf{h}_{l+1} = \text{LN}(\mathbf{h}_l + F(\mathbf{h}_l))$</li>
</ul>
<p>Pre-LN advantages: Better gradient flow, more stable training, enables training deeper models.</p>
<h2 id="4-high-dimensional-geometry--similarity">4. High-Dimensional Geometry &amp; Similarity<a class="headerlink" href="#4-high-dimensional-geometry--similarity" title="Permanent link">&para;</a></h2>
<h3 id="41-distance-metrics-in-high-dimensions">4.1 Distance Metrics in High Dimensions<a class="headerlink" href="#41-distance-metrics-in-high-dimensions" title="Permanent link">&para;</a></h3>
<p>Euclidean Distance: 
<div class="highlight"><pre><span></span><code><a id="__codelineno-37-1" name="__codelineno-37-1" href="#__codelineno-37-1"></a>d_2(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|_2
</code></pre></div></p>
<p>Cosine Similarity: 
<div class="highlight"><pre><span></span><code><a id="__codelineno-38-1" name="__codelineno-38-1" href="#__codelineno-38-1"></a>\cos(\theta) = \frac{\mathbf{u}^T \mathbf{v}}{\|\mathbf{u}\|_2 \|\mathbf{v}\|_2}
</code></pre></div></p>
<p>Concentration of Measure: In high dimensions, most random vectors are approximately orthogonal, making cosine similarity more discriminative than Euclidean distance.</p>
<h3 id="42-distance-calculations-with-concrete-examples">4.2 Distance Calculations with Concrete Examples<a class="headerlink" href="#42-distance-calculations-with-concrete-examples" title="Permanent link">&para;</a></h3>
<p>Using simple example vectors to illustrate the concepts:</p>
<ul>
<li>"cat" vector: [0.8, 0.2, 0.1]</li>
<li>"dog" vector: [0.7, 0.3, 0.2]</li>
</ul>
<h4 id="421-cosine-similarity">4.2.1 Cosine Similarity<a class="headerlink" href="#421-cosine-similarity" title="Permanent link">&para;</a></h4>
<p>Formula: 
<div class="highlight"><pre><span></span><code><a id="__codelineno-39-1" name="__codelineno-39-1" href="#__codelineno-39-1"></a>\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \times \|\mathbf{B}\|}
</code></pre></div></p>
<p>Step-by-step calculation:</p>
<ol>
<li>Dot product: A·B = (0.8×0.7) + (0.2×0.3) + (0.1×0.2) = 0.56 + 0.06 + 0.02 = 0.64</li>
<li>Magnitudes:</li>
<li>||A|| = √(0.8² + 0.2² + 0.1²) = √(0.64 + 0.04 + 0.01) = √0.69 ≈ 0.83</li>
<li>
<p>||B|| = √(0.7² + 0.3² + 0.2²) = √(0.49 + 0.09 + 0.04) = √0.62 ≈ 0.79</p>
</li>
<li>
<p>Cosine similarity: 0.64 / (0.83 × 0.79) ≈ 0.64 / 0.66 ≈ 0.97</p>
</li>
</ol>
<p>Interpretation: Values range from -1 (opposite) to 1 (identical). 0.97 indicates high similarity - "cat" and "dog" point in nearly the same direction in semantic space.</p>
<h4 id="422-euclidean-distance">4.2.2 Euclidean Distance<a class="headerlink" href="#422-euclidean-distance" title="Permanent link">&para;</a></h4>
<p>Formula: 
<div class="highlight"><pre><span></span><code><a id="__codelineno-40-1" name="__codelineno-40-1" href="#__codelineno-40-1"></a>d = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 + (z_1-z_2)^2}
</code></pre></div></p>
<p>Step-by-step calculation:</p>
<ol>
<li>Differences: (0.8-0.7)² + (0.2-0.3)² + (0.1-0.2)² = 0.01 + 0.01 + 0.01 = 0.03</li>
<li>Distance: √0.03 ≈ 0.17</li>
</ol>
<p>Interpretation: Lower values indicate closer proximity. 0.17 is small, confirming "cat" and "dog" are close in space.</p>
<p>When to use each:</p>
<ul>
<li>Cosine: When direction matters more than magnitude (text similarity, semantic relationships)</li>
<li>Euclidean: When absolute distance matters (image features, exact matching)</li>
</ul>
<h3 id="43-maximum-inner-product-search-mips">4.3 Maximum Inner Product Search (MIPS)<a class="headerlink" href="#43-maximum-inner-product-search-mips" title="Permanent link">&para;</a></h3>
<p>Problem: Find 
<div class="highlight"><pre><span></span><code><a id="__codelineno-41-1" name="__codelineno-41-1" href="#__codelineno-41-1"></a>\mathbf{v}^* = \arg\max_{\mathbf{v} \in \mathcal{V}} \mathbf{q}^T \mathbf{v}
</code></pre></div></p>
<p>This is exactly what attention computes when finding relevant keys for a given query!</p>
<p>Connection to Attention: Query-key similarity in attention is inner product search over learned embeddings.</p>
<p>💻 Implementation Example: For high-dimensional similarity comparisons, see <a href="./pynb/math_ref/vectors_geometry.ipynb">Vectors &amp; Geometry Notebook</a></p>
<h2 id="5-from-similarity-to-attention">5. From Similarity to Attention<a class="headerlink" href="#5-from-similarity-to-attention" title="Permanent link">&para;</a></h2>
<p>📚 Quick Reference: See <a href="../math_quick_ref/#mathematical-quick-reference-for-neural-networks">Scaled Dot-Product Attention</a> in the mathematical reference table.</p>
<h3 id="51-deriving-scaled-dot-product-attention">5.1 Deriving Scaled Dot-Product Attention<a class="headerlink" href="#51-deriving-scaled-dot-product-attention" title="Permanent link">&para;</a></h3>
<p>Step 1: Start with similarity search between query $\mathbf{q}$ and keys ${\mathbf{k}_i}$:
<div class="highlight"><pre><span></span><code><a id="__codelineno-42-1" name="__codelineno-42-1" href="#__codelineno-42-1"></a>s_i = \mathbf{q}^T \mathbf{k}_i \quad (20)
</code></pre></div></p>
<p>Step 2: Convert similarities to weights via softmax:
<div class="highlight"><pre><span></span><code><a id="__codelineno-43-1" name="__codelineno-43-1" href="#__codelineno-43-1"></a>\alpha_i = \frac{e^{s_i}}{\sum_{j=1}^n e^{s_j}} \quad (21)
</code></pre></div></p>
<p>Understanding $\alpha$ (alpha): These are the attention weights - they tell us "how much should I pay attention to each word?" The $\alpha_i$ values all add up to 1, like percentages.</p>
<p>Why use softmax instead of just raw similarities?</p>
<ol>
<li>Probabilities must sum to 1: We want a weighted average, so weights must be between 0 and 1 and sum to 1. Raw similarities could be negative or not sum to 1.</li>
<li>Differentiable selection: Softmax provides a "soft" way to pick the most relevant items. Instead of hard selection (pick the best, ignore the rest), it gives more weight to better matches while still considering others.</li>
<li>Handles different scales: Raw similarity scores might vary wildly in range. Softmax normalizes them into a consistent 0-1 probability scale.</li>
<li>Amplifies differences: The exponential in softmax amplifies differences between scores. If one word is much more relevant, it gets much more attention.</li>
<li>Smooth gradients: Unlike hard max (which would create step functions), softmax is smooth everywhere, enabling gradient-based learning.</li>
</ol>
<p>Real-world analogy: Like deciding how much attention to give each person in a room - you don't ignore everyone except one person (hard max), but you do focus more on the most interesting people while still being somewhat aware of others.</p>
<p>Step 3: Aggregate values using weights:
<div class="highlight"><pre><span></span><code><a id="__codelineno-44-1" name="__codelineno-44-1" href="#__codelineno-44-1"></a>\mathbf{o} = \sum_{i=1}^n \alpha_i \mathbf{v}_i \quad (22)
</code></pre></div></p>
<p>What this step does: Take a weighted average of all the value vectors. It's like saying "give me 30% of word 1's information, 50% of word 2's information, and 20% of word 3's information."</p>
<p>Matrix Form: For sequences, this becomes:
<div class="highlight"><pre><span></span><code><a id="__codelineno-45-1" name="__codelineno-45-1" href="#__codelineno-45-1"></a>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \quad (23)
</code></pre></div></p>
<p>Shape Analysis: $Q \in \mathbb{R}^{n \times d_k}, K \in \mathbb{R}^{n \times d_k}, V \in \mathbb{R}^{n \times d_v} \Rightarrow \text{Output} \in \mathbb{R}^{n \times d_v}$</p>
<p>What this equation accomplishes step-by-step:</p>
<ol>
<li>$QK^T$ creates a "compatibility matrix" - every query checks against every key</li>
<li>$\frac{1}{\sqrt{d_k}}$ scales the scores to prevent them from getting too large</li>
<li>$\text{softmax}$ converts raw compatibility scores into probability-like weights</li>
<li>Multiply by $V$ to get a weighted sum of value vectors</li>
</ol>
<p>Library analogy: Think of attention like a smart librarian. Q is your question ("I need books about neural networks"), K represents the subject tags of all books, V contains the actual book contents. The librarian (attention mechanism) looks at your question, checks which books are most relevant (QK^T), decides how much attention to give each book (softmax), and gives you a summary that's mostly from the most relevant books but includes a little from others (weighted sum with V).</p>
<h3 id="54-masked-attention">5.4 Masked Attention<a class="headerlink" href="#54-masked-attention" title="Permanent link">&para;</a></h3>
<p>Causal Masking: For autoregressive models, prevent attention to future tokens:
<div class="highlight"><pre><span></span><code><a id="__codelineno-46-1" name="__codelineno-46-1" href="#__codelineno-46-1"></a>P = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right) \quad (24)
</code></pre></div></p>
<p>where mask $M_{ij} \in {0, -\infty}$ with $M_{ij} = -\infty$ if $i &lt; j$ (future positions).</p>
<p>Numerical Stability: Instead of $-\infty$, use large negative values (e.g., $-10^9$) to prevent NaN gradients:</p>
<p>💻 Implementation Example: For causal mask implementation, see <a href="./pynb/math_ref/advanced_concepts.ipynb">Advanced Concepts Notebook</a></p>
<p>Padding Masks: Mask out padding tokens by setting their attention scores to $-\infty$ before softmax. This ensures padding tokens receive zero attention weight.</p>
<p>Key Insight: Additive masking (adding $-\infty$) is preferred over multiplicative masking because it works naturally with softmax normalization.</p>
<h3 id="52-why-the-sqrtd_k-scaling">5.2 Why the $\sqrt{d_k}$ Scaling?<a class="headerlink" href="#52-why-the-sqrtd_k-scaling" title="Permanent link">&para;</a></h3>
<p>Variance Analysis: If $Q, K$ have i.i.d. entries with variance $\sigma^2$, then:
<div class="highlight"><pre><span></span><code><a id="__codelineno-47-1" name="__codelineno-47-1" href="#__codelineno-47-1"></a>\text{Var}(QK^T) = d_k \sigma^4 \quad (24)
</code></pre></div></p>
<p>Understanding $\sigma$ (sigma): This represents the variance - how spread out the numbers are. Think of it as measuring "how noisy" or "how varied" the data is.</p>
<p>What "i.i.d." means: Independent and identically distributed - each number is random and doesn't depend on the others, like rolling dice multiple times.</p>
<p>Without scaling, attention weights become too peaked (sharp) as $d_k$ increases, leading to poor gradients and attention collapse.</p>
<p>Pitfall: Forgetting this scaling leads to attention collapse - weights become too concentrated rather than appropriately distributed.</p>
<h3 id="53-backpropagation-through-attention">5.3 Backpropagation Through Attention<a class="headerlink" href="#53-backpropagation-through-attention" title="Permanent link">&para;</a></h3>
<p>Softmax Gradient: For $\mathbf{p} = \text{softmax}(\mathbf{z})$:
<div class="highlight"><pre><span></span><code><a id="__codelineno-48-1" name="__codelineno-48-1" href="#__codelineno-48-1"></a>\frac{\partial p_i}{\partial z_j} = p_i(\delta_{ij} - p_j) \quad (25)
</code></pre></div></p>
<p>where $\delta_{ij}$ is the Kronecker delta.</p>
<p>What is $\delta_{ij}$ (delta)? It's a simple function that equals 1 when i=j (same position) and 0 otherwise. Think of it as asking "are these the same thing?" - if yes, return 1; if no, return 0.</p>
<p>Attention Gradients:</p>
<p>Let $S = QK^T/\sqrt{d_k}$, $P=\mathrm{softmax}(S)$ (row-wise), $O = PV$. Given $G_O=\partial \mathcal{L}/\partial O$:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-49-1" name="__codelineno-49-1" href="#__codelineno-49-1"></a>\begin{aligned}
<a id="__codelineno-49-2" name="__codelineno-49-2" href="#__codelineno-49-2"></a>G_V &amp;= P^T G_O,\quad &amp;&amp;\text{(V same shape as }V\text{)}\\
<a id="__codelineno-49-3" name="__codelineno-49-3" href="#__codelineno-49-3"></a>G_P &amp;= G_O V^T,\\
<a id="__codelineno-49-4" name="__codelineno-49-4" href="#__codelineno-49-4"></a>G_{S,r} &amp;= \big(\mathrm{diag}(P_r) - P_r P_r^T\big)\, G_{P,r}\quad &amp;&amp;\text{(row }r\text{; softmax Jacobian)}\\
<a id="__codelineno-49-5" name="__codelineno-49-5" href="#__codelineno-49-5"></a>G_Q &amp;= G_S K/\sqrt{d_k},\quad G_K = G_S^T Q/\sqrt{d_k}.
<a id="__codelineno-49-6" name="__codelineno-49-6" href="#__codelineno-49-6"></a>\end{aligned}
</code></pre></div>
<p>Parameters: $Q,K\in\mathbb{R}^{n\times d_k}$, $V\in\mathbb{R}^{n\times d_v}$.
Intuition: Backprop splits into (i) linear parts, (ii) softmax Jacobian per row.</p>
<h2 id="6-multi-head-attention--positional-information">6. Multi-Head Attention &amp; Positional Information<a class="headerlink" href="#6-multi-head-attention--positional-information" title="Permanent link">&para;</a></h2>
<p>📚 Quick Reference: See <a href="../math_quick_ref/#mathematical-quick-reference-for-neural-networks">Multi-Head Attention</a> and <a href="../math_quick_ref/#mathematical-quick-reference-for-neural-networks">Positional Encoding</a> in the mathematical reference table.</p>
<h3 id="61-multi-head-as-subspace-projections">6.1 Multi-Head as Subspace Projections<a class="headerlink" href="#61-multi-head-as-subspace-projections" title="Permanent link">&para;</a></h3>
<p>Single Head: Projects to subspace of dimension $d_k = d_{\text{model}}/h$:
<div class="highlight"><pre><span></span><code><a id="__codelineno-50-1" name="__codelineno-50-1" href="#__codelineno-50-1"></a>\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \quad (26)
</code></pre></div></p>
<p>Multi-Head Combination:
<div class="highlight"><pre><span></span><code><a id="__codelineno-51-1" name="__codelineno-51-1" href="#__codelineno-51-1"></a>\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \quad (27)
</code></pre></div></p>
<p>Why multiple heads instead of one big head? Different heads can specialize in different types of relationships:</p>
<ul>
<li>Head 1 might focus on syntax: "What words are grammatically related?"</li>
<li>Head 2 might focus on semantics: "What words are conceptually similar?"</li>
<li>Head 3 might focus on position: "What words are nearby?"</li>
<li>Head 4 might focus on long-range dependencies: "What words are related despite being far apart?"</li>
</ul>
<p>Meeting analogy: Like having different experts in a meeting. Instead of one generalist trying to understand everything, you have specialists: a grammar expert, a meaning expert, a structure expert, etc. Each contributes their perspective, then all insights are combined (concatenated and projected through $W^O$).</p>
<p>Why split the dimension? Rather than having 8 heads each looking at 512-dimensional vectors, we have 8 heads each looking at 64-dimensional projections (512/8=64). This forces each head to focus on a specific subset of features, encouraging specialization.</p>
<p>Implementation Details:</p>
<p>Linear Projections (Not MLPs):
Each head uses simple linear transformations:</p>
<ul>
<li>$W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k}$ where $d_k = d_{\text{model}}/h$</li>
<li>These are single linear layers, not multi-layer perceptrons</li>
<li>Total projection parameters per layer: $3 \times h \times d_{\text{model}} \times d_k = 3d_{\text{model}}^2$</li>
</ul>
<p>Efficient Implementation:
Instead of computing each head separately, implementations often:</p>
<ol>
<li>Concatenate all head projections: $W^Q = [W_1^Q, W_2^Q, ..., W_h^Q] \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$</li>
<li>Compute $Q = XW^Q, K = XW^K, V = XW^V$ in parallel</li>
<li>Reshape tensors to separate heads: $(n, d_{\text{model}}) \to (n, h, d_k)$</li>
<li>Apply attention computation across all heads simultaneously</li>
</ol>
<p>Parameter Analysis:</p>
<ul>
<li>Per-head projections: $3 \times d_{\text{model}} \times d_k = 3 \times d_{\text{model}} \times (d_{\text{model}}/h)$</li>
<li>Total projections: $3d_{\text{model}}^2$ (same as single-head with full dimension)</li>
<li>Output projection: $W^O \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$ adds $d_{\text{model}}^2$ parameters</li>
<li>Total multi-head parameters: $4d_{\text{model}}^2$</li>
</ul>
<h3 id="62-advanced-positional-encodings">6.2 Advanced Positional Encodings<a class="headerlink" href="#62-advanced-positional-encodings" title="Permanent link">&para;</a></h3>
<p>Sinusoidal Encoding: Provides absolute position information:
<div class="highlight"><pre><span></span><code><a id="__codelineno-52-1" name="__codelineno-52-1" href="#__codelineno-52-1"></a>\begin{align}
<a id="__codelineno-52-2" name="__codelineno-52-2" href="#__codelineno-52-2"></a>PE_{(pos,2i)} &amp;= \sin(pos/10000^{2i/d_{\text{model}}}) \quad (28)\\
<a id="__codelineno-52-3" name="__codelineno-52-3" href="#__codelineno-52-3"></a>PE_{(pos,2i+1)} &amp;= \cos(pos/10000^{2i/d_{\text{model}}}) \quad (29)
<a id="__codelineno-52-4" name="__codelineno-52-4" href="#__codelineno-52-4"></a>\end{align}
</code></pre></div></p>
<p>Mathematical Properties of Sinusoidal Encoding:</p>
<ul>
<li>Linearity: For any fixed offset $k$, $PE_{pos+k}$ can be expressed as a linear function of $PE_{pos}$</li>
<li>Relative position encoding: The dot product $PE_{pos_i} \cdot PE_{pos_j}$ depends only on $|pos_i - pos_j|$</li>
<li>Extrapolation: Can handle sequences longer than seen during training</li>
</ul>
<p>RoPE (Rotary Position Embedding): Rotates query-key pairs by position-dependent angles:
<div class="highlight"><pre><span></span><code><a id="__codelineno-53-1" name="__codelineno-53-1" href="#__codelineno-53-1"></a>\begin{align}
<a id="__codelineno-53-2" name="__codelineno-53-2" href="#__codelineno-53-2"></a>\mathbf{q}_m^{(i)} &amp;= R_{\Theta,m}^{(i)} \mathbf{q}^{(i)} \quad (30)\\
<a id="__codelineno-53-3" name="__codelineno-53-3" href="#__codelineno-53-3"></a>\mathbf{k}_n^{(i)} &amp;= R_{\Theta,n}^{(i)} \mathbf{k}^{(i)} \quad (31)
<a id="__codelineno-53-4" name="__codelineno-53-4" href="#__codelineno-53-4"></a>\end{align}
</code></pre></div></p>
<p>RoPE Rotation Matrix:
<div class="highlight"><pre><span></span><code><a id="__codelineno-54-1" name="__codelineno-54-1" href="#__codelineno-54-1"></a>R_{\Theta,m}^{(i)} = \begin{pmatrix}
<a id="__codelineno-54-2" name="__codelineno-54-2" href="#__codelineno-54-2"></a>\cos(m\theta_i) &amp; -\sin(m\theta_i) \\
<a id="__codelineno-54-3" name="__codelineno-54-3" href="#__codelineno-54-3"></a>\sin(m\theta_i) &amp; \cos(m\theta_i)
<a id="__codelineno-54-4" name="__codelineno-54-4" href="#__codelineno-54-4"></a>\end{pmatrix}
</code></pre></div></p>
<p>where $\theta_i = 10000^{-2i/d_{\text{model}}}$ for dimension pairs.</p>
<p>Key RoPE Properties:</p>
<ul>
<li>Relative position dependency: $\mathbf{q}_m^T \mathbf{k}_n$ depends only on $(m-n)$, not absolute positions</li>
<li>Length preservation: Rotation matrices preserve vector norms</li>
<li>Computational efficiency: Can be implemented without explicit matrix multiplication</li>
<li>Long sequence generalization: Better extrapolation to longer sequences than learned embeddings</li>
</ul>
<p>RoPE Implementation Insight: Instead of rotating the full vectors, RoPE applies rotations to consecutive dimension pairs, enabling efficient implementation via element-wise operations.</p>
<p>💻 Implementation Example: For RoPE (Rotary Position Embedding) implementation, see <a href="./pynb/math_ref/advanced_concepts.ipynb">Advanced Concepts Notebook</a></p>
<h3 id="63-alternative-position-encodings">6.3 Alternative Position Encodings<a class="headerlink" href="#63-alternative-position-encodings" title="Permanent link">&para;</a></h3>
<p>ALiBi (Attention with Linear Biases): Adds position-dependent bias to attention scores:
<div class="highlight"><pre><span></span><code><a id="__codelineno-55-1" name="__codelineno-55-1" href="#__codelineno-55-1"></a>\text{ALiBi-Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + \text{bias}_{ij}\right)V
</code></pre></div></p>
<p>where $\text{bias}_{ij} = -m \cdot |i - j|$ and $m$ is a head-specific slope.</p>
<p>Benefits of ALiBi:</p>
<ul>
<li>No position embeddings needed</li>
<li>Excellent extrapolation to longer sequences</li>
<li>Linear relationship between position distance and attention bias</li>
</ul>
<p>T5 Relative Position Bias: Learns relative position embeddings:
<div class="highlight"><pre><span></span><code><a id="__codelineno-56-1" name="__codelineno-56-1" href="#__codelineno-56-1"></a>A_{ij} = \frac{q_i^T k_j}{\sqrt{d_k}} + b_{\text{rel}(i,j)}
</code></pre></div></p>
<p>where $b_{\text{rel}(i,j)}$ is a learned bias based on relative distance $\text{rel}(i,j)$.</p>
<p>RoPE Scaling Variants:</p>
<ul>
<li>Base scaling: Increase base frequency: $\theta_i = \alpha^{-2i/d} \cdot 10000^{-2i/d}$</li>
<li>NTK scaling: Interpolate frequencies for better long-context performance</li>
<li>When to use: Base scaling for modest extensions (2-4x), NTK for extreme length</li>
</ul>
<h2 id="7-transformer-block-mathematics">7. Transformer Block Mathematics<a class="headerlink" href="#7-transformer-block-mathematics" title="Permanent link">&para;</a></h2>
<h3 id="71-complete-block-equations">7.1 Complete Block Equations<a class="headerlink" href="#71-complete-block-equations" title="Permanent link">&para;</a></h3>
<p>Pre-LayerNorm Architecture:
<div class="highlight"><pre><span></span><code><a id="__codelineno-57-1" name="__codelineno-57-1" href="#__codelineno-57-1"></a>\begin{align}
<a id="__codelineno-57-2" name="__codelineno-57-2" href="#__codelineno-57-2"></a>\mathbf{h}_1 &amp;= \text{LayerNorm}(\mathbf{x}) \quad (32)\\
<a id="__codelineno-57-3" name="__codelineno-57-3" href="#__codelineno-57-3"></a>\mathbf{h}_2 &amp;= \mathbf{x} + \text{MultiHeadAttn}(\mathbf{h}_1, \mathbf{h}_1, \mathbf{h}_1) \quad (33)\\
<a id="__codelineno-57-4" name="__codelineno-57-4" href="#__codelineno-57-4"></a>\mathbf{h}_3 &amp;= \text{LayerNorm}(\mathbf{h}_2) \quad (34)\\
<a id="__codelineno-57-5" name="__codelineno-57-5" href="#__codelineno-57-5"></a>\mathbf{y} &amp;= \mathbf{h}_2 + \text{FFN}(\mathbf{h}_3) \quad (35)
<a id="__codelineno-57-6" name="__codelineno-57-6" href="#__codelineno-57-6"></a>\end{align}
</code></pre></div></p>
<p>Feed-Forward Network:
<div class="highlight"><pre><span></span><code><a id="__codelineno-58-1" name="__codelineno-58-1" href="#__codelineno-58-1"></a>\text{FFN}(\mathbf{x}) = \text{GELU}(\mathbf{x}W_1 + \mathbf{b}_1)W_2 + \mathbf{b}_2 \quad (36)
</code></pre></div></p>
<p>Shape Analysis: $\mathbf{x} \in \mathbb{R}^{n \times d_{\text{model}}}, W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ffn}}}, W_2 \in \mathbb{R}^{d_{\text{ffn}} \times d_{\text{model}}}$</p>
<p>What the FFN does intuitively: Think of it as a "thinking step" for each word individually. After attention has mixed information between words, the FFN lets each word "process" and transform its representation. It's like giving each word some individual processing time to digest the information it received from other words.</p>
<p>Why expand then contract? The FFN first expands the representation to a higher dimension (usually 4× larger), applies a nonlinearity, then contracts back down. This is like having a "working space" where the model can perform more complex computations before producing the final result.</p>
<p>Shape Tracking: For input $\mathbf{x} \in \mathbb{R}^{n \times d_{\text{model}}}$:</p>
<ul>
<li>$W_1 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ffn}}}$ (typically $d_{\text{ffn}} = 4d_{\text{model}}$) - "expansion layer"</li>
<li>$W_2 \in \mathbb{R}^{d_{\text{ffn}} \times d_{\text{model}}}$ - "contraction layer"</li>
</ul>
<h3 id="72-why-gelu-over-relu">7.2 Why GELU over ReLU?<a class="headerlink" href="#72-why-gelu-over-relu" title="Permanent link">&para;</a></h3>
<p>GELU Definition:
<div class="highlight"><pre><span></span><code><a id="__codelineno-59-1" name="__codelineno-59-1" href="#__codelineno-59-1"></a>\text{GELU}(x) = x \cdot \Phi(x) \quad (37)
</code></pre></div></p>
<p>What GELU does intuitively: GELU is like a "smooth switch." Unlike ReLU which harshly cuts off negative values to zero, GELU gradually transitions from "mostly off" to "mostly on." It asks "how much should I activate this neuron?" and gives a smooth answer between 0 and the input value.</p>
<p>Why smoother is better:</p>
<ul>
<li>Better gradients: ReLU has a sharp corner at zero (gradient jumps from 0 to 1). GELU is smooth everywhere, so gradients flow better during training.</li>
<li>Probabilistic interpretation: GELU can be seen as randomly dropping out inputs based on their value - inputs closer to the mean of a normal distribution are more likely to "survive."</li>
<li>More expressive: The smooth transition lets the model make more nuanced decisions about what information to keep.</li>
</ul>
<p>Real-world analogy: ReLU is like a light switch (on/off), while GELU is like a dimmer switch (gradual control).</p>
<p>where $\Phi(x)$ is the standard normal CDF (cumulative distribution function - tells you the probability that a normal random variable is less than x). GELU provides smoother gradients than ReLU, improving optimization.</p>
<h2 id="8-training-objective--tokenizationembeddings">8. Training Objective &amp; Tokenization/Embeddings<a class="headerlink" href="#8-training-objective--tokenizationembeddings" title="Permanent link">&para;</a></h2>
<h3 id="81-next-token-prediction">8.1 Next-Token Prediction<a class="headerlink" href="#81-next-token-prediction" title="Permanent link">&para;</a></h3>
<p>Autoregressive Objective:
<div class="highlight"><pre><span></span><code><a id="__codelineno-60-1" name="__codelineno-60-1" href="#__codelineno-60-1"></a>\mathcal{L} = -\sum_{t=1}^T \log P(x_t | x_1, ..., x_{t-1}) \quad (38)
</code></pre></div></p>
<p>Shape Analysis: For batch size $B$, sequence length $T$: loss computed per sequence, averaged over batch</p>
<p>Implementation: Use causal mask in attention to prevent information leakage from future tokens.</p>
<h3 id="82-embedding-mathematics">8.2 Embedding Mathematics<a class="headerlink" href="#82-embedding-mathematics" title="Permanent link">&para;</a></h3>
<p>Token Embeddings: Map discrete tokens to continuous vectors:
<div class="highlight"><pre><span></span><code><a id="__codelineno-61-1" name="__codelineno-61-1" href="#__codelineno-61-1"></a>\mathbf{e}_i = E[i] \in \mathbb{R}^{d_{\text{model}}} \quad (39)
</code></pre></div></p>
<p>where $E \in \mathbb{R}^{V \times d_{\text{model}}}$ is the embedding matrix.</p>
<p>Weight Tying: Share embedding matrix $E$ with output projection to reduce parameters:
<div class="highlight"><pre><span></span><code><a id="__codelineno-62-1" name="__codelineno-62-1" href="#__codelineno-62-1"></a>P(w_t | \text{context}) = \text{softmax}(\mathbf{h}_t E^T) \quad (40)
</code></pre></div></p>
<p>Shape Analysis: For $\mathbf{h}<em _text_model="\text{model">t \in \mathbb{R}^{1 \times d</em>$:}}}$ and $E \in \mathbb{R}^{V \times d_{\text{model}}</p>
<ul>
<li>$\mathbf{h}_t E^T \in \mathbb{R}^{1 \times V}$ (logits over vocabulary)</li>
<li>Consistent with row-vector convention</li>
</ul>
<p>Perplexity: Measures model uncertainty:</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-63-1" name="__codelineno-63-1" href="#__codelineno-63-1"></a>\mathrm{PPL} = \exp\left(-\frac{1}{T} \sum_{t=1}^{T} \log P\left(x_t \mid x_{&lt;t}\right) \right) \quad (41)
</code></pre></div>
<blockquote>
<p>Note: This equation may not render correctly in GitHub. Use a Markdown viewer!</p>
</blockquote>
<h2 id="9-worked-mini-examples">9. Worked Mini-Examples<a class="headerlink" href="#9-worked-mini-examples" title="Permanent link">&para;</a></h2>
<h3 id="91-tiny-attention-forward-pass">9.1 Tiny Attention Forward Pass<a class="headerlink" href="#91-tiny-attention-forward-pass" title="Permanent link">&para;</a></h3>
<p>Setup: $n=2$ tokens, $d_k=d_v=3$, single head.</p>
<p>Input:
<div class="highlight"><pre><span></span><code><a id="__codelineno-64-1" name="__codelineno-64-1" href="#__codelineno-64-1"></a>Q = [[1, 0, 1],    K = [[1, 1, 0],    V = [[2, 0, 1],
<a id="__codelineno-64-2" name="__codelineno-64-2" href="#__codelineno-64-2"></a>     [0, 1, 1]]         [1, 0, 1]]         [1, 1, 0]]
</code></pre></div></p>
<p>Step 1: Compute raw scores $QK^T$:
<div class="highlight"><pre><span></span><code><a id="__codelineno-65-1" name="__codelineno-65-1" href="#__codelineno-65-1"></a>QK^T = \begin{bmatrix}1 &amp; 0 &amp; 1\\0 &amp; 1 &amp; 1\end{bmatrix} \begin{bmatrix}1 &amp; 1\\1 &amp; 0\\0 &amp; 1\end{bmatrix} = \begin{bmatrix}1 &amp; 2\\1 &amp; 1\end{bmatrix}
</code></pre></div></p>
<p>Step 2: Scale by $1/\sqrt{d_k} = 1/\sqrt{3} \approx 0.577$:
<div class="highlight"><pre><span></span><code><a id="__codelineno-66-1" name="__codelineno-66-1" href="#__codelineno-66-1"></a>S = \frac{QK^T}{\sqrt{3}} = \begin{bmatrix}0.577 &amp; 1.155\\0.577 &amp; 0.577\end{bmatrix}
</code></pre></div></p>
<p>Step 3: Apply softmax (row-wise, rounded to 3 d.p.):</p>
<ul>
<li>Row 1: $e^{0.577} = 1.781, e^{1.155} = 3.173$, sum $= 4.954$</li>
<li>Row 2: $e^{0.577} = 1.781, e^{0.577} = 1.781$, sum $= 3.562$</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-67-1" name="__codelineno-67-1" href="#__codelineno-67-1"></a>A = \begin{bmatrix}
<a id="__codelineno-67-2" name="__codelineno-67-2" href="#__codelineno-67-2"></a>0.359 &amp; 0.641 \\
<a id="__codelineno-67-3" name="__codelineno-67-3" href="#__codelineno-67-3"></a>0.500 &amp; 0.500
<a id="__codelineno-67-4" name="__codelineno-67-4" href="#__codelineno-67-4"></a>\end{bmatrix}
</code></pre></div>
<p>Step 4: Compute output $O = AV$:
<div class="highlight"><pre><span></span><code><a id="__codelineno-68-1" name="__codelineno-68-1" href="#__codelineno-68-1"></a>O = \begin{bmatrix}0.359 &amp; 0.641\\0.500 &amp; 0.500\end{bmatrix} \begin{bmatrix}2 &amp; 0 &amp; 1\\1 &amp; 1 &amp; 0\end{bmatrix} = \begin{bmatrix}1.359 &amp; 0.641 &amp; 0.359\\1.500 &amp; 0.500 &amp; 0.500\end{bmatrix}
</code></pre></div></p>
<p>💻 Implementation Example: For attention computation verification, see <a href="./pynb/math_ref/advanced_concepts.ipynb">Advanced Concepts Notebook</a></p>
<h3 id="92-backprop-through-simple-attention">9.2 Backprop Through Simple Attention<a class="headerlink" href="#92-backprop-through-simple-attention" title="Permanent link">&para;</a></h3>
<p>Given: 
<div class="highlight"><pre><span></span><code><a id="__codelineno-69-1" name="__codelineno-69-1" href="#__codelineno-69-1"></a>\frac{\partial \mathcal{L}}{\partial O} = \begin{bmatrix}1 &amp; 0 &amp; 1\\0 &amp; 1 &amp; 0\end{bmatrix}
</code></pre></div></p>
<p>Gradient w.r.t. Values:
<div class="highlight"><pre><span></span><code><a id="__codelineno-70-1" name="__codelineno-70-1" href="#__codelineno-70-1"></a>\frac{\partial \mathcal{L}}{\partial V} = A^T \frac{\partial \mathcal{L}}{\partial O} = \begin{bmatrix}0.359 &amp; 0.500\\0.641 &amp; 0.500\end{bmatrix}\begin{bmatrix}1 &amp; 0 &amp; 1\\0 &amp; 1 &amp; 0\end{bmatrix} = \begin{bmatrix}0.359 &amp; 0.500 &amp; 0.359\\0.641 &amp; 0.500 &amp; 0.641\end{bmatrix}
</code></pre></div></p>
<p>💻 Implementation Example: For gradient verification using finite differences, see <a href="./pynb/math_ref/advanced_concepts.ipynb">Advanced Concepts Notebook</a></p>
<p>Check for Understanding: Verify that gradient shapes match parameter shapes and that the chain rule is applied correctly.</p>
<hr />
<h2 id="continuing-to-advanced-topics">Continuing to Advanced Topics<a class="headerlink" href="#continuing-to-advanced-topics" title="Permanent link">&para;</a></h2>
<p>This concludes Part 1 of the mathematics tutorial, covering the foundational concepts needed to understand how Transformers work. You now understand:</p>
<ol>
<li><strong>Mathematical foundations</strong> - from basic calculus to gradient descent</li>
<li><strong>Attention as similarity search</strong> - how Q/K/V naturally emerge</li>
<li><strong>Multi-head attention</strong> - parallel specialized attention patterns</li>
<li><strong>Transformer blocks</strong> - combining attention with feed-forward networks</li>
<li><strong>Training objectives</strong> - next-token prediction and embeddings</li>
</ol>
<p>Continue to <a href="../transformers_math2/">Part 2: Advanced Concepts and Scaling</a> to learn about:</p>
<ul>
<li>Advanced optimization techniques (Adam, learning rate schedules)</li>
<li>Efficient attention implementations (FlashAttention, KV caching)</li>
<li>Regularization and generalization techniques</li>
<li>Implementation best practices and common pitfalls</li>
<li>Scaling laws and practical considerations</li>
</ul>
<p>The mathematical foundation you've built here will serve you well as we explore more sophisticated training techniques and efficiency optimizations in Part 2.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.sections", "navigation.top", "navigation.instant", "content.code.copy", "content.tabs.link", "toc.follow"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="../javascripts/mathjax-init.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../javascripts/mathjax-refresh.js"></script>
      
    
  </body>
</html>