{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Recurrent Neural Networks: From Vanilla RNNs to LSTMs\n\n## ðŸŽ¯ Introduction\n\nWelcome to the world of sequential processing! This notebook will take you through the evolution of recurrent neural networks, from simple vanilla RNNs to sophisticated LSTMs and GRUs. Understanding RNNs is crucial for appreciating why transformers were such a breakthrough.\n\n### ðŸ§  What You'll Understand\n\nThis comprehensive guide covers:\n- **Vanilla RNNs**: The basic recurrent mechanism and why it struggles\n- **The vanishing gradient problem**: Why deep-in-time training fails\n- **LSTM architecture**: How gates solve the memory problem\n- **GRU simplification**: A streamlined alternative to LSTMs\n- **Bidirectional processing**: Using context from both directions\n\n### ðŸŽ“ Prerequisites\n\n- Solid understanding of MLPs and backpropagation\n- Familiarity with sequence modeling concepts\n- Basic knowledge of PyTorch modules and training loops\n- Understanding of gradient flow in neural networks\n\n### ðŸš€ Why RNNs Were Revolutionary\n\nRNNs introduced several key innovations:\n- **Sequential processing**: Handle variable-length sequences naturally\n- **Parameter sharing**: Same weights across all time steps\n- **Memory mechanism**: Hidden state carries information through time\n- **Temporal modeling**: Understand patterns that unfold over time\n- **Contextual understanding**: Each output depends on entire history\n\n---\n\n## ðŸ“š Table of Contents\n\n1. **[Vanilla RNN Mechanics](#vanilla-rnn-mechanics)** - Understanding the basic recurrent mechanism\n2. **[The Vanishing Gradient Problem](#vanishing-gradient-problem)** - Why simple RNNs struggle with long sequences\n3. **[LSTM Architecture Deep Dive](#lstm-architecture-deep-dive)** - How gates enable long-term memory\n4. **[GRU: Simplified Gating](#gru-simplified-gating)** - A streamlined alternative to LSTMs\n5. **[Bidirectional RNNs](#bidirectional-rnns)** - Using context from both directions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Vanilla RNN Mechanics\n\n### ðŸ”„ The Basic Recurrent Mechanism\n\nVanilla RNNs introduced the revolutionary idea of memory in neural networks. Unlike feedforward networks that process inputs independently, RNNs maintain a hidden state that gets updated at each time step, allowing them to remember past information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# VANILLA RNN: THE FOUNDATION OF SEQUENTIAL MODELING\n# =============================================================================\n\nprint(\"ðŸ”„ Vanilla RNN Implementation\")\nprint(\"=\" * 50)\n\nclass VanillaRNN(nn.Module):\n    \"\"\"\n    Basic RNN implementation to understand the core mechanism.\n    \n    The key insight: At each time step, combine current input with\n    previous hidden state to produce new hidden state.\n    \"\"\"\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        \n        self.hidden_size = hidden_size\n        \n        # Weight matrices for RNN computation\n        # W_ih: input-to-hidden transformation\n        # W_hh: hidden-to-hidden transformation (the \"recurrent\" part)\n        # W_ho: hidden-to-output transformation\n        self.W_ih = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.W_ho = nn.Linear(hidden_size, output_size)\n        \n        # Activation function (tanh is traditional for vanilla RNNs)\n        self.activation = nn.Tanh()\n    \n    def forward(self, input_sequence, h_0=None):\n        \"\"\"\n        Process a sequence through the RNN.\n        \n        Args:\n            input_sequence: [batch_size, seq_len, input_size]\n            h_0: Initial hidden state [batch_size, hidden_size] (optional)\n        \n        Returns:\n            outputs: [batch_size, seq_len, output_size]\n            final_hidden: [batch_size, hidden_size]\n        \"\"\"\n        batch_size, seq_len, input_size = input_sequence.shape\n        \n        # Initialize hidden state if not provided\n        if h_0 is None:\n            h_t = torch.zeros(batch_size, self.hidden_size)\n        else:\n            h_t = h_0\n        \n        outputs = []\n        hidden_states = [h_t]  # Track hidden state evolution\n        \n        print(f\"Processing sequence: batch_size={batch_size}, seq_len={seq_len}\")\n        \n        # Process each time step sequentially (this is the key limitation!)\n        for t in range(seq_len):\n            # Get input at current time step\n            x_t = input_sequence[:, t, :]  # [batch_size, input_size]\n            \n            # Core RNN computation: h_t = tanh(W_ih * x_t + W_hh * h_{t-1})\n            # This is where the \"memory\" happens - h_t depends on h_{t-1}\n            input_contribution = self.W_ih(x_t)      # Input â†’ Hidden\n            hidden_contribution = self.W_hh(h_t)     # Previous Hidden â†’ Current Hidden\n            \n            # Combine input and hidden contributions\n            h_t = self.activation(input_contribution + hidden_contribution)\n            \n            # Compute output at this time step\n            y_t = self.W_ho(h_t)  # Hidden â†’ Output\n            \n            outputs.append(y_t)\n            hidden_states.append(h_t)\n            \n            if t < 3:  # Show first few steps in detail\n                print(f\"  Step {t}: input {x_t.shape} + hidden {h_t.shape} â†’ output {y_t.shape}\")\n        \n        # Stack outputs across time dimension\n        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len, output_size]\n        \n        print(f\"Final output shape: {outputs.shape}\")\n        \n        return outputs, h_t, hidden_states\n\n# Demonstrate RNN with a simple sequence\nprint(f\"\\nðŸŽ¯ RNN in Action\")\nprint(\"=\" * 50)\n\n# Create a simple RNN\nbatch_size, seq_len, input_size = 2, 5, 3\nhidden_size, output_size = 4, 2\n\nrnn = VanillaRNN(input_size, hidden_size, output_size)\n\n# Create a simple input sequence\n# Let's use a sequence where each time step has increasing values\ninput_seq = torch.randn(batch_size, seq_len, input_size)\n\nprint(f\"Input sequence shape: {input_seq.shape}\")\nprint(f\"Network: {input_size} â†’ {hidden_size} â†’ {output_size}\")\n\n# Process through RNN\noutputs, final_hidden, hidden_states = rnn(input_seq)\n\nprint(f\"\\nðŸ“Š RNN Processing Analysis\")\nprint(\"=\" * 50)\n\nprint(f\"Hidden state evolution (shape {hidden_states[0].shape}):\")\nprint(\"Time | Hidden State Mean | Hidden State Std\")\nprint(\"-----|-------------------|------------------\")\n\nfor t, h in enumerate(hidden_states):\n    mean_val = h.mean().item()\n    std_val = h.std().item()\n    print(f\"{t:4d} | {mean_val:16.4f} | {std_val:16.4f}\")\n\nprint(f\"\\nðŸ’¡ Key RNN Properties\")\nprint(\"=\" * 50)\nprint(\"1. **Sequential Processing**: Must process t=0 before t=1, etc.\")\nprint(\"2. **Parameter Sharing**: Same W_ih, W_hh, W_ho for all time steps\")\nprint(\"3. **Variable Length**: Can handle any sequence length\")\nprint(\"4. **Memory**: Hidden state h_t carries information from all previous steps\")\nprint(\"5. **Bottleneck**: All history compressed into fixed-size hidden state\")\n\nprint(f\"\\nâš ï¸ Vanilla RNN Limitations\")\nprint(\"=\" * 50)\nprint(\"1. **Vanishing Gradients**: Hard to learn long-term dependencies\")\nprint(\"2. **No Parallelization**: Sequential nature prevents parallel computation\")\nprint(\"3. **Memory Overwriting**: New information can erase old information\")\nprint(\"4. **Gradient Explosion**: Gradients can grow exponentially\")\nprint(\"5. **Limited Context**: Fixed hidden size limits memory capacity\")\n\n# Demonstrate the parameter sharing aspect\nprint(f\"\\nðŸ” Parameter Sharing Demonstration\")\nprint(\"=\" * 50)\n\ntotal_params = sum(p.numel() for p in rnn.parameters())\nprint(f\"Total parameters: {total_params}\")\nprint(f\"Parameters used at EVERY time step - this is efficient!\")\n\nprint(f\"\\nParameter breakdown:\")\nprint(f\"W_ih (inputâ†’hidden): {rnn.W_ih.weight.shape} = {rnn.W_ih.weight.numel()} params\")\nprint(f\"W_hh (hiddenâ†’hidden): {rnn.W_hh.weight.shape} = {rnn.W_hh.weight.numel()} params\")\nprint(f\"W_ho (hiddenâ†’output): {rnn.W_ho.weight.shape} = {rnn.W_ho.weight.numel()} params\")\n\nprint(f\"\\nâœ… RNN achieves memory with constant parameters!\")\nprint(f\"Sequence length doesn't affect parameter count.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Sequence Classifier with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        super(SequenceClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM returns output for all time steps and final hidden state\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Use the last hidden state for classification\n",
    "        # hidden shape: (n_layers, batch, hidden_dim)\n",
    "        last_hidden = hidden[-1]  # Take last layer: (batch, hidden_dim)\n",
    "        \n",
    "        # Apply dropout and classify\n",
    "        output = self.fc(self.dropout(last_hidden))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create synthetic sequence data for sentiment analysis\n",
    "def create_synthetic_sequences(vocab_size=100, n_samples=1000, min_len=5, max_len=20):\n",
    "    \"\"\"Create synthetic sequences with binary sentiment labels\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Random sequence length\n",
    "        length = torch.randint(min_len, max_len + 1, (1,)).item()\n",
    "        \n",
    "        # Generate random sequence\n",
    "        seq = torch.randint(1, vocab_size, (length,))  # Start from 1 (0 is padding)\n",
    "        \n",
    "        # Simple rule: if sum of tokens is even -> positive (1), else negative (0)\n",
    "        label = int(seq.sum().item() % 2)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sequences, torch.tensor(labels)\n",
    "\n",
    "# Generate data\n",
    "vocab_size = 50\n",
    "sequences, labels = create_synthetic_sequences(vocab_size, n_samples=1000)\n",
    "\n",
    "print(f\"Generated {len(sequences)} sequences\")\n",
    "print(f\"Example sequence: {sequences[0]}\")\n",
    "print(f\"Example label: {labels[0]}\")\n",
    "print(f\"Label distribution: {torch.bincount(labels)}\")\n",
    "\n",
    "# Pad sequences to same length\n",
    "def collate_batch(sequences, labels, pad_token=0):\n",
    "    \"\"\"Pad sequences and create batch\"\"\"\n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=pad_token)\n",
    "    return padded_sequences, labels\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_sequences = sequences[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "val_sequences = sequences[train_size:]\n",
    "val_labels = labels[train_size:]\n",
    "\n",
    "# Create batches\n",
    "train_X, train_y = collate_batch(train_sequences, train_labels)\n",
    "val_X, val_y = collate_batch(val_sequences, val_labels)\n",
    "\n",
    "print(f\"\\nTrain data shape: {train_X.shape}, {train_y.shape}\")\n",
    "print(f\"Val data shape: {val_X.shape}, {val_y.shape}\")\n",
    "\n",
    "# Create model\n",
    "model = SequenceClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=32,\n",
    "    hidden_dim=64,\n",
    "    output_dim=2,  # Binary classification\n",
    "    n_layers=2\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_X, train_y, val_X, val_y, epochs=30):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(train_X)\n",
    "        loss = criterion(outputs, train_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        with torch.no_grad():\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_acc = (predicted == train_y).sum().item() / len(train_y)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_X)\n",
    "            val_loss = criterion(val_outputs, val_y)\n",
    "            \n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_acc = (val_predicted == val_y).sum().item() / len(val_y)\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Train Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "    model, train_X, train_y, val_X, val_y, epochs=30\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal validation accuracy: {val_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Length Sequence Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient variable length sequence processing\n",
    "def create_variable_length_batch():\n",
    "    \"\"\"Create a batch with different sequence lengths\"\"\"\n",
    "    sequences = [\n",
    "        torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]),      # length 8\n",
    "        torch.tensor([10, 11, 12, 13]),               # length 4  \n",
    "        torch.tensor([20, 21, 22, 23, 24, 25]),      # length 6\n",
    "        torch.tensor([30, 31]),                       # length 2\n",
    "        torch.tensor([40, 41, 42, 43, 44])           # length 5\n",
    "    ]\n",
    "    \n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    return sequences, lengths\n",
    "\n",
    "sequences, lengths = create_variable_length_batch()\n",
    "print(\"Original sequences:\")\n",
    "for i, (seq, length) in enumerate(zip(sequences, lengths)):\n",
    "    print(f\"Seq {i}: {seq.tolist()} (length: {length})\")\n",
    "\n",
    "# Method 1: Simple padding (inefficient)\n",
    "print(\"\\n=== Method 1: Simple Padding ===\")\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")\n",
    "print(\"Padded sequences:\")\n",
    "print(padded_sequences.numpy())\n",
    "\n",
    "# Method 2: Packed sequences (efficient)\n",
    "print(\"\\n=== Method 2: Packed Sequences ===\")\n",
    "\n",
    "class EfficientLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EfficientLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, sequences, lengths):\n",
    "        # Pack padded sequences\n",
    "        packed = pack_padded_sequence(\n",
    "            sequences, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Run LSTM on packed sequences\n",
    "        packed_output, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack sequences\n",
    "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last valid output for each sequence\n",
    "        batch_size = output.size(0)\n",
    "        last_outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get the last valid time step for this sequence\n",
    "            last_idx = output_lengths[i] - 1\n",
    "            last_outputs.append(output[i, last_idx, :])\n",
    "        \n",
    "        last_outputs = torch.stack(last_outputs)\n",
    "        \n",
    "        # Final classification\n",
    "        return self.fc(last_outputs)\n",
    "\n",
    "# Create embeddings for our sequences (treat as token indices)\n",
    "vocab_size = 50\n",
    "embed_dim = 16\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# Convert sequences to embeddings\n",
    "embedded_sequences = [embedding(seq) for seq in sequences]\n",
    "embedded_padded = pad_sequence(embedded_sequences, batch_first=True)\n",
    "\n",
    "print(f\"Embedded padded shape: {embedded_padded.shape}\")  # (batch, max_seq_len, embed_dim)\n",
    "\n",
    "# Test efficient LSTM\n",
    "efficient_model = EfficientLSTM(embed_dim, 32, 2)\n",
    "output = efficient_model(embedded_padded, lengths)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # (batch_size, output_size)\n",
    "print(\"Efficient processing completed - no computation wasted on padding!\")\n",
    "\n",
    "# Show the difference in computation\n",
    "print(f\"\\nTotal padded length: {padded_sequences.shape[0] * padded_sequences.shape[1]}\")\n",
    "print(f\"Total actual length: {sum(lengths)}\")\n",
    "print(f\"Efficiency gain: {(1 - sum(lengths)/(padded_sequences.shape[0] * padded_sequences.shape[1]))*100:.1f}% less computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN vs LSTM vs GRU Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different RNN architectures\n",
    "class RNNComparison(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rnn_type='LSTM'):\n",
    "        super(RNNComparison, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        if rnn_type == 'RNN':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.rnn(x)\n",
    "            last_hidden = hidden[-1]\n",
    "        else:  # RNN or GRU\n",
    "            output, hidden = self.rnn(x)\n",
    "            last_hidden = hidden[-1]\n",
    "        \n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "# Create test models\n",
    "input_size, hidden_size, output_size = 32, 64, 2\n",
    "\n",
    "models = {\n",
    "    'RNN': RNNComparison(input_size, hidden_size, output_size, 'RNN'),\n",
    "    'LSTM': RNNComparison(input_size, hidden_size, output_size, 'LSTM'),\n",
    "    'GRU': RNNComparison(input_size, hidden_size, output_size, 'GRU')\n",
    "}\n",
    "\n",
    "# Compare parameter counts\n",
    "print(\"=== Parameter Comparison ===\")\n",
    "for name, model in models.items():\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name}: {param_count:,} parameters\")\n",
    "\n",
    "# Test on long sequence (to show vanishing gradient problem)\n",
    "seq_len = 100\n",
    "batch_size = 16\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "print(f\"\\n=== Forward Pass Test ===\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    output = model(x)\n",
    "    print(f\"{name} output shape: {output.shape}\")\n",
    "\n",
    "# Gradient analysis\n",
    "print(f\"\\n=== Gradient Flow Analysis ===\")\n",
    "\n",
    "# Create a simple task: remember the first input value\n",
    "def create_memory_task(seq_len=50, batch_size=32):\n",
    "    \"\"\"Create a task that requires remembering the first time step\"\"\"\n",
    "    x = torch.randn(batch_size, seq_len, 1)\n",
    "    # Target is based on the sign of the first time step\n",
    "    y = (x[:, 0, 0] > 0).long()\n",
    "    return x, y\n",
    "\n",
    "# Test gradient flow\n",
    "x, y = create_memory_task(seq_len=50, batch_size=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Reset model\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    \n",
    "    # Forward and backward\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradient norms\n",
    "    total_norm = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    \n",
    "    print(f\"{name}: Loss = {loss.item():.4f}, Gradient norm = {total_norm:.4f}\")\n",
    "\n",
    "print(\"\\n=== Architecture Summary ===\")\n",
    "print(\"RNN: Simple, fast, but suffers from vanishing gradients\")\n",
    "print(\"LSTM: Complex gating, best for long sequences, most parameters\")\n",
    "print(\"GRU: Simplified gating, good compromise between RNN and LSTM\")\n",
    "print(\"\\nRule of thumb:\")\n",
    "print(\"- Short sequences (<20): RNN might be sufficient\")\n",
    "print(\"- Long sequences (>50): LSTM or GRU\")\n",
    "print(\"- When in doubt: try GRU first (good balance of performance/complexity)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}