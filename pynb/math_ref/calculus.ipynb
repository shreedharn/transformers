{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Calculus for Deep Learning: The Mathematics of Learning\n\n## ðŸŽ¯ Introduction\n\nWelcome to the mathematical engine that makes neural networks learn! This notebook will demystify how calculus powers every aspect of deep learning, from simple gradient descent to complex backpropagation through transformer architectures. Understanding calculus is essential for truly mastering how neural networks optimize themselves.\n\n### ðŸ§  What You'll Master\n\nThis comprehensive guide covers:\n- **Derivatives and gradients**: How neural networks know which direction to improve\n- **Chain rule mastery**: The mathematical foundation of backpropagation\n- **Partial derivatives**: Understanding how multi-variable functions change\n- **Optimization theory**: Why gradient descent works and when it fails\n- **Computational graphs**: How automatic differentiation computes gradients\n\n### ðŸŽ“ Prerequisites\n\n- Basic understanding of functions and limits\n- Familiarity with PyTorch autograd system\n- Elementary knowledge of matrix operations\n- High school algebra and basic function concepts\n\n### ðŸš€ Why Calculus is the Heart of Deep Learning\n\nCalculus enables neural network learning because:\n- **Optimization**: Finding minima in high-dimensional loss landscapes\n- **Backpropagation**: Efficiently computing gradients through complex networks\n- **Learning rates**: Understanding how fast to update parameters\n- **Convergence**: Knowing when and why training succeeds or fails\n- **Architecture design**: Understanding gradient flow through different layers\n\n---\n\n## ðŸ“š Table of Contents\n\n1. **[Derivatives and Gradients](#derivatives-and-gradients)** - The direction of improvement\n2. **[Chain Rule and Backpropagation](#chain-rule-and-backpropagation)** - How gradients flow backward\n3. **[Partial Derivatives in Action](#partial-derivatives-in-action)** - Multi-variable optimization\n4. **[Computational Graphs](#computational-graphs)** - Automatic differentiation explained\n5. **[Optimization Landscapes](#optimization-landscapes)** - Understanding loss surfaces"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Derivatives and Gradients\n\n### ðŸ“ˆ The Mathematics of Direction\n\nDerivatives tell us how functions change, and in deep learning, this translates to knowing which direction to adjust parameters to reduce loss. Gradients are the multivariable extension that powers all neural network optimization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DERIVATIVES AND GRADIENTS: THE MATHEMATICS OF LEARNING\n# =============================================================================\n\nprint(\"ðŸ“ˆ Derivatives in Deep Learning Context\")\nprint(\"=\" * 50)\n\n# Show how derivatives guide neural network optimization\nprint(\"ðŸŽ¯ What Derivatives Tell Us in Neural Networks:\")\nprint(\"â€¢ How loss changes with respect to each parameter\")\nprint(\"â€¢ Which direction to move parameters to reduce loss\")  \nprint(\"â€¢ How sensitive the output is to input changes\")\nprint(\"â€¢ Whether we're near a minimum or still need to optimize\")\n\n# Demonstrate basic derivative concepts with PyTorch\nx = torch.tensor(2.0, requires_grad=True)\n\n# Simple function: f(x) = xÂ² + 3x + 1\n# Derivative: f'(x) = 2x + 3\nf = x**2 + 3*x + 1\nprint(f\"\\nFunction: f(x) = xÂ² + 3x + 1\")\nprint(f\"At x = {x.item()}\")\nprint(f\"f({x.item()}) = {f.item()}\")\n\n# Compute derivative using autograd\nf.backward()\nprint(f\"f'({x.item()}) = {x.grad.item()}\")\nprint(f\"Analytical: f'(2) = 2(2) + 3 = {2*2 + 3}\")\n\nprint(f\"\\nðŸ§® Gradient Descent in Action\")\nprint(\"=\" * 50)\n\n# Demonstrate how gradients guide optimization\n# Goal: minimize f(x) = (x - 5)Â² using gradient descent\n\nx = torch.tensor(0.0, requires_grad=True)  # Start at x = 0\nlearning_rate = 0.1\ntarget = 5.0\n\nprint(f\"Minimizing f(x) = (x - {target})Â² starting from x = {x.item()}\")\nprint(f\"True minimum is at x = {target}\")\n\nprint(\"\\nStep | x value | f(x) value | Gradient | Next step\")\nprint(\"-----|---------|------------|----------|----------\")\n\nfor step in range(10):\n    # Compute function value\n    f = (x - target)**2\n    \n    # Compute gradient\n    if x.grad is not None:\n        x.grad.zero_()  # Clear previous gradients\n    f.backward()\n    \n    current_x = x.item()\n    current_f = f.item()\n    current_grad = x.grad.item()\n    \n    print(f\"{step:4d} | {current_x:7.3f} | {current_f:10.3f} | {current_grad:8.3f} | \", end=\"\")\n    \n    # Gradient descent update: x = x - learning_rate * gradient\n    with torch.no_grad():\n        x -= learning_rate * x.grad\n    \n    print(f\"{x.item():7.3f}\")\n    \n    # Stop if very close to minimum\n    if abs(x.item() - target) < 0.001:\n        print(f\"Converged to target {target}!\")\n        break\n\nprint(f\"\\nðŸ’¡ Key Insights About Gradients\")\nprint(\"=\" * 50)\nprint(\"1. **Direction**: Gradient points toward steepest increase\")\nprint(\"2. **Magnitude**: Larger gradient = steeper slope = faster learning\")\nprint(\"3. **Zero gradient**: Indicates critical point (minimum, maximum, or saddle)\")\nprint(\"4. **Opposite direction**: We move opposite to gradient to minimize\")\nprint(\"5. **Learning rate**: Controls how big steps we take\")\n\nprint(f\"\\nðŸ” Multivariable Functions and Partial Derivatives\")\nprint(\"=\" * 50)\n\n# Neural networks have many parameters, so we need partial derivatives\nx = torch.tensor(1.0, requires_grad=True)\ny = torch.tensor(2.0, requires_grad=True)\n\n# Function of two variables: f(x,y) = xÂ²y + xyÂ² + 2\n# âˆ‚f/âˆ‚x = 2xy + yÂ²\n# âˆ‚f/âˆ‚y = xÂ² + 2xy\nf = x**2 * y + x * y**2 + 2\n\nprint(f\"Function: f(x,y) = xÂ²y + xyÂ² + 2\")\nprint(f\"At point ({x.item()}, {y.item()})\")\nprint(f\"f({x.item()}, {y.item()}) = {f.item()}\")\n\nf.backward()\n\nprint(f\"\\nPartial derivatives:\")\nprint(f\"âˆ‚f/âˆ‚x = {x.grad.item():.3f}\")\nprint(f\"âˆ‚f/âˆ‚y = {y.grad.item():.3f}\")\n\n# Verify analytical computation\nanalytical_dx = 2*x.item()*y.item() + y.item()**2\nanalytical_dy = x.item()**2 + 2*x.item()*y.item()\nprint(f\"\\nAnalytical verification:\")\nprint(f\"âˆ‚f/âˆ‚x = 2xy + yÂ² = 2({x.item()})({y.item()}) + ({y.item()})Â² = {analytical_dx}\")\nprint(f\"âˆ‚f/âˆ‚y = xÂ² + 2xy = ({x.item()})Â² + 2({x.item()})({y.item()}) = {analytical_dy}\")\n\nprint(f\"\\nðŸŽ¯ The Gradient Vector\")\nprint(\"=\" * 30)\n\ngradient = torch.tensor([x.grad.item(), y.grad.item()])\nprint(f\"Gradient vector: âˆ‡f = {gradient}\")\nprint(f\"Gradient magnitude: ||âˆ‡f|| = {torch.norm(gradient).item():.3f}\")\n\n# The gradient points in the direction of steepest increase\nprint(f\"\\nGradient interpretation:\")\nprint(f\"â€¢ Direction of steepest increase: {gradient/torch.norm(gradient)}\")\nprint(f\"â€¢ To minimize, move in direction: {-gradient/torch.norm(gradient)}\")\n\nprint(f\"\\nðŸ§  Neural Network Parameter Update\")\nprint(\"=\" * 50)\n\n# Simulate a simple neural network parameter update\nprint(\"In a neural network, each parameter gets updated based on its gradient:\")\n\n# Simulate some network parameters and their gradients\nparams = {\n    'weight_1': torch.tensor(0.5, requires_grad=True),\n    'weight_2': torch.tensor(-0.3, requires_grad=True), \n    'bias': torch.tensor(0.1, requires_grad=True)\n}\n\n# Simulate a loss computation\nloss = params['weight_1']**2 + params['weight_2']**2 + params['bias']**2\nprint(f\"Simulated loss: {loss.item():.4f}\")\n\n# Compute gradients\nloss.backward()\n\n# Show gradient-based updates\nlearning_rate = 0.1\nprint(f\"\\nParameter updates (learning_rate = {learning_rate}):\")\nprint(\"Parameter | Current | Gradient | New Value\")\nprint(\"----------|---------|----------|----------\")\n\nfor name, param in params.items():\n    current_val = param.item()\n    grad_val = param.grad.item()\n    new_val = current_val - learning_rate * grad_val\n    print(f\"{name:9} | {current_val:7.3f} | {grad_val:8.3f} | {new_val:7.3f}\")\n\nprint(f\"\\nâœ¨ The Magic of Automatic Differentiation\")\nprint(\"=\" * 50)\nprint(\"PyTorch automatically computes gradients for ANY function!\")\nprint(\"â€¢ Forward pass: Compute function values\")\nprint(\"â€¢ Backward pass: Compute gradients using chain rule\")\nprint(\"â€¢ No manual derivative calculations needed\")\nprint(\"â€¢ Works with arbitrarily complex neural network architectures\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Derivatives\n",
    "\n",
    "**Formula:** $\\frac{\\partial f}{\\partial x_i}$\n",
    "\n",
    "Derivative with respect to one variable while holding others constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variable function\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = x**2 * y + x * y**2  # f(x,y) = xÂ²y + xyÂ²\n",
    "\n",
    "z.backward()\n",
    "print(f\"f({x.item()}, {y.item()}) = {z.item()}\")\n",
    "print(f\"âˆ‚f/âˆ‚x = {x.grad.item()}\")  # Should be 2xy + yÂ²\n",
    "print(f\"âˆ‚f/âˆ‚y = {y.grad.item()}\")  # Should be xÂ² + 2xy\n",
    "\n",
    "# Neural network layer with multiple parameters\n",
    "batch_size, input_dim, output_dim = 4, 3, 2\n",
    "X = torch.randn(batch_size, input_dim)\n",
    "W = torch.randn(output_dim, input_dim, requires_grad=True)\n",
    "b = torch.randn(output_dim, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "Y = X @ W.T + b\n",
    "loss = Y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nWeight gradients shape: {W.grad.shape}\")\n",
    "print(f\"Bias gradients shape: {b.grad.shape}\")\n",
    "print(f\"Each gradient shows how loss changes w.r.t. that parameter\")\n",
    "\n",
    "# Examine specific parameter gradients\n",
    "print(f\"âˆ‚loss/âˆ‚W[0,0] = {W.grad[0,0].item():.3f}\")\n",
    "print(f\"âˆ‚loss/âˆ‚b[0] = {b.grad[0].item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "**Formula:** $\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$\n",
    "\n",
    "Mathematical foundation of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual chain rule demonstration\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Composition: f(g(h(x))) where h(x)=xÂ², g(u)=u+1, f(v)=vÂ³\n",
    "h = x**2        # h(x) = xÂ²\n",
    "g = h + 1       # g(h) = h + 1  \n",
    "f = g**3        # f(g) = gÂ³\n",
    "\n",
    "f.backward()\n",
    "print(f\"Input: {x.item()}\")\n",
    "print(f\"h(x) = xÂ² = {h.item()}\")\n",
    "print(f\"g(h) = h + 1 = {g.item()}\")\n",
    "print(f\"f(g) = gÂ³ = {f.item()}\")\n",
    "print(f\"df/dx via chain rule: {x.grad.item()}\")\n",
    "\n",
    "# Manual verification: \n",
    "# df/dx = df/dg * dg/dh * dh/dx = 3gÂ² * 1 * 2x = 3(xÂ²+1)Â² * 2x\n",
    "manual = 3 * (x.item()**2 + 1)**2 * 2 * x.item()\n",
    "print(f\"Manual calculation: {manual}\")\n",
    "\n",
    "# Neural network chain rule\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(2, 3)\n",
    "        self.layer2 = torch.nn.Linear(3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.relu(self.layer1(x))  # First composition\n",
    "        h2 = self.layer2(h1)             # Second composition\n",
    "        return h2\n",
    "\n",
    "net = SimpleNet()\n",
    "x_input = torch.randn(1, 2)\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "output = net(x_input)\n",
    "loss = torch.nn.functional.mse_loss(output, target)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nNetwork output: {output.item():.3f}\")\n",
    "print(f\"Loss: {loss.item():.3f}\")\n",
    "print(f\"Layer 1 weight gradients: {net.layer1.weight.grad[0][:2]}\")\n",
    "print(f\"Layer 2 weight gradients: {net.layer2.weight.grad[0][:2]}\")\n",
    "print(\"Gradients computed via automatic chain rule application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "**Formula:** $\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right]$\n",
    "\n",
    "Points in direction of steepest increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D function visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function: f(x,y) = xÂ² + yÂ² - 2x - 4y + 5 (has minimum at (1,2))\n",
    "def f(x, y):\n",
    "    return x**2 + y**2 - 2*x - 4*y + 5\n",
    "\n",
    "# Gradient: âˆ‡f = [2x-2, 2y-4]\n",
    "def gradient(x, y):\n",
    "    return torch.tensor([2*x - 2, 2*y - 4])\n",
    "\n",
    "# Starting point\n",
    "position = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "learning_rate = 0.1\n",
    "path = [position.detach().clone()]\n",
    "\n",
    "print(\"Gradient descent optimization:\")\n",
    "for step in range(10):\n",
    "    # Compute function value and gradient\n",
    "    x, y = position\n",
    "    loss = f(x, y)\n",
    "    \n",
    "    # Clear previous gradients\n",
    "    if position.grad is not None:\n",
    "        position.grad.zero_()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Step {step}: pos=({x:.2f}, {y:.2f}), f={loss:.3f}, grad=({position.grad[0]:.2f}, {position.grad[1]:.2f})\")\n",
    "    \n",
    "    # Update position (gradient descent step)\n",
    "    with torch.no_grad():\n",
    "        position -= learning_rate * position.grad\n",
    "    \n",
    "    path.append(position.detach().clone())\n",
    "    \n",
    "    # Stop if gradient is small\n",
    "    if torch.norm(position.grad) < 0.01:\n",
    "        break\n",
    "\n",
    "print(f\"\\nFinal position: ({position[0]:.3f}, {position[1]:.3f})\")\n",
    "print(f\"Theoretical minimum: (1.000, 2.000)\")\n",
    "\n",
    "# Gradient-based feature importance\n",
    "model = torch.nn.Linear(5, 1)\n",
    "input_data = torch.randn(1, 5, requires_grad=True)\n",
    "target = torch.randn(1, 1)\n",
    "\n",
    "output = model(input_data)\n",
    "loss = torch.nn.functional.mse_loss(output, target)\n",
    "loss.backward()\n",
    "\n",
    "feature_importance = torch.abs(input_data.grad).squeeze()\n",
    "print(f\"\\nFeature importance (|gradient|): {feature_importance}\")\n",
    "print(f\"Most important feature: {feature_importance.argmax().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian\n",
    "\n",
    "**Formula:** $\\mathbf{H}_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$\n",
    "\n",
    "Matrix of second derivatives describing curvature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Hessian for simple function\n",
    "def quadratic_loss(x):\n",
    "    return 0.5 * (x[0]**2 + 2*x[1]**2 + x[0]*x[1])\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "loss = quadratic_loss(x)\n",
    "\n",
    "# Compute gradients\n",
    "grad = torch.autograd.grad(loss, x, create_graph=True)[0]\n",
    "\n",
    "# Compute Hessian (second derivatives)\n",
    "hessian = torch.zeros(2, 2)\n",
    "for i in range(2):\n",
    "    grad2 = torch.autograd.grad(grad[i], x, retain_graph=True)[0]\n",
    "    hessian[i] = grad2\n",
    "\n",
    "print(f\"Loss: {loss.item():.3f}\")\n",
    "print(f\"Gradient: {grad}\")\n",
    "print(f\"Hessian:\\n{hessian}\")\n",
    "\n",
    "# Condition number analysis\n",
    "eigenvals = torch.linalg.eigvals(hessian).real\n",
    "condition_number = eigenvals.max() / eigenvals.min()\n",
    "print(f\"Condition number: {condition_number:.2f}\")\n",
    "print(f\"Well-conditioned: {condition_number < 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "\n",
    "**Formula:** $\\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j}$\n",
    "\n",
    "Matrix of first derivatives for vector-valued functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-valued function example\n",
    "def vector_function(x):\n",
    "    return torch.stack([\n",
    "        x[0]**2 + x[1],\n",
    "        x[0] * x[1],\n",
    "        torch.sin(x[0]) + torch.cos(x[1])\n",
    "    ])\n",
    "\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = vector_function(x)\n",
    "\n",
    "# Compute Jacobian\n",
    "jacobian = torch.zeros(3, 2)\n",
    "for i in range(3):\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    y[i].backward(retain_graph=True)\n",
    "    jacobian[i] = x.grad.clone()\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {y}\")\n",
    "print(f\"Jacobian:\\n{jacobian}\")\n",
    "\n",
    "# Neural network layer Jacobian\n",
    "layer = torch.nn.Linear(3, 2)\n",
    "x_batch = torch.randn(1, 3, requires_grad=True)\n",
    "y_batch = layer(x_batch)\n",
    "\n",
    "# Jacobian for neural network layer\n",
    "jac = torch.autograd.functional.jacobian(layer, x_batch)\n",
    "print(f\"NN Jacobian shape: {jac.shape}\")  # (batch, output_dim, batch, input_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}