{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Linear Algebra for Deep Learning: The Mathematical Foundation\n\n## ðŸŽ¯ Introduction\n\nWelcome to the mathematical backbone of deep learning! This notebook will transform you from someone who uses linear algebra operations into someone who understands why they work and how they enable neural network magic. Every operation in deep learning - from simple matrix multiplication to complex attention mechanisms - relies on linear algebra.\n\n### ðŸ§  What You'll Master\n\nThis comprehensive guide covers:\n- **Vector operations**: The building blocks of all neural network computations\n- **Matrix multiplication**: The fundamental operation that powers every layer\n- **Eigenvalues and eigenvectors**: Understanding principal components and transformations\n- **Matrix decompositions**: SVD, PCA, and their roles in model compression\n- **Norms and distances**: Essential for optimization and regularization\n\n### ðŸŽ“ Prerequisites\n\n- Basic understanding of vectors and matrices\n- Familiarity with PyTorch tensor operations\n- Elementary knowledge of coordinate systems\n- High school algebra and basic calculus\n\n### ðŸš€ Why Linear Algebra is Deep Learning's Language\n\nLinear algebra enables deep learning because:\n- **Efficient computation**: Matrix operations are highly optimized on GPUs\n- **Dimensional transformations**: Networks learn to map between feature spaces\n- **Batch processing**: Linear operations naturally handle multiple samples\n- **Optimization**: Gradients and parameter updates are vector operations\n- **Interpretability**: Understanding transformations helps debug and improve models\n\n---\n\n## ðŸ“š Table of Contents\n\n1. **[Vectors and Vector Spaces](#vectors-and-vector-spaces)** - The fundamental building blocks\n2. **[Matrix Operations in Deep Learning](#matrix-operations-in-deep-learning)** - Core transformations\n3. **[Eigendecomposition and PCA](#eigendecomposition-and-pca)** - Understanding data structure\n4. **[Matrix Norms and Distances](#matrix-norms-and-distances)** - Measuring and regularizing\n5. **[Advanced Decompositions](#advanced-decompositions)** - SVD and low-rank approximations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Vectors and Vector Spaces\n\n### ðŸ“ The Foundation of All Neural Network Operations\n\nVectors are the fundamental data structure in deep learning. Every input, output, parameter, and gradient is a vector or collection of vectors. Understanding vector operations deeply is essential for mastering how neural networks transform information."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# VECTORS: THE BUILDING BLOCKS OF DEEP LEARNING\n# =============================================================================\n\nprint(\"ðŸ“ Vector Operations in Deep Learning Context\")\nprint(\"=\" * 50)\n\n# Vectors in deep learning represent features, embeddings, parameters, gradients\nprint(\"ðŸŽ¯ What Vectors Represent in Neural Networks:\")\nprint(\"â€¢ Input features: [height, weight, age, income]\")\nprint(\"â€¢ Word embeddings: [semantic_dim_1, semantic_dim_2, ..., semantic_dim_n]\") \nprint(\"â€¢ Hidden activations: [neuron_1_output, neuron_2_output, ...]\")\nprint(\"â€¢ Gradients: [âˆ‚loss/âˆ‚param_1, âˆ‚loss/âˆ‚param_2, ...]\")\nprint(\"â€¢ Model parameters: [weight_1, weight_2, bias_1, ...]\")\n\n# Demonstrate fundamental vector operations\na = torch.tensor([2.0, 1.0, 3.0])    # Feature vector (e.g., RGB color)\nb = torch.tensor([1.0, 4.0, 2.0])    # Another feature vector\n\nprint(f\"\\nðŸ“Š Basic Vector Operations\")\nprint(\"=\" * 30)\nprint(f\"Vector a: {a}\")\nprint(f\"Vector b: {b}\")\n\n# Addition: Combining features or gradients\nvector_sum = a + b\nprint(f\"\\nAddition (a + b): {vector_sum}\")\nprint(\"Use case: Combining gradients from different loss terms\")\n\n# Scalar multiplication: Scaling learning rates or regularization\nscaled = 0.5 * a\nprint(f\"\\nScalar multiplication (0.5 * a): {scaled}\")\nprint(\"Use case: Applying learning rate to gradient updates\")\n\n# Dot product: Similarity, projections, attention scores\ndot_product = torch.dot(a, b)\nprint(f\"\\nDot product (a Â· b): {dot_product.item():.3f}\")\nprint(\"Use case: Computing attention scores, similarity measures\")\n\n# Norms: Magnitude, regularization, gradient clipping\nl2_norm = torch.norm(a, p=2)\nl1_norm = torch.norm(a, p=1)\nprint(f\"\\nL2 norm (||a||â‚‚): {l2_norm.item():.3f}\")\nprint(f\"L1 norm (||a||â‚): {l1_norm.item():.3f}\")\nprint(\"Use case: L2 for weight decay, L1 for sparsity, gradient clipping\")\n\nprint(f\"\\nðŸŽ¯ Vector Geometry in High Dimensions\")\nprint(\"=\" * 50)\n\n# Demonstrate how vectors behave in high-dimensional spaces\ndims = [2, 10, 100, 1000]\nprint(\"Dimension | Random vectors | Avg dot product | Avg angle (degrees)\")\nprint(\"----------|----------------|------------------|--------------------\")\n\nfor dim in dims:\n    # Generate random unit vectors (normalized)\n    v1 = torch.randn(dim)\n    v1 = v1 / torch.norm(v1)  # Normalize to unit length\n    \n    v2 = torch.randn(dim) \n    v2 = v2 / torch.norm(v2)  # Normalize to unit length\n    \n    # Compute dot product and angle\n    dot_prod = torch.dot(v1, v2).item()\n    angle_rad = torch.acos(torch.clamp(torch.dot(v1, v2), -1, 1))\n    angle_deg = torch.rad2deg(angle_rad).item()\n    \n    print(f\"{dim:9d} | Unit vectors      | {dot_prod:16.4f} | {angle_deg:18.1f}\")\n\nprint(f\"\\nðŸ’¡ High-Dimensional Insight:\")\nprint(\"As dimensions increase, random vectors become nearly orthogonal!\")\nprint(\"This is why embeddings can represent many concepts without interference.\")\n\nprint(f\"\\nðŸ§® Linear Combinations: The Core of Neural Networks\")\nprint(\"=\" * 50)\n\n# Linear combinations are what every layer computes\nprint(\"Every neural network layer computes: y = W @ x + b\")\nprint(\"This is a linear combination of input features!\")\n\n# Demonstrate with a mini example\ninput_features = torch.tensor([0.5, 0.8, 0.2])  # 3 input features\nweights = torch.tensor([                          # 2 neurons, 3 inputs each\n    [1.0, -0.5, 2.0],   # Neuron 1 weights\n    [0.3, 1.5, -1.0]    # Neuron 2 weights\n])\nbias = torch.tensor([0.1, -0.2])\n\nprint(f\"\\nInput features: {input_features}\")\nprint(f\"Weight matrix shape: {weights.shape}\")\nprint(f\"Bias vector: {bias}\")\n\n# Matrix multiplication + bias (linear layer computation)\noutput = weights @ input_features + bias\nprint(f\"\\nLayer output: {output}\")\n\nprint(f\"\\nWhat each neuron computes:\")\nfor i, (w_row, b_val) in enumerate(zip(weights, bias)):\n    linear_combo = torch.dot(w_row, input_features) + b_val\n    print(f\"Neuron {i+1}: {w_row[0]:.1f}*{input_features[0]:.1f} + {w_row[1]:.1f}*{input_features[1]:.1f} + {w_row[2]:.1f}*{input_features[2]:.1f} + {b_val:.1f} = {linear_combo:.3f}\")\n\nprint(f\"\\nðŸ” Vector Spaces in Deep Learning\")\nprint(\"=\" * 50)\n\n# Show how neural networks learn to transform vector spaces\nprint(\"Neural networks learn transformations between vector spaces:\")\nprint(\"â€¢ Input space: Raw features (pixels, words, sensor readings)\")\nprint(\"â€¢ Hidden spaces: Learned representations (edges, concepts, patterns)\")\nprint(\"â€¢ Output space: Task-specific features (class probabilities, predictions)\")\n\n# Demonstrate basis vectors and linear independence\nprint(f\"\\nðŸ“ Basis Vectors and Linear Independence\")\nprint(\"-\" * 40)\n\n# Standard basis in 3D\ne1 = torch.tensor([1.0, 0.0, 0.0])\ne2 = torch.tensor([0.0, 1.0, 0.0]) \ne3 = torch.tensor([0.0, 0.0, 1.0])\n\nprint(f\"Standard basis vectors:\")\nprint(f\"eâ‚: {e1}\")\nprint(f\"eâ‚‚: {e2}\")\nprint(f\"eâ‚ƒ: {e3}\")\n\n# Any vector can be written as a linear combination of basis vectors\narbitrary_vector = torch.tensor([2.5, -1.0, 3.2])\nprint(f\"\\nArbitrary vector: {arbitrary_vector}\")\nprint(f\"As linear combination: {arbitrary_vector[0]:.1f}*eâ‚ + {arbitrary_vector[1]:.1f}*eâ‚‚ + {arbitrary_vector[2]:.1f}*eâ‚ƒ\")\n\n# Verify this is correct\nreconstructed = arbitrary_vector[0]*e1 + arbitrary_vector[1]*e2 + arbitrary_vector[2]*e3\nprint(f\"Reconstructed: {reconstructed}\")\nprint(f\"Match: {torch.allclose(arbitrary_vector, reconstructed)}\")\n\nprint(f\"\\nðŸ’¡ Deep Learning Connection:\")\nprint(\"Neural networks learn new basis vectors (features) that are more\")\nprint(\"useful for the task than the original input features!\")\nprint(\"Each hidden layer finds a new coordinate system for the data.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Transpose\n",
    "\n",
    "**Formula:** $(\\mathbf{A})^T_{ij} = \\mathbf{A}_{ji}$\n",
    "\n",
    "Essential for backpropagation - the transpose \"reverses\" the forward direction of information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass: y = x @ W.T\n",
    "# Backward pass: dx = dy @ W (using transpose automatically)\n",
    "x = torch.randn(32, 784, requires_grad=True)\n",
    "W = torch.randn(128, 784, requires_grad=True)\n",
    "y = x @ W.T\n",
    "\n",
    "# Create dummy loss and backpropagate\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Forward: x {x.shape} @ W.T {W.T.shape} = y {y.shape}\")\n",
    "print(f\"Gradient flows back through transpose automatically\")\n",
    "print(f\"x.grad shape: {x.grad.shape}\")  # Same as x.shape\n",
    "print(f\"W.grad shape: {W.grad.shape}\")  # Same as W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inverse\n",
    "\n",
    "**Formula:** $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$\n",
    "\n",
    "Used in analytical solutions and understanding linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal equations for linear regression: Î¸ = (X.T @ X)^(-1) @ X.T @ y\n",
    "n_samples, n_features = 100, 10\n",
    "X = torch.randn(n_samples, n_features)\n",
    "true_theta = torch.randn(n_features)\n",
    "y = X @ true_theta + 0.1 * torch.randn(n_samples)\n",
    "\n",
    "# Analytical solution using matrix inverse\n",
    "XtX = X.T @ X\n",
    "XtX_inv = torch.inverse(XtX)\n",
    "theta_analytical = XtX_inv @ X.T @ y\n",
    "\n",
    "print(f\"True theta: {true_theta[:3]}\")\n",
    "print(f\"Estimated theta: {theta_analytical[:3]}\")\n",
    "print(f\"Error: {torch.norm(true_theta - theta_analytical):.6f}\")\n",
    "\n",
    "# Note: In practice, use torch.linalg.lstsq for numerical stability\n",
    "theta_stable = torch.linalg.lstsq(X, y).solution\n",
    "print(f\"Stable solution: {theta_stable[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues & Eigenvectors\n",
    "\n",
    "**Formula:** $\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}$\n",
    "\n",
    "Reveals principal directions of data variation and helps analyze gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing weight matrix conditioning\n",
    "W = torch.randn(100, 100)\n",
    "eigenvals, eigenvecs = torch.linalg.eig(W @ W.T)  # Eigendecomposition\n",
    "eigenvals = eigenvals.real  # Take real part\n",
    "\n",
    "condition_number = eigenvals.max() / eigenvals.min()\n",
    "print(f\"Condition number: {condition_number:.2f}\")\n",
    "print(f\"Max eigenvalue: {eigenvals.max():.2f}\")\n",
    "print(f\"Min eigenvalue: {eigenvals.min():.2f}\")\n",
    "\n",
    "# PCA example - find principal components\n",
    "data = torch.randn(1000, 50)  # 1000 samples, 50 features\n",
    "centered_data = data - data.mean(dim=0)\n",
    "cov_matrix = (centered_data.T @ centered_data) / (len(data) - 1)\n",
    "\n",
    "eigenvals, eigenvecs = torch.linalg.eigh(cov_matrix)  # For symmetric matrices\n",
    "# Sort by eigenvalue magnitude\n",
    "sorted_indices = torch.argsort(eigenvals, descending=True)\n",
    "principal_components = eigenvecs[:, sorted_indices]\n",
    "\n",
    "print(f\"Explained variance ratios: {eigenvals[sorted_indices][:5] / eigenvals.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "**Formula:** $\\mathbf{A} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T$\n",
    "\n",
    "Decomposes any matrix into orthogonal transformations and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD for dimensionality reduction and analysis\n",
    "data = torch.randn(1000, 100)  # High-dimensional data\n",
    "\n",
    "# Perform SVD\n",
    "U, S, Vt = torch.linalg.svd(data, full_matrices=False)\n",
    "\n",
    "# Analyze the singular values\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"U shape: {U.shape}\")    # Left singular vectors\n",
    "print(f\"S shape: {S.shape}\")    # Singular values\n",
    "print(f\"Vt shape: {Vt.shape}\")  # Right singular vectors\n",
    "\n",
    "# Reconstruct with fewer components (dimensionality reduction)\n",
    "k = 20  # Keep top 20 components\n",
    "data_reduced = U[:, :k] @ torch.diag(S[:k]) @ Vt[:k, :]\n",
    "\n",
    "reconstruction_error = torch.norm(data - data_reduced)\n",
    "compression_ratio = (k * (U.shape[0] + Vt.shape[1])) / (data.shape[0] * data.shape[1])\n",
    "\n",
    "print(f\"Reconstruction error: {reconstruction_error:.2f}\")\n",
    "print(f\"Compression ratio: {compression_ratio:.2%}\")\n",
    "print(f\"Variance explained by top {k} components: {(S[:k]**2).sum() / (S**2).sum():.2%}\")\n",
    "\n",
    "# SVD for weight initialization (orthogonal initialization)\n",
    "def svd_init(tensor):\n",
    "    \"\"\"Initialize weights using SVD for orthogonal matrices\"\"\"\n",
    "    if tensor.dim() >= 2:\n",
    "        U, _, Vt = torch.linalg.svd(tensor, full_matrices=False)\n",
    "        return U if U.shape == tensor.shape else Vt\n",
    "    return tensor\n",
    "\n",
    "weight = torch.empty(128, 64)\n",
    "orthogonal_weight = svd_init(weight)\n",
    "print(f\"Orthogonality check: {torch.norm(orthogonal_weight @ orthogonal_weight.T - torch.eye(128)):.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}