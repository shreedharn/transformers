{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PyTorch Optimization & Training: Making Neural Networks Learn\n\n## üéØ Introduction\n\nWelcome to the heart of deep learning - the training process! This notebook will transform you from someone who can build neural networks into someone who can train them effectively. Understanding optimization is what separates toy examples from production-ready models.\n\n### üß† What You'll Master\n\nThis comprehensive guide covers:\n- **Training loop architecture**: The fundamental structure of neural network training\n- **Optimizer comparison**: SGD, Adam, AdamW, and when to use each\n- **Loss function selection**: Choosing the right objective for your problem\n- **Learning rate strategies**: Schedules, warmup, and adaptive methods\n- **Training stability**: Preventing divergence and ensuring convergence\n\n### üéì Prerequisites\n\n- Solid understanding of modules, parameters, and autograd\n- Familiarity with neural network architecture concepts\n- Basic knowledge of gradient descent and backpropagation\n\n### üöÄ Why Training Mastery Matters\n\nEffective training enables:\n- **Convergence**: Reaching optimal solutions reliably\n- **Efficiency**: Training faster with fewer resources\n- **Stability**: Avoiding common training failures\n- **Generalization**: Models that work on new data\n- **Scalability**: Techniques that work from tiny to massive models\n\n---\n\n## üìö Table of Contents\n\n1. **[Training Loop Fundamentals](#training-loop-fundamentals)** - The core structure of neural network training\n2. **[Optimizer Deep Dive](#optimizer-deep-dive)** - Understanding and comparing optimization algorithms\n3. **[Loss Functions & Objectives](#loss-functions-objectives)** - Choosing the right training signal\n4. **[Learning Rate Strategies](#learning-rate-strategies)** - Schedules and adaptive methods\n5. **[Training Stability & Debugging](#training-stability-debugging)** - Ensuring reliable convergence"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Training Loop Fundamentals\n\n### üîÑ The Heart of Deep Learning\n\nEvery neural network training follows the same fundamental pattern: forward pass, loss computation, backward pass, parameter update. Let's master this essential cycle and understand why each step matters!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# THE COMPLETE TRAINING LOOP ARCHITECTURE\n# =============================================================================\n\nprint(\"üîÑ Training Loop Mastery\")\nprint(\"=\" * 50)\n\n# Create a simple model and dataset for training demonstration\nclass SimpleRegressor(nn.Module):\n    \"\"\"Simple model for demonstrating training concepts.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(), \n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Generate synthetic dataset for training\ntorch.manual_seed(42)  # For reproducibility\n\n# Create synthetic data: y = 2*x1 + 3*x2 + noise\nn_samples, input_dim = 1000, 5\nX = torch.randn(n_samples, input_dim)\n# Only first two features matter, rest are noise\ntrue_weights = torch.tensor([2.0, 3.0, 0.0, 0.0, 0.0])\ny = X @ true_weights + 0.1 * torch.randn(n_samples)  # Add noise\n\nprint(f\"Dataset created: {n_samples} samples, {input_dim} features\")\nprint(f\"True relationship: y = 2*x1 + 3*x2 + noise\")\n\n# Split into train/validation\ntrain_size = int(0.8 * n_samples)\nX_train, X_val = X[:train_size], X[train_size:]\ny_train, y_val = y[:train_size], y[train_size:]\n\nprint(f\"Train set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\n\n# Initialize model, loss, and optimizer\nmodel = SimpleRegressor(input_dim=input_dim, hidden_dim=32, output_dim=1)\ncriterion = nn.MSELoss()  # Mean Squared Error for regression\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nprint(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\nüéØ The Universal Training Loop\")\nprint(\"=\" * 50)\n\n# Training configuration\nnum_epochs = 100\nbatch_size = 32\n\n# Create data loaders for batching\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataset = TensorDataset(X_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Training loop with detailed commentary\ntrain_losses = []\nval_losses = []\n\nprint(\"Training progress (every 20 epochs):\")\nprint(\"Epoch | Train Loss | Val Loss   | Notes\")\nprint(\"------|------------|------------|-------------\")\n\nfor epoch in range(num_epochs):\n    # =================================\n    # TRAINING PHASE\n    # =================================\n    model.train()  # Set to training mode (affects dropout, batchnorm)\n    epoch_train_loss = 0.0\n    num_train_batches = 0\n    \n    for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n        # Step 1: Zero gradients from previous iteration\n        # CRITICAL: Without this, gradients accumulate!\n        optimizer.zero_grad()\n        \n        # Step 2: Forward pass - compute predictions\n        predictions = model(batch_X)\n        \n        # Step 3: Compute loss between predictions and targets\n        loss = criterion(predictions.squeeze(), batch_y)\n        \n        # Step 4: Backward pass - compute gradients\n        loss.backward()\n        \n        # Step 5: Update parameters using computed gradients\n        optimizer.step()\n        \n        # Track training progress\n        epoch_train_loss += loss.item()\n        num_train_batches += 1\n    \n    avg_train_loss = epoch_train_loss / num_train_batches\n    train_losses.append(avg_train_loss)\n    \n    # =================================\n    # VALIDATION PHASE\n    # =================================\n    model.eval()  # Set to evaluation mode\n    epoch_val_loss = 0.0\n    num_val_batches = 0\n    \n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for batch_X, batch_y in val_loader:\n            predictions = model(batch_X)\n            loss = criterion(predictions.squeeze(), batch_y)\n            epoch_val_loss += loss.item()\n            num_val_batches += 1\n    \n    avg_val_loss = epoch_val_loss / num_val_batches\n    val_losses.append(avg_val_loss)\n    \n    # Progress reporting\n    if epoch % 20 == 0 or epoch == num_epochs - 1:\n        if epoch == 0:\n            note = \"Initial random weights\"\n        elif avg_val_loss < min(val_losses[:-1]):\n            note = \"New best validation!\"\n        elif avg_train_loss < 0.01:\n            note = \"Near convergence\"\n        else:\n            note = \"Training...\"\n            \n        print(f\"{epoch:5d} | {avg_train_loss:10.6f} | {avg_val_loss:10.6f} | {note}\")\n\nprint(f\"\\n‚úÖ Training completed!\")\nprint(f\"Final train loss: {train_losses[-1]:.6f}\")\nprint(f\"Final validation loss: {val_losses[-1]:.6f}\")\n\n# Check how well we learned the true relationship\nlearned_weights = model.network[0].weight[0].detach()  # First layer weights\nprint(f\"\\nLearned vs True weights (first 2 should be ~2.0, ~3.0):\")\nfor i, (learned, true) in enumerate(zip(learned_weights, true_weights)):\n    print(f\"  Feature {i+1}: learned={learned.item():6.3f}, true={true.item():6.3f}\")\n\nprint(f\"\\nüîç Key Training Loop Elements\")\nprint(\"=\" * 50)\nprint(\"1. **model.train()**: Enable training mode (dropout, batchnorm active)\")\nprint(\"2. **optimizer.zero_grad()**: Clear gradients from previous iteration\")\nprint(\"3. **Forward pass**: Compute predictions from inputs\")\nprint(\"4. **Loss computation**: Measure prediction quality\")\nprint(\"5. **loss.backward()**: Compute gradients via backpropagation\")\nprint(\"6. **optimizer.step()**: Update parameters using gradients\")\nprint(\"7. **model.eval()**: Switch to evaluation mode for validation\")\nprint(\"8. **torch.no_grad()**: Disable gradients during validation\")\n\nprint(f\"\\n‚ö° Performance Insights\")\nprint(\"=\" * 50)\nbest_val_epoch = val_losses.index(min(val_losses))\nprint(f\"Best validation loss: {min(val_losses):.6f} at epoch {best_val_epoch}\")\nprint(f\"Training efficiency: {'High' if train_losses[-1] < 0.01 else 'Moderate'}\")\nprint(f\"Overfitting check: {'Good' if val_losses[-1] < 2 * min(val_losses) else 'Check'}\")"
  },
  {
   "cell_type": "code",
   "id": "188q9jvhnam",
   "source": "# =============================================================================\n# MISSING DEFINITIONS FOR COMPREHENSIVE TRAINING DEMO\n# =============================================================================\n\n# SimpleClassifier class for classification tasks\nclass SimpleClassifier(nn.Module):\n    \"\"\"Simple neural network for multi-class classification.\"\"\"\n    \n    def __init__(self, input_size, hidden_size, num_classes):\n        super(SimpleClassifier, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.network(x)\n\n# Universal training function\ndef train_model(model, train_loader, val_loader=None, num_epochs=10, lr=0.001, device='cpu'):\n    \"\"\"\n    Universal training loop for classification tasks.\n    \n    Args:\n        model: PyTorch model to train\n        train_loader: DataLoader for training data\n        val_loader: DataLoader for validation data (optional)\n        num_epochs: Number of training epochs\n        lr: Learning rate\n        device: Device to train on ('cpu' or 'cuda')\n    \n    Returns:\n        Dictionary with training history\n    \"\"\"\n    model = model.to(device)\n    \n    # Setup loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    \n    # Training history\n    history = {\n        'train_loss': [],\n        'train_acc': [],\n        'val_loss': [],\n        'val_acc': []\n    }\n    \n    print(f\"Training for {num_epochs} epochs on {device}\")\n    print(\"Epoch | Train Loss | Train Acc | Val Loss | Val Acc\")\n    print(\"------|------------|-----------|----------|--------\")\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(data)\n            loss = criterion(outputs, target)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            # Statistics\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_train += target.size(0)\n            correct_train += (predicted == target).sum().item()\n        \n        # Calculate training metrics\n        epoch_train_loss = running_loss / len(train_loader)\n        epoch_train_acc = 100.0 * correct_train / total_train\n        \n        history['train_loss'].append(epoch_train_loss)\n        history['train_acc'].append(epoch_train_acc)\n        \n        # Validation phase\n        epoch_val_loss = 0.0\n        epoch_val_acc = 0.0\n        \n        if val_loader is not None:\n            model.eval()\n            val_loss = 0.0\n            correct_val = 0\n            total_val = 0\n            \n            with torch.no_grad():\n                for data, target in val_loader:\n                    data, target = data.to(device), target.to(device)\n                    outputs = model(data)\n                    loss = criterion(outputs, target)\n                    \n                    val_loss += loss.item()\n                    _, predicted = torch.max(outputs.data, 1)\n                    total_val += target.size(0)\n                    correct_val += (predicted == target).sum().item()\n            \n            epoch_val_loss = val_loss / len(val_loader)\n            epoch_val_acc = 100.0 * correct_val / total_val\n            \n            history['val_loss'].append(epoch_val_loss)\n            history['val_acc'].append(epoch_val_acc)\n        \n        # Print progress\n        if val_loader is not None:\n            print(f\"{epoch+1:5d} | {epoch_train_loss:10.4f} | {epoch_train_acc:8.2f}% | {epoch_val_loss:8.4f} | {epoch_val_acc:7.2f}%\")\n        else:\n            print(f\"{epoch+1:5d} | {epoch_train_loss:10.4f} | {epoch_train_acc:8.2f}% |    N/A   |   N/A\")\n    \n    return history\n\n# Sample dataset creation for classification\ndef create_classification_dataset(n_samples=1000, n_features=20, n_classes=5, noise=0.1):\n    \"\"\"\n    Create a synthetic classification dataset.\n    \n    Args:\n        n_samples: Number of samples to generate\n        n_features: Number of input features\n        n_classes: Number of classes\n        noise: Amount of noise to add\n    \n    Returns:\n        X: Input features\n        y: Class labels\n    \"\"\"\n    torch.manual_seed(42)  # For reproducibility\n    \n    # Generate random centers for each class\n    class_centers = torch.randn(n_classes, n_features) * 2\n    \n    # Generate samples\n    X = torch.zeros(n_samples, n_features)\n    y = torch.zeros(n_samples, dtype=torch.long)\n    \n    samples_per_class = n_samples // n_classes\n    \n    for class_idx in range(n_classes):\n        start_idx = class_idx * samples_per_class\n        end_idx = start_idx + samples_per_class\n        \n        if class_idx == n_classes - 1:  # Last class gets remaining samples\n            end_idx = n_samples\n        \n        # Generate samples around class center\n        class_samples = class_centers[class_idx].unsqueeze(0) + noise * torch.randn(end_idx - start_idx, n_features)\n        \n        X[start_idx:end_idx] = class_samples\n        y[start_idx:end_idx] = class_idx\n    \n    # Shuffle the dataset\n    perm = torch.randperm(n_samples)\n    X = X[perm]\n    y = y[perm]\n    \n    return X, y\n\n# Create the classification dataset\nprint(\"Creating classification dataset...\")\nX_class, y_class = create_classification_dataset(n_samples=2000, n_features=20, n_classes=5)\n\n# Split into train/validation\ntrain_size = int(0.8 * len(X_class))\nX_train_class = X_class[:train_size]\ny_train_class = y_class[:train_size]\nX_val_class = X_class[train_size:]\ny_val_class = y_class[train_size:]\n\nprint(f\"Classification dataset created:\")\nprint(f\"  Total samples: {len(X_class)}\")\nprint(f\"  Features: {X_class.shape[1]}\")\nprint(f\"  Classes: {len(torch.unique(y_class))}\")\nprint(f\"  Train samples: {len(X_train_class)}\")\nprint(f\"  Validation samples: {len(X_val_class)}\")\n\n# Create data loaders\ntrain_dataset_class = TensorDataset(X_train_class, y_train_class)\ntrain_loader = DataLoader(train_dataset_class, batch_size=32, shuffle=True)\n\nval_dataset_class = TensorDataset(X_val_class, y_val_class)\nval_loader = DataLoader(val_dataset_class, batch_size=32, shuffle=False)\n\nprint(f\"Data loaders created with batch size 32\")\nprint(f\"  Training batches: {len(train_loader)}\")\nprint(f\"  Validation batches: {len(val_loader)}\")\n\n# Visualization function for training curves  \ndef plot_training_curves_detailed(history):\n    \"\"\"\n    Plot comprehensive training curves with additional insights.\n    \n    Args:\n        history: Dictionary with training metrics\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Loss curves\n    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n    if history['val_loss']:\n        axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].set_title('Training and Validation Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Accuracy curves\n    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n    if history['val_acc']:\n        axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Accuracy (%)')\n    axes[0, 1].set_title('Training and Validation Accuracy')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Loss difference (overfitting indicator)\n    if history['val_loss']:\n        loss_diff = [val - train for train, val in zip(history['train_loss'], history['val_loss'])]\n        axes[1, 0].plot(epochs, loss_diff, 'g-', linewidth=2)\n        axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n        axes[1, 0].set_xlabel('Epoch')\n        axes[1, 0].set_ylabel('Validation Loss - Training Loss')\n        axes[1, 0].set_title('Overfitting Monitor (Higher = More Overfitting)')\n        axes[1, 0].grid(True, alpha=0.3)\n    else:\n        axes[1, 0].text(0.5, 0.5, 'No validation data\\\\nfor overfitting analysis', \n                       ha='center', va='center', transform=axes[1, 0].transAxes, fontsize=12)\n        axes[1, 0].set_title('Overfitting Monitor')\n    \n    # Learning progress (accuracy improvement)\n    if len(history['train_acc']) > 1:\n        train_acc_diff = [history['train_acc'][i] - history['train_acc'][i-1] \n                         for i in range(1, len(history['train_acc']))]\n        axes[1, 1].plot(epochs[1:], train_acc_diff, 'b-', alpha=0.7, label='Train Acc Improvement')\n        \n        if history['val_acc'] and len(history['val_acc']) > 1:\n            val_acc_diff = [history['val_acc'][i] - history['val_acc'][i-1] \n                           for i in range(1, len(history['val_acc']))]\n            axes[1, 1].plot(epochs[1:], val_acc_diff, 'r-', alpha=0.7, label='Val Acc Improvement')\n        \n        axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n        axes[1, 1].set_xlabel('Epoch')\n        axes[1, 1].set_ylabel('Accuracy Improvement (%)')\n        axes[1, 1].set_title('Learning Progress')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n    else:\n        axes[1, 1].text(0.5, 0.5, 'Need more epochs\\\\nfor progress analysis', \n                       ha='center', va='center', transform=axes[1, 1].transAxes, fontsize=12)\n        axes[1, 1].set_title('Learning Progress')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print comprehensive metrics\n    print(\"\\\\n\" + \"=\"*60)\n    print(\"TRAINING SUMMARY\")\n    print(\"=\"*60)\n    \n    print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n    print(f\"Final Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n    \n    if history['val_loss']:\n        print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n        print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n        \n        # Best validation performance\n        best_val_epoch = np.argmax(history['val_acc'])\n        best_val_acc = history['val_acc'][best_val_epoch]\n        best_val_loss = history['val_loss'][best_val_epoch]\n        \n        print(f\"Best Validation Accuracy: {best_val_acc:.2f}% (Epoch {best_val_epoch + 1})\")\n        print(f\"Best Validation Loss: {best_val_loss:.4f} (Epoch {best_val_epoch + 1})\")\n        \n        # Overfitting analysis\n        final_gap = history['val_loss'][-1] - history['train_loss'][-1]\n        print(f\"Final Train-Val Loss Gap: {final_gap:.4f}\")\n        \n        if final_gap > 0.5:\n            print(\"‚ö†Ô∏è  Potential overfitting detected!\")\n        elif final_gap < 0.1:\n            print(\"‚úÖ Good generalization\")\n        else:\n            print(\"üìä Moderate generalization\")\n    \n    # Training efficiency\n    if len(history['train_acc']) >= 2:\n        total_improvement = history['train_acc'][-1] - history['train_acc'][0]\n        print(f\"Total Training Improvement: {total_improvement:.2f}%\")\n        \n        if total_improvement > 20:\n            print(\"üöÄ Excellent learning progress\")\n        elif total_improvement > 10:\n            print(\"‚úÖ Good learning progress\")\n        else:\n            print(\"üìà Moderate learning progress\")\n\nprint(\"\\\\n‚úÖ All missing definitions added successfully!\")\nprint(\"The notebook is now ready for comprehensive training demonstrations.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves with comprehensive analysis\nplot_training_curves_detailed(history)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "def plot_training_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    if history['val_loss']:\n",
    "        ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    if history['val_acc']:\n",
    "        ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"Final Training Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Training Accuracy: {history['train_acc'][-1]:.2f}%\")\n",
    "    if history['val_loss']:\n",
    "        print(f\"Final Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Optimizers: SGD vs AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers\n",
    "def compare_optimizers():\n",
    "    \"\"\"Compare SGD, Adam, and AdamW optimizers\"\"\"\n",
    "    \n",
    "    # Create identical models\n",
    "    models = {\n",
    "        'SGD': SimpleClassifier(20, 128, 5),\n",
    "        'Adam': SimpleClassifier(20, 128, 5),\n",
    "        'AdamW': SimpleClassifier(20, 128, 5)\n",
    "    }\n",
    "    \n",
    "    # Make sure they start with identical weights\n",
    "    state_dict = models['SGD'].state_dict()\n",
    "    for model in models.values():\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    # Create optimizers\n",
    "    optimizers = {\n",
    "        'SGD': optim.SGD(models['SGD'].parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4),\n",
    "        'Adam': optim.Adam(models['Adam'].parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4),\n",
    "        'AdamW': optim.AdamW(models['AdamW'].parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    }\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Track progress for each optimizer\n",
    "    histories = {name: {'loss': [], 'acc': []} for name in models.keys()}\n",
    "    \n",
    "    num_epochs = 15\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for name in models.keys():\n",
    "            model = models[name]\n",
    "            optimizer = optimizers[name]\n",
    "            \n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for data, target in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = 100.0 * correct / total\n",
    "            \n",
    "            histories[name]['loss'].append(avg_loss)\n",
    "            histories[name]['acc'].append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1:2d}:\")\n",
    "            for name in models.keys():\n",
    "                loss = histories[name]['loss'][-1]\n",
    "                acc = histories[name]['acc'][-1]\n",
    "                print(f\"  {name:5s}: Loss = {loss:.4f}, Acc = {acc:.2f}%\")\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# Compare optimizers\n",
    "print(\"Comparing optimizers...\")\n",
    "optimizer_histories = compare_optimizers()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, len(optimizer_histories['SGD']['loss']) + 1)\n",
    "colors = {'SGD': 'blue', 'Adam': 'green', 'AdamW': 'red'}\n",
    "\n",
    "# Loss comparison\n",
    "for name, history in optimizer_histories.items():\n",
    "    ax1.plot(epochs, history['loss'], color=colors[name], label=f'{name}', marker='o', markersize=3)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Optimizer Comparison: Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "for name, history in optimizer_histories.items():\n",
    "    ax2.plot(epochs, history['acc'], color=colors[name], label=f'{name}', marker='o', markersize=3)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Optimizer Comparison: Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\nFinal Results:\")\n",
    "for name, history in optimizer_histories.items():\n",
    "    final_loss = history['loss'][-1]\n",
    "    final_acc = history['acc'][-1]\n",
    "    print(f\"{name:5s}: Loss = {final_loss:.4f}, Accuracy = {final_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(\"‚Ä¢ SGD: Simple, requires learning rate tuning, benefits from momentum\")\n",
    "print(\"‚Ä¢ Adam: Adaptive learning rates, fast convergence, can overfit\")\n",
    "print(\"‚Ä¢ AdamW: Adam with decoupled weight decay, better generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Common Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different loss functions\n",
    "print(\"=== Common Loss Functions Demo ===\")\n",
    "\n",
    "# 1. Classification: CrossEntropyLoss\n",
    "print(\"\\n1. CrossEntropyLoss (Multi-class Classification)\")\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create sample data\n",
    "batch_size, num_classes = 4, 5\n",
    "logits = torch.randn(batch_size, num_classes)  # Raw scores from model\n",
    "targets = torch.randint(0, num_classes, (batch_size,))  # Class indices\n",
    "\n",
    "loss_ce = ce_loss(logits, targets)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "print(f\"CrossEntropyLoss: {loss_ce.item():.4f}\")\n",
    "\n",
    "# Show what happens with perfect predictions\n",
    "perfect_logits = torch.zeros_like(logits)\n",
    "for i, target in enumerate(targets):\n",
    "    perfect_logits[i, target] = 10.0  # High score for correct class\n",
    "perfect_loss = ce_loss(perfect_logits, targets)\n",
    "print(f\"Perfect prediction loss: {perfect_loss.item():.4f}\")\n",
    "\n",
    "# 2. Regression: MSELoss\n",
    "print(\"\\n2. MSELoss (Regression)\")\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "predictions = torch.randn(batch_size, 1)\n",
    "true_values = torch.randn(batch_size, 1)\n",
    "\n",
    "loss_mse = mse_loss(predictions, true_values)\n",
    "print(f\"Predictions: {predictions.squeeze()}\")\n",
    "print(f\"True values: {true_values.squeeze()}\")\n",
    "print(f\"MSELoss: {loss_mse.item():.4f}\")\n",
    "\n",
    "# Show difference between MSE and MAE\n",
    "mae_loss = nn.L1Loss()\n",
    "loss_mae = mae_loss(predictions, true_values)\n",
    "print(f\"MAE (L1) Loss: {loss_mae.item():.4f}\")\n",
    "\n",
    "# 3. Binary Classification: BCEWithLogitsLoss\n",
    "print(\"\\n3. BCEWithLogitsLoss (Binary Classification)\")\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "binary_logits = torch.randn(batch_size, 1)  # Raw logits\n",
    "binary_targets = torch.randint(0, 2, (batch_size, 1)).float()  # 0 or 1\n",
    "\n",
    "loss_bce = bce_loss(binary_logits, binary_targets)\n",
    "print(f\"Binary logits: {binary_logits.squeeze()}\")\n",
    "print(f\"Binary targets: {binary_targets.squeeze()}\")\n",
    "print(f\"BCEWithLogitsLoss: {loss_bce.item():.4f}\")\n",
    "\n",
    "# Compare with manual BCE calculation\n",
    "probs = torch.sigmoid(binary_logits)\n",
    "print(f\"Converted to probabilities: {probs.squeeze()}\")\n",
    "\n",
    "# 4. Negative Log Likelihood: NLLLoss\n",
    "print(\"\\n4. NLLLoss (when you already have log probabilities)\")\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "# NLLLoss expects log probabilities\n",
    "log_probs = F.log_softmax(logits, dim=1)\n",
    "loss_nll = nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"Log probabilities shape: {log_probs.shape}\")\n",
    "print(f\"NLLLoss: {loss_nll.item():.4f}\")\n",
    "print(f\"Note: CrossEntropyLoss = LogSoftmax + NLLLoss\")\n",
    "print(f\"Verification - CE loss: {loss_ce.item():.4f}, NLL loss: {loss_nll.item():.4f}\")\n",
    "print(f\"Match: {abs(loss_ce.item() - loss_nll.item()) < 1e-6}\")\n",
    "\n",
    "# 5. Multi-label classification: BCEWithLogitsLoss with multiple outputs\n",
    "print(\"\\n5. Multi-label Classification\")\n",
    "num_labels = 3\n",
    "multi_logits = torch.randn(batch_size, num_labels)\n",
    "multi_targets = torch.randint(0, 2, (batch_size, num_labels)).float()  # Multiple binary labels\n",
    "\n",
    "multi_bce_loss = nn.BCEWithLogitsLoss()\n",
    "loss_multi = multi_bce_loss(multi_logits, multi_targets)\n",
    "\n",
    "print(f\"Multi-label logits shape: {multi_logits.shape}\")\n",
    "print(f\"Multi-label targets shape: {multi_targets.shape}\")\n",
    "print(f\"Multi-label BCE loss: {loss_multi.item():.4f}\")\n",
    "print(f\"Sample targets: {multi_targets[0]}  (can have multiple 1s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Train vs Eval Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate train vs eval mode differences\n",
    "print(\"=== Train vs Eval Mode Demo ===\")\n",
    "\n",
    "# Create model with dropout and batch norm\n",
    "class ModelWithDropoutAndBN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 50% dropout\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size//2)\n",
    "        self.dropout2 = nn.Dropout(0.3)  # 30% dropout\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropoutAndBN(20, 128, 5)\n",
    "x = torch.randn(10, 20)  # Batch of 10 samples\n",
    "\n",
    "print(\"1. Dropout behavior:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Training mode: dropout is active\n",
    "model.train()\n",
    "print(\"Training mode (dropout active):\")\n",
    "out1 = model(x)\n",
    "out2 = model(x)  # Same input, different output due to dropout\n",
    "\n",
    "print(f\"Output 1 mean: {out1.mean().item():.4f}\")\n",
    "print(f\"Output 2 mean: {out2.mean().item():.4f}\")\n",
    "print(f\"Outputs are different: {not torch.allclose(out1, out2)}\")\n",
    "\n",
    "# Evaluation mode: dropout is disabled\n",
    "model.eval()\n",
    "print(\"\\nEvaluation mode (dropout disabled):\")\n",
    "out3 = model(x)\n",
    "out4 = model(x)  # Same input, same output\n",
    "\n",
    "print(f\"Output 3 mean: {out3.mean().item():.4f}\")\n",
    "print(f\"Output 4 mean: {out4.mean().item():.4f}\")\n",
    "print(f\"Outputs are identical: {torch.allclose(out3, out4)}\")\n",
    "\n",
    "print(\"\\n2. BatchNorm behavior:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# BatchNorm tracks running statistics differently in train vs eval\n",
    "def check_bn_stats(model, mode_name):\n",
    "    bn_layer = model.bn1\n",
    "    print(f\"{mode_name} mode:\")\n",
    "    print(f\"  Running mean: {bn_layer.running_mean[:5]}\")\n",
    "    print(f\"  Running var:  {bn_layer.running_var[:5]}\")\n",
    "    print(f\"  Training: {bn_layer.training}\")\n",
    "\n",
    "# Reset batch norm statistics\n",
    "def reset_bn_stats(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.BatchNorm1d):\n",
    "            module.reset_running_stats()\n",
    "\n",
    "reset_bn_stats(model)\n",
    "\n",
    "# Training mode: updates running statistics\n",
    "model.train()\n",
    "_ = model(x)\n",
    "check_bn_stats(model, \"Training\")\n",
    "\n",
    "# Evaluation mode: uses fixed running statistics\n",
    "model.eval()\n",
    "old_running_mean = model.bn1.running_mean.clone()\n",
    "_ = model(x)\n",
    "check_bn_stats(model, \"Evaluation\")\n",
    "\n",
    "print(f\"Running mean changed in eval: {not torch.allclose(old_running_mean, model.bn1.running_mean)}\")\n",
    "\n",
    "print(\"\\n3. Practical implications:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"‚úì Always call model.train() before training\")\n",
    "print(\"‚úì Always call model.eval() before inference\")\n",
    "print(\"‚úì Use torch.no_grad() during inference to save memory\")\n",
    "print(\"‚úì Dropout provides regularization during training\")\n",
    "print(\"‚úì BatchNorm uses batch statistics in training, running stats in eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate correct inference pattern\n",
    "def inference_example():\n",
    "    \"\"\"Show proper inference setup\"\"\"\n",
    "    \n",
    "    # Assume we have a trained model\n",
    "    model = SimpleClassifier(20, 64, 5)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Sample test data\n",
    "    test_data = torch.randn(100, 20)\n",
    "    \n",
    "    print(\"Inference setup:\")\n",
    "    \n",
    "    # Method 1: Basic inference\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        predictions = model(test_data)\n",
    "        probabilities = F.softmax(predictions, dim=1)\n",
    "        predicted_classes = torch.argmax(predictions, dim=1)\n",
    "    \n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Sample probabilities: {probabilities[0]}\")\n",
    "    print(f\"Predicted classes: {predicted_classes[:10]}\")\n",
    "    \n",
    "    # Method 2: Batch processing for large datasets\n",
    "    def batch_inference(model, data, batch_size=32):\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                batch_pred = model(batch)\n",
    "                all_predictions.append(batch_pred)\n",
    "        \n",
    "        return torch.cat(all_predictions, dim=0)\n",
    "    \n",
    "    # Process large dataset in batches\n",
    "    large_test_data = torch.randn(1000, 20)\n",
    "    batch_predictions = batch_inference(model, large_test_data, batch_size=64)\n",
    "    \n",
    "    print(f\"\\nBatch inference on {len(large_test_data)} samples:\")\n",
    "    print(f\"Output shape: {batch_predictions.shape}\")\n",
    "    \n",
    "    # Memory usage comparison\n",
    "    import torch.cuda\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nMemory usage patterns:\")\n",
    "        print(\"‚Ä¢ torch.no_grad() reduces memory usage by ~2x\")\n",
    "        print(\"‚Ä¢ Batch processing prevents OOM on large datasets\")\n",
    "        print(\"‚Ä¢ model.eval() ensures consistent results\")\n",
    "\n",
    "inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate learning rate scheduling\n",
    "print(\"=== Learning Rate Scheduling Demo ===\")\n",
    "\n",
    "def demonstrate_lr_scheduling():\n",
    "    \"\"\"Show different learning rate scheduling strategies\"\"\"\n",
    "    \n",
    "    # Create a simple model and optimizer\n",
    "    model = SimpleClassifier(20, 64, 5)\n",
    "    base_lr = 0.1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    \n",
    "    # Different schedulers\n",
    "    schedulers = {\n",
    "        'StepLR': optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5),\n",
    "        'ExponentialLR': optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95),\n",
    "        'CosineAnnealing': optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.001),\n",
    "        'ReduceLROnPlateau': optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "    }\n",
    "    \n",
    "    # Track learning rates for each scheduler\n",
    "    lr_histories = {name: [] for name in schedulers.keys()}\n",
    "    lr_histories['No Scheduling'] = []\n",
    "    \n",
    "    # Simulate training with different schedulers\n",
    "    for name, scheduler in schedulers.items():\n",
    "        # Reset optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=base_lr)\n",
    "        if name != 'ReduceLROnPlateau':\n",
    "            scheduler = type(scheduler)(optimizer, **scheduler.state_dict())\n",
    "        else:\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            lr_histories[name].append(current_lr)\n",
    "            \n",
    "            # For ReduceLROnPlateau, we need to provide a metric\n",
    "            if name == 'ReduceLROnPlateau':\n",
    "                # Simulate a loss that decreases then plateaus\n",
    "                fake_loss = 1.0 * np.exp(-epoch/10) + 0.1 + 0.05 * np.random.random()\n",
    "                if epoch > 20:  # Start plateauing\n",
    "                    fake_loss = 0.15 + 0.02 * np.random.random()\n",
    "                scheduler.step(fake_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    \n",
    "    # Add no scheduling baseline\n",
    "    lr_histories['No Scheduling'] = [base_lr] * num_epochs\n",
    "    \n",
    "    return lr_histories\n",
    "\n",
    "# Generate learning rate histories\n",
    "lr_histories = demonstrate_lr_scheduling()\n",
    "\n",
    "# Plot learning rate schedules\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "epochs = range(len(lr_histories['No Scheduling']))\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "\n",
    "for i, (name, lr_history) in enumerate(lr_histories.items()):\n",
    "    plt.plot(epochs, lr_history, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Scheduling Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to see all schedules clearly\n",
    "plt.show()\n",
    "\n",
    "# Explain each scheduler\n",
    "print(\"\\nScheduler explanations:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"StepLR: Reduces LR by factor Œ≥ every step_size epochs\")\n",
    "print(\"ExponentialLR: Multiplies LR by Œ≥ each epoch\")\n",
    "print(\"CosineAnnealing: Follows cosine curve from max to min LR\")\n",
    "print(\"ReduceLROnPlateau: Reduces LR when metric stops improving\")\n",
    "print(\"No Scheduling: Constant learning rate\")\n",
    "\n",
    "print(\"\\nWhen to use each:\")\n",
    "print(\"‚Ä¢ StepLR: Simple, works well with SGD\")\n",
    "print(\"‚Ä¢ ExponentialLR: Smooth decay, good for long training\")\n",
    "print(\"‚Ä¢ CosineAnnealing: Popular for transformers, smooth restart\")\n",
    "print(\"‚Ä¢ ReduceLROnPlateau: Adaptive, responds to actual performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Learning rate warmup\n",
    "class WarmupScheduler:\n",
    "    \"\"\"Learning rate warmup followed by decay\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, warmup_epochs, max_lr, total_epochs):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_lr = max_lr\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            lr = self.max_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
    "        else:\n",
    "            # Cosine decay after warmup\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = 0.5 * self.max_lr * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        self.current_epoch += 1\n",
    "        return lr\n",
    "\n",
    "# Demonstrate warmup scheduling\n",
    "model = SimpleClassifier(20, 64, 5)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)  # Will be overridden\n",
    "\n",
    "warmup_scheduler = WarmupScheduler(\n",
    "    optimizer=optimizer,\n",
    "    warmup_epochs=10,\n",
    "    max_lr=0.01,\n",
    "    total_epochs=100\n",
    ")\n",
    "\n",
    "# Track warmup schedule\n",
    "warmup_lrs = []\n",
    "for epoch in range(100):\n",
    "    lr = warmup_scheduler.step()\n",
    "    warmup_lrs.append(lr)\n",
    "\n",
    "# Plot warmup schedule\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(100), warmup_lrs, 'b-', linewidth=2, label='Warmup + Cosine Decay')\n",
    "plt.axvline(x=10, color='r', linestyle='--', alpha=0.7, label='End of Warmup')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Warmup + Cosine Decay')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Warmup benefits:\")\n",
    "print(\"‚Ä¢ Prevents early training instability\")\n",
    "print(\"‚Ä¢ Especially important for large batch sizes\")\n",
    "print(\"‚Ä¢ Common in transformer training\")\n",
    "print(\"‚Ä¢ Allows higher maximum learning rates\")\n",
    "\n",
    "print(\"\\nüéâ Optimization & Training exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"‚Ä¢ Use a systematic training loop structure\")\n",
    "print(\"‚Ä¢ AdamW is generally a good default optimizer\")\n",
    "print(\"‚Ä¢ Match loss function to your task type\")\n",
    "print(\"‚Ä¢ Always set model.train()/model.eval() appropriately\")\n",
    "print(\"‚Ä¢ Learning rate scheduling can significantly improve results\")\n",
    "print(\"‚Ä¢ Monitor both training and validation metrics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}