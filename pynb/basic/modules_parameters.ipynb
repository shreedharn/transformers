{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PyTorch Modules & Parameters: Neural Network Building Blocks\n\n## ðŸŽ¯ Introduction\n\nWelcome to the architectural world of PyTorch! This notebook will transform you from someone who writes individual tensor operations into someone who designs elegant, reusable neural network components. Understanding modules and parameters is the key to building scalable deep learning systems.\n\n### ðŸ§  What You'll Master\n\nThis comprehensive guide covers:\n- **Module hierarchy**: How PyTorch organizes neural network components\n- **Parameter management**: Tracking, initializing, and optimizing learnable weights\n- **Module composition**: Building complex architectures from simple components\n- **Memory efficiency**: Understanding parameter sharing and storage\n- **Advanced patterns**: Custom modules, hooks, and state management\n\n### ðŸŽ“ Prerequisites\n\n- Solid understanding of tensors and basic operations\n- Familiarity with autograd and gradient computation\n- Basic knowledge of neural network concepts (layers, weights, biases)\n\n### ðŸš€ Why Module Design Matters\n\nProper module design enables:\n- **Modularity**: Reusable components that compose elegantly\n- **Automatic differentiation**: Parameters tracked and optimized automatically\n- **GPU acceleration**: Seamless device transfers and distributed training\n- **Save/load functionality**: Model persistence and deployment\n- **Debugging**: Clear component boundaries and parameter inspection\n\n---\n\n## ðŸ“š Table of Contents\n\n1. **[Module Fundamentals](#module-fundamentals)** - Understanding the nn.Module base class\n2. **[Parameter Management](#parameter-management)** - How PyTorch tracks and optimizes parameters\n3. **[Module Composition Patterns](#module-composition-patterns)** - Building complex architectures\n4. **[Parameter Initialization Strategies](#parameter-initialization-strategies)** - Setting up weights for successful training\n5. **[Advanced Module Patterns](#advanced-module-patterns)** - Custom components and optimization tricks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Module Fundamentals\n\n### ðŸ—ï¸ The nn.Module Foundation\n\nEvery neural network in PyTorch inherits from `nn.Module`. This base class provides the infrastructure for parameter tracking, device management, training state, and much more. Let's explore what makes modules so powerful!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# UNDERSTANDING nn.MODULE ARCHITECTURE\n# =============================================================================\n\nprint(\"ðŸ—ï¸ The Power of nn.Module\")\nprint(\"=\" * 50)\n\n# Create a simple linear layer to explore module functionality\nlinear_layer = nn.Linear(4, 2)  # Input: 4 features â†’ Output: 2 features\n\nprint(f\"Module type: {type(linear_layer)}\")\nprint(f\"Module representation: {linear_layer}\")\n\n# nn.Module automatically tracks parameters\nprint(f\"\\nParameters found by PyTorch:\")\nfor name, param in linear_layer.named_parameters():\n    print(f\"  {name}: shape {param.shape}, requires_grad={param.requires_grad}\")\n\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in linear_layer.parameters()):,}\")\n\nprint(f\"\\nðŸ” Module State Information\")\nprint(\"=\" * 50)\n\n# Modules track training vs evaluation state\nprint(f\"Training mode: {linear_layer.training}\")\nprint(f\"Device: {next(linear_layer.parameters()).device}\")\n\n# Submodules (empty for simple linear layer)\nprint(f\"Named submodules: {list(linear_layer.named_children())}\")\n\n# Built-in methods for parameter management\nprint(f\"\\nBuilt-in parameter methods:\")\nprint(f\"  .parameters() - iterator over all parameters\")\nprint(f\"  .named_parameters() - parameter names and tensors\")\nprint(f\"  .state_dict() - complete state for saving/loading\")\nprint(f\"  .train()/.eval() - switch between training/inference modes\")\nprint(f\"  .to(device) - move all parameters to device\")\nprint(f\"  .zero_grad() - clear gradients of all parameters\")\n\nprint(f\"\\nðŸŽ¯ Why This Design is Brilliant\")\nprint(\"=\" * 50)\nprint(\"â€¢ Parameters automatically registered and tracked\")\nprint(\"â€¢ Device management handled transparently\")\nprint(\"â€¢ Training/evaluation state managed automatically\")  \nprint(\"â€¢ Save/load functionality built-in\")\nprint(\"â€¢ Gradient computation integrated seamlessly\")\nprint(\"â€¢ Module composition works recursively\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": "## Parameter Management\n\n### ðŸŽ›ï¸ How PyTorch Tracks and Optimizes Parameters\n\nParameters are the heart of neural networks - they're the learnable weights that get updated during training. PyTorch's parameter management system is designed to make this process seamless and efficient."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# DEEP DIVE INTO PARAMETER MECHANICS\n# =============================================================================\n\nprint(\"ðŸŽ›ï¸ Parameter Deep Dive\")\nprint(\"=\" * 50)\n\n# Create a module to examine parameter behavior\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        # These become registered parameters automatically\n        self.fc1 = nn.Linear(input_size, hidden_size)    # weight + bias parameters\n        self.fc2 = nn.Linear(hidden_size, output_size)   # weight + bias parameters\n        \n        # Manual parameter registration (less common but useful)\n        self.manual_param = nn.Parameter(torch.randn(hidden_size, 1))\n        \n        # Non-parameter tensors (not optimized)\n        self.register_buffer('running_mean', torch.zeros(hidden_size))\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Instantiate model to explore parameter structure\nmodel = SimpleModel(input_size=5, hidden_size=10, output_size=3)\n\nprint(f\"Model architecture:\")\nprint(model)\n\nprint(f\"\\nðŸ“Š Parameter Analysis\")\nprint(\"=\" * 50)\n\ntotal_params = 0\ntrainable_params = 0\n\nprint(\"Layer-by-layer parameter breakdown:\")\nfor name, param in model.named_parameters():\n    param_count = param.numel()\n    total_params += param_count\n    if param.requires_grad:\n        trainable_params += param_count\n    \n    print(f\"  {name:15} | Shape: {str(param.shape):15} | Count: {param_count:6,} | Trainable: {param.requires_grad}\")\n\nprint(f\"\\nParameter Summary:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"  Non-trainable parameters: {total_params - trainable_params:,}\")\n\n# Memory usage calculation\nparam_memory_mb = sum(p.numel() * 4 for p in model.parameters()) / (1024**2)  # 4 bytes per float32\nprint(f\"  Memory usage: {param_memory_mb:.2f} MB\")\n\nprint(f\"\\nðŸ” Buffers vs Parameters\")\nprint(\"=\" * 50)\n\nprint(\"Registered buffers (not optimized):\")\nfor name, buffer in model.named_buffers():\n    print(f\"  {name}: shape {buffer.shape}, device {buffer.device}\")\n\nprint(\"\\nKey differences:\")\nprint(\"â€¢ Parameters: Learnable, included in optimizer, require gradients\")\nprint(\"â€¢ Buffers: Fixed values, moved with model, not optimized\")\nprint(\"â€¢ Use buffers for: running statistics, lookup tables, constants\")\n\nprint(f\"\\nâš™ï¸ Parameter State Management\")\nprint(\"=\" * 50)\n\n# Demonstrate state dictionary functionality\nstate_dict = model.state_dict()\nprint(f\"State dict keys: {list(state_dict.keys())}\")\n\n# Show how parameters can be frozen/unfrozen\nprint(f\"\\nFreezing/unfreezing parameters:\")\nprint(\"Before freezing fc1:\")\nfor name, param in model.named_parameters():\n    if 'fc1' in name:\n        print(f\"  {name}: requires_grad = {param.requires_grad}\")\n\n# Freeze fc1 parameters (common in transfer learning)\nfor param in model.fc1.parameters():\n    param.requires_grad = False\n\nprint(\"\\nAfter freezing fc1:\")\nfor name, param in model.named_parameters():\n    if 'fc1' in name:\n        print(f\"  {name}: requires_grad = {param.requires_grad}\")\n\n# Count trainable parameters after freezing\ntrainable_after_freeze = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nTrainable parameters after freezing fc1: {trainable_after_freeze:,}\")\nprint(f\"Reduction: {total_params - trainable_after_freeze:,} parameters frozen\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "## Module Composition Patterns\n\n### ðŸ§© Building Complex Architectures\n\nThe real power of PyTorch modules comes from composition - building complex networks from simple, reusable components. This is how modern architectures like transformers are constructed!"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "# =============================================================================\n# ADVANCED MODULE COMPOSITION PATTERNS\n# =============================================================================\n\nprint(\"ðŸ§© Module Composition Mastery\")\nprint(\"=\" * 50)\n\n# Pattern 1: Sequential composition (most common)\nclass MLPBlock(nn.Module):\n    \"\"\"A reusable MLP block with configurable activation and dropout.\"\"\"\n    \n    def __init__(self, input_dim, output_dim, dropout_rate=0.1, activation='relu'):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Flexible activation selection\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        else:\n            raise ValueError(f\"Unknown activation: {activation}\")\n    \n    def forward(self, x):\n        # Standard pattern: linear â†’ activation â†’ dropout\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        return x\n\n# Pattern 2: Container-based composition\nclass FlexibleMLP(nn.Module):\n    \"\"\"MLP with variable depth using nn.ModuleList.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dims, output_dim, dropout_rate=0.1):\n        super().__init__()\n        \n        # Input projection\n        self.input_layer = MLPBlock(input_dim, hidden_dims[0], dropout_rate)\n        \n        # Hidden layers using ModuleList for dynamic composition\n        self.hidden_layers = nn.ModuleList([\n            MLPBlock(hidden_dims[i], hidden_dims[i+1], dropout_rate)\n            for i in range(len(hidden_dims) - 1)\n        ])\n        \n        # Output projection (no activation/dropout typically)\n        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n        \n        print(f\"Created FlexibleMLP with {len(hidden_dims)} hidden layers\")\n        print(f\"Architecture: {input_dim} â†’ {' â†’ '.join(map(str, hidden_dims))} â†’ {output_dim}\")\n    \n    def forward(self, x):\n        # Process through all layers sequentially\n        x = self.input_layer(x)\n        \n        for layer in self.hidden_layers:\n            x = layer(x)\n        \n        x = self.output_layer(x)\n        return x\n\n# Demonstrate flexible architecture creation\nprint(\"Example 1: Simple 2-layer MLP\")\nsimple_mlp = FlexibleMLP(\n    input_dim=10,\n    hidden_dims=[64, 32],  # Two hidden layers\n    output_dim=5,\n    dropout_rate=0.2\n)\n\nprint(f\"\\nExample 2: Deep 5-layer MLP\")\ndeep_mlp = FlexibleMLP(\n    input_dim=50,\n    hidden_dims=[256, 128, 64, 32, 16],  # Five hidden layers  \n    output_dim=10,\n    dropout_rate=0.1\n)\n\nprint(f\"\\nðŸ“Š Parameter Comparison\")\nprint(\"=\" * 50)\n\nsimple_params = sum(p.numel() for p in simple_mlp.parameters())\ndeep_params = sum(p.numel() for p in deep_mlp.parameters())\n\nprint(f\"Simple MLP parameters: {simple_params:,}\")\nprint(f\"Deep MLP parameters: {deep_params:,}\")\nprint(f\"Parameter ratio (deep/simple): {deep_params / simple_params:.1f}x\")\n\n# Pattern 3: Residual connections (transformer-style)\nclass ResidualBlock(nn.Module):\n    \"\"\"Block with residual connection - foundation of modern architectures.\"\"\"\n    \n    def __init__(self, dim, expansion_factor=4, dropout_rate=0.1):\n        super().__init__()\n        hidden_dim = dim * expansion_factor\n        \n        # Two-layer feedforward with expansion and contraction\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),  # Modern activation choice\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout_rate)\n        )\n        \n        # Layer normalization (applied before FFN in modern architectures)\n        self.norm = nn.LayerNorm(dim)\n    \n    def forward(self, x):\n        # Pre-norm residual connection pattern\n        # This is the pattern used in GPT, T5, and other modern models\n        residual = x\n        x = self.norm(x)        # Normalize first\n        x = self.ffn(x)         # Apply transformation\n        x = x + residual        # Add residual connection\n        return x\n\n# Pattern 4: Transformer-style stacking\nclass MiniTransformerBlock(nn.Module):\n    \"\"\"Simplified transformer block showing composition patterns.\"\"\"\n    \n    def __init__(self, d_model, num_layers=3, expansion_factor=4, dropout_rate=0.1):\n        super().__init__()\n        \n        # Stack multiple residual blocks\n        self.layers = nn.ModuleList([\n            ResidualBlock(d_model, expansion_factor, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        \n        # Final normalization\n        self.final_norm = nn.LayerNorm(d_model)\n    \n    def forward(self, x):\n        # Process through each layer\n        for layer in self.layers:\n            x = layer(x)\n        \n        # Final normalization\n        x = self.final_norm(x)\n        return x\n\n# Test the transformer-style architecture\nprint(f\"\\nðŸ—ï¸ Advanced Architecture Example\")\nprint(\"=\" * 50)\n\nmini_transformer = MiniTransformerBlock(d_model=128, num_layers=4)\n\nprint(f\"Mini-transformer parameters: {sum(p.numel() for p in mini_transformer.parameters()):,}\")\n\n# Test with realistic input\nbatch_size, seq_len, d_model = 2, 10, 128\ntest_input = torch.randn(batch_size, seq_len, d_model)\noutput = mini_transformer(test_input)\n\nprint(f\"Input shape: {test_input.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"âœ“ Shape preserved through all layers (essential for residual connections)\")\n\nprint(f\"\\nðŸ’¡ Composition Principles\")\nprint(\"=\" * 50)\nprint(\"1. **Modularity**: Each component has a single, clear responsibility\")\nprint(\"2. **Reusability**: Blocks can be used in different contexts\")\nprint(\"3. **Composability**: Complex architectures built from simple parts\")\nprint(\"4. **Parameter sharing**: Same block type, different instances\")\nprint(\"5. **Shape consistency**: Outputs match expected inputs for next layer\")\nprint(\"6. **Gradient flow**: Residual connections enable deep networks\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different initialization strategies\n",
    "def xavier_init(m):\n",
    "    \"\"\"Xavier (Glorot) initialization - good for tanh/sigmoid activations\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def kaiming_init(m):\n",
    "    \"\"\"Kaiming (He) initialization - good for ReLU activations\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, std=0.1)\n",
    "\n",
    "def normal_init(m):\n",
    "    \"\"\"Simple normal initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, std=0.1)\n",
    "\n",
    "# Compare initialization effects\n",
    "def compare_initializations():\n",
    "    \"\"\"Compare different initialization strategies\"\"\"\n",
    "    \n",
    "    # Create three identical models\n",
    "    model_xavier = SimpleNet(100, 50, 10)\n",
    "    model_kaiming = SimpleNet(100, 50, 10)\n",
    "    model_normal = SimpleNet(100, 50, 10)\n",
    "    \n",
    "    # Apply different initializations\n",
    "    model_xavier.apply(xavier_init)\n",
    "    model_kaiming.apply(kaiming_init)\n",
    "    model_normal.apply(normal_init)\n",
    "    \n",
    "    models = {\n",
    "        'Xavier (Glorot)': model_xavier,\n",
    "        'Kaiming (He)': model_kaiming,\n",
    "        'Normal (0.01)': model_normal\n",
    "    }\n",
    "    \n",
    "    # Test with random input\n",
    "    x = torch.randn(32, 100)  # Batch of 32 samples\n",
    "    \n",
    "    print(\"Initialization comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()  # Set to eval mode for consistent comparison\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            \n",
    "        # Analyze weight statistics\n",
    "        fc1_weight = model.fc1.weight\n",
    "        fc2_weight = model.fc2.weight\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  FC1 weight stats: mean={fc1_weight.mean().item():.6f}, std={fc1_weight.std().item():.6f}\")\n",
    "        print(f\"  FC2 weight stats: mean={fc2_weight.mean().item():.6f}, std={fc2_weight.std().item():.6f}\")\n",
    "        print(f\"  Output stats: mean={output.mean().item():.6f}, std={output.std().item():.6f}\")\n",
    "        print(f\"  Output range: [{output.min().item():.6f}, {output.max().item():.6f}]\")\n",
    "\n",
    "compare_initializations()\n",
    "\n",
    "# Demonstrate the impact of bad initialization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Effect of bad initialization:\")\n",
    "\n",
    "def bad_init(m):\n",
    "    \"\"\"Intentionally bad initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.constant_(m.weight, 10.0)  # Too large!\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "model_bad = SimpleNet(10, 20, 5)\n",
    "model_bad.apply(bad_init)\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "with torch.no_grad():\n",
    "    output_bad = model_bad(x)\n",
    "    \n",
    "print(f\"Bad initialization output stats:\")\n",
    "print(f\"  Mean: {output_bad.mean().item():.2f}\")\n",
    "print(f\"  Std: {output_bad.std().item():.2f}\")\n",
    "print(f\"  Range: [{output_bad.min().item():.2f}, {output_bad.max().item():.2f}]\")\n",
    "print(f\"  Contains NaN: {torch.isnan(output_bad).any().item()}\")\n",
    "print(\"\\nNote: Large values can lead to vanishing/exploding gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distributions after initialization\n",
    "def visualize_weight_distributions():\n",
    "    # Create models with different initializations\n",
    "    model_xavier = nn.Linear(100, 100)\n",
    "    model_kaiming = nn.Linear(100, 100)\n",
    "    model_normal = nn.Linear(100, 100)\n",
    "    \n",
    "    nn.init.xavier_uniform_(model_xavier.weight)\n",
    "    nn.init.kaiming_uniform_(model_kaiming.weight, nonlinearity='relu')\n",
    "    nn.init.normal_(model_normal.weight, std=0.01)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    weights = {\n",
    "        'Xavier': model_xavier.weight.detach().numpy().flatten(),\n",
    "        'Kaiming': model_kaiming.weight.detach().numpy().flatten(),\n",
    "        'Normal (0.01)': model_normal.weight.detach().numpy().flatten()\n",
    "    }\n",
    "    \n",
    "    for i, (name, weight) in enumerate(weights.items()):\n",
    "        axes[i].hist(weight, bins=50, alpha=0.7, density=True)\n",
    "        axes[i].set_title(f'{name} Initialization')\n",
    "        axes[i].set_xlabel('Weight Value')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean, std = weight.mean(), weight.std()\n",
    "        axes[i].axvline(mean, color='red', linestyle='--', label=f'Mean: {mean:.4f}')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_title(f'{name}\\nMean: {mean:.4f}, Std: {std:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insights:\")\n",
    "    print(\"â€¢ Xavier: Balanced for tanh/sigmoid activations\")\n",
    "    print(\"â€¢ Kaiming: Wider distribution for ReLU activations\")\n",
    "    print(\"â€¢ Normal (0.01): Very narrow, might cause vanishing gradients\")\n",
    "\n",
    "visualize_weight_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Advanced Module Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom module with learnable parameters\n",
    "class CustomLinear(nn.Module):\n",
    "    \"\"\"Custom linear layer to demonstrate parameter creation\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Create learnable parameters\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            # Register as None so it doesn't appear in parameters()\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Manual implementation of linear transformation\n",
    "        output = torch.matmul(x, self.weight.t())\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # Custom string representation\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "# Test custom linear layer\n",
    "import math\n",
    "\n",
    "custom_linear = CustomLinear(10, 5)\n",
    "builtin_linear = nn.Linear(10, 5)\n",
    "\n",
    "print(\"Custom linear layer:\")\n",
    "print(custom_linear)\n",
    "print(f\"Parameters: {sum(p.numel() for p in custom_linear.parameters())}\")\n",
    "\n",
    "# Test they produce similar results\n",
    "x = torch.randn(3, 10)\n",
    "output_custom = custom_linear(x)\n",
    "output_builtin = builtin_linear(x)\n",
    "\n",
    "print(f\"\\nOutput shapes - Custom: {output_custom.shape}, Built-in: {output_builtin.shape}\")\n",
    "print(\"Custom and built-in linear layers work equivalently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module with submodules and parameter sharing\n",
    "class ModularNet(nn.Module):\n",
    "    \"\"\"Demonstrate modular architecture and parameter sharing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Shared transformation (parameter sharing)\n",
    "        self.shared_transform = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer-specific transformations\n",
    "        self.layer_transforms = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization for each layer\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = F.relu(self.input_proj(x))\n",
    "        \n",
    "        # Process through layers\n",
    "        for i in range(self.num_layers):\n",
    "            residual = x\n",
    "            \n",
    "            # Apply shared transformation (parameter sharing across layers)\n",
    "            x = self.shared_transform(x)\n",
    "            \n",
    "            # Apply layer-specific transformation\n",
    "            x = self.layer_transforms[i](x)\n",
    "            \n",
    "            # Residual connection and layer norm\n",
    "            x = self.layer_norms[i](x + residual)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "\n",
    "# Create and analyze modular network\n",
    "modular_net = ModularNet(input_size=64, hidden_size=128, num_layers=4)\n",
    "\n",
    "print(\"Modular Network Architecture:\")\n",
    "print(modular_net)\n",
    "\n",
    "print(\"\\nParameter analysis:\")\n",
    "total_params = 0\n",
    "for name, param in modular_net.named_parameters():\n",
    "    print(f\"{name:30s}: {str(param.shape):20s} {param.numel():>8,d}\")\n",
    "    total_params += param.numel()\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Note: shared_transform parameters are used across all layers\n",
    "print(f\"\\nShared parameters (used {modular_net.num_layers} times): {modular_net.shared_transform.weight.numel() + modular_net.shared_transform.bias.numel():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(5, 64)\n",
    "output = modular_net(x)\n",
    "print(f\"\\nForward pass: {x.shape} -> {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hooks for monitoring activations and gradients\n",
    "class MonitoredNet(nn.Module):\n",
    "    \"\"\"Network with built-in monitoring capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Storage for activations and gradients\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Register hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        def save_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                self.activations[name] = output.detach()\n",
    "            return hook\n",
    "        \n",
    "        def save_gradient(name):\n",
    "            def hook(module, grad_input, grad_output):\n",
    "                if grad_output[0] is not None:\n",
    "                    self.gradients[name] = grad_output[0].detach()\n",
    "            return hook\n",
    "        \n",
    "        # Register forward and backward hooks\n",
    "        self.fc1.register_forward_hook(save_activation('fc1'))\n",
    "        self.fc2.register_forward_hook(save_activation('fc2'))\n",
    "        self.fc3.register_forward_hook(save_activation('fc3'))\n",
    "        \n",
    "        self.fc1.register_backward_hook(save_gradient('fc1'))\n",
    "        self.fc2.register_backward_hook(save_gradient('fc2'))\n",
    "        self.fc3.register_backward_hook(save_gradient('fc3'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_activation_stats(self):\n",
    "        \"\"\"Get statistics about activations\"\"\"\n",
    "        stats = {}\n",
    "        for name, activation in self.activations.items():\n",
    "            stats[name] = {\n",
    "                'mean': activation.mean().item(),\n",
    "                'std': activation.std().item(),\n",
    "                'min': activation.min().item(),\n",
    "                'max': activation.max().item(),\n",
    "                'zeros': (activation == 0).float().mean().item()  # Sparsity for ReLU\n",
    "            }\n",
    "        return stats\n",
    "    \n",
    "    def get_gradient_stats(self):\n",
    "        \"\"\"Get statistics about gradients\"\"\"\n",
    "        stats = {}\n",
    "        for name, gradient in self.gradients.items():\n",
    "            stats[name] = {\n",
    "                'mean': gradient.mean().item(),\n",
    "                'std': gradient.std().item(),\n",
    "                'norm': gradient.norm().item()\n",
    "            }\n",
    "        return stats\n",
    "\n",
    "# Test monitored network\n",
    "monitored_net = MonitoredNet(20, 50, 5)\n",
    "optimizer = optim.SGD(monitored_net.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward and backward pass\n",
    "x = torch.randn(10, 20)\n",
    "y_true = torch.randn(10, 5)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = monitored_net(x)\n",
    "loss = criterion(y_pred, y_true)\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Analyze activations and gradients\n",
    "print(\"Activation Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "activation_stats = monitored_net.get_activation_stats()\n",
    "for layer, stats in activation_stats.items():\n",
    "    print(f\"{layer}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "    print(f\"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "    print(f\"  Sparsity (zeros): {stats['zeros']:.2%}\")\n",
    "\n",
    "print(\"\\nGradient Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "gradient_stats = monitored_net.get_gradient_stats()\n",
    "for layer, stats in gradient_stats.items():\n",
    "    print(f\"{layer}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}\")\n",
    "    print(f\"  Norm: {stats['norm']:.6f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Module exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"â€¢ Use nn.Module as base class for all models\")\n",
    "print(\"â€¢ Parameters are automatically tracked when using nn.Parameter\")\n",
    "print(\"â€¢ Proper initialization is crucial for training success\")\n",
    "print(\"â€¢ ModuleList and ModuleDict help organize complex architectures\")\n",
    "print(\"â€¢ Hooks enable monitoring and debugging of model internals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}