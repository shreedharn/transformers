{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PyTorch Modules, Parameters, Initialization\n",
    "\n",
    "This notebook covers PyTorch's building blocks - modules, parameters, and proper initialization strategies.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Module Structure](#basic-module-structure)\n",
    "2. [Parameter Counting Formulas](#parameter-counting-formulas)\n",
    "3. [Initialization Strategies](#initialization-strategies)\n",
    "4. [Advanced Module Patterns](#advanced-module-patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Basic Module Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()  # Always call parent constructor\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout during training\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create and inspect\n",
    "model = SimpleNet(10, 20, 5)\n",
    "print(\"Model structure:\")\n",
    "print(model)\n",
    "\n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} ({param.numel()} parameters)\")\n",
    "\n",
    "# State dict for saving/loading\n",
    "state_dict = model.state_dict()\n",
    "print(f\"\\nState dict keys: {list(state_dict.keys())}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(3, 10)  # Batch of 3 samples\n",
    "output = model(x)\n",
    "print(f\"\\nInput shape: {x.shape}, Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Parameter Counting Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(layer):\n",
    "    \"\"\"Count parameters in common layer types\"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        # Linear(in_features, out_features): in*out + out (bias)\n",
    "        weight_params = layer.in_features * layer.out_features\n",
    "        bias_params = layer.out_features if layer.bias is not None else 0\n",
    "        return weight_params + bias_params\n",
    "    elif isinstance(layer, nn.Embedding):\n",
    "        # Embedding(num_embeddings, embedding_dim): num*dim\n",
    "        return layer.num_embeddings * layer.embedding_dim\n",
    "    elif isinstance(layer, nn.LSTM):\n",
    "        # LSTM has 4 gates, each with input and hidden weights + bias\n",
    "        input_size, hidden_size = layer.input_size, layer.hidden_size\n",
    "        num_layers = layer.num_layers\n",
    "        bidirectional = 2 if layer.bidirectional else 1\n",
    "        \n",
    "        # Per layer: 4 gates * (input_weights + hidden_weights + 2*bias)\n",
    "        # Note: LSTM has input bias and hidden bias for each gate\n",
    "        per_layer = 4 * (input_size * hidden_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "        \n",
    "        # First layer uses input_size, subsequent layers use hidden_size as input\n",
    "        if num_layers > 1:\n",
    "            first_layer = 4 * (input_size * hidden_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "            other_layers = (num_layers - 1) * 4 * (hidden_size * hidden_size + hidden_size * hidden_size + 2 * hidden_size)\n",
    "            return (first_layer + other_layers) * bidirectional\n",
    "        else:\n",
    "            return per_layer * bidirectional\n",
    "    else:\n",
    "        return sum(p.numel() for p in layer.parameters())\n",
    "\n",
    "# Examples with verification\n",
    "print(\"=== Parameter Counting Examples ===\")\n",
    "\n",
    "# Linear layer\n",
    "linear = nn.Linear(100, 50)\n",
    "calculated = count_parameters(linear)\n",
    "actual = sum(p.numel() for p in linear.parameters())\n",
    "print(f\"Linear(100, 50):\")\n",
    "print(f\"  Calculated: {calculated} (100*50 + 50 = {100*50 + 50})\")\n",
    "print(f\"  Actual: {actual}\")\n",
    "print(f\"  Match: {calculated == actual}\")\n",
    "\n",
    "# Embedding layer\n",
    "embedding = nn.Embedding(1000, 128)\n",
    "calculated = count_parameters(embedding)\n",
    "actual = sum(p.numel() for p in embedding.parameters())\n",
    "print(f\"\\nEmbedding(1000, 128):\")\n",
    "print(f\"  Calculated: {calculated} (1000*128 = {1000*128})\")\n",
    "print(f\"  Actual: {actual}\")\n",
    "print(f\"  Match: {calculated == actual}\")\n",
    "\n",
    "# LSTM layer\n",
    "lstm = nn.LSTM(64, 32, num_layers=1, batch_first=True)\n",
    "calculated = count_parameters(lstm)\n",
    "actual = sum(p.numel() for p in lstm.parameters())\n",
    "print(f\"\\nLSTM(64, 32, layers=1):\")\n",
    "print(f\"  Calculated: {calculated}\")\n",
    "print(f\"  Actual: {actual}\")\n",
    "print(f\"  Match: {calculated == actual}\")\n",
    "\n",
    "# Multi-layer LSTM\n",
    "lstm_multi = nn.LSTM(64, 32, num_layers=2, batch_first=True)\n",
    "calculated_multi = count_parameters(lstm_multi)\n",
    "actual_multi = sum(p.numel() for p in lstm_multi.parameters())\n",
    "print(f\"\\nLSTM(64, 32, layers=2):\")\n",
    "print(f\"  Calculated: {calculated_multi}\")\n",
    "print(f\"  Actual: {actual_multi}\")\n",
    "print(f\"  Match: {calculated_multi == actual_multi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter analysis for complete model\n",
    "def analyze_model_parameters(model):\n",
    "    \"\"\"Detailed parameter analysis\"\"\"\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    print(\"Parameter breakdown:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_count = param.numel()\n",
    "        total_params += param_count\n",
    "        \n",
    "        if param.requires_grad:\n",
    "            trainable_params += param_count\n",
    "            status = \"Trainable\"\n",
    "        else:\n",
    "            status = \"Frozen\"\n",
    "        \n",
    "        print(f\"{name:20s}: {str(param.shape):15s} {param_count:8,d} ({status})\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Total parameters:':<20s} {total_params:>15,d}\")\n",
    "    print(f\"{'Trainable parameters:':<20s} {trainable_params:>15,d}\")\n",
    "    print(f\"{'Non-trainable:':<20s} {total_params - trainable_params:>15,d}\")\n",
    "    \n",
    "    # Memory estimation (rough)\n",
    "    param_memory_mb = total_params * 4 / (1024**2)  # 4 bytes per float32\n",
    "    print(f\"{'Estimated memory:':<20s} {param_memory_mb:>12.1f} MB\")\n",
    "\n",
    "# Analyze our simple model\n",
    "analyze_model_parameters(model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Create a more complex model for comparison\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(5000, 128)\n",
    "        self.lstm = nn.LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.attention = nn.MultiheadAttention(256, 8, batch_first=True)\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x, _ = self.attention(x, x, x)\n",
    "        x = self.classifier(x.mean(dim=1))  # Global average pooling\n",
    "        return x\n",
    "\n",
    "complex_model = ComplexModel()\n",
    "print(\"Complex model analysis:\")\n",
    "analyze_model_parameters(complex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Initialization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different initialization strategies\n",
    "def xavier_init(m):\n",
    "    \"\"\"Xavier (Glorot) initialization - good for tanh/sigmoid activations\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def kaiming_init(m):\n",
    "    \"\"\"Kaiming (He) initialization - good for ReLU activations\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, std=0.1)\n",
    "\n",
    "def normal_init(m):\n",
    "    \"\"\"Simple normal initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Embedding):\n",
    "        nn.init.normal_(m.weight, std=0.1)\n",
    "\n",
    "# Compare initialization effects\n",
    "def compare_initializations():\n",
    "    \"\"\"Compare different initialization strategies\"\"\"\n",
    "    \n",
    "    # Create three identical models\n",
    "    model_xavier = SimpleNet(100, 50, 10)\n",
    "    model_kaiming = SimpleNet(100, 50, 10)\n",
    "    model_normal = SimpleNet(100, 50, 10)\n",
    "    \n",
    "    # Apply different initializations\n",
    "    model_xavier.apply(xavier_init)\n",
    "    model_kaiming.apply(kaiming_init)\n",
    "    model_normal.apply(normal_init)\n",
    "    \n",
    "    models = {\n",
    "        'Xavier (Glorot)': model_xavier,\n",
    "        'Kaiming (He)': model_kaiming,\n",
    "        'Normal (0.01)': model_normal\n",
    "    }\n",
    "    \n",
    "    # Test with random input\n",
    "    x = torch.randn(32, 100)  # Batch of 32 samples\n",
    "    \n",
    "    print(\"Initialization comparison:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()  # Set to eval mode for consistent comparison\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(x)\n",
    "            \n",
    "        # Analyze weight statistics\n",
    "        fc1_weight = model.fc1.weight\n",
    "        fc2_weight = model.fc2.weight\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  FC1 weight stats: mean={fc1_weight.mean().item():.6f}, std={fc1_weight.std().item():.6f}\")\n",
    "        print(f\"  FC2 weight stats: mean={fc2_weight.mean().item():.6f}, std={fc2_weight.std().item():.6f}\")\n",
    "        print(f\"  Output stats: mean={output.mean().item():.6f}, std={output.std().item():.6f}\")\n",
    "        print(f\"  Output range: [{output.min().item():.6f}, {output.max().item():.6f}]\")\n",
    "\n",
    "compare_initializations()\n",
    "\n",
    "# Demonstrate the impact of bad initialization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Effect of bad initialization:\")\n",
    "\n",
    "def bad_init(m):\n",
    "    \"\"\"Intentionally bad initialization\"\"\"\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.constant_(m.weight, 10.0)  # Too large!\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "model_bad = SimpleNet(10, 20, 5)\n",
    "model_bad.apply(bad_init)\n",
    "\n",
    "x = torch.randn(5, 10)\n",
    "with torch.no_grad():\n",
    "    output_bad = model_bad(x)\n",
    "    \n",
    "print(f\"Bad initialization output stats:\")\n",
    "print(f\"  Mean: {output_bad.mean().item():.2f}\")\n",
    "print(f\"  Std: {output_bad.std().item():.2f}\")\n",
    "print(f\"  Range: [{output_bad.min().item():.2f}, {output_bad.max().item():.2f}]\")\n",
    "print(f\"  Contains NaN: {torch.isnan(output_bad).any().item()}\")\n",
    "print(\"\\nNote: Large values can lead to vanishing/exploding gradients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distributions after initialization\n",
    "def visualize_weight_distributions():\n",
    "    # Create models with different initializations\n",
    "    model_xavier = nn.Linear(100, 100)\n",
    "    model_kaiming = nn.Linear(100, 100)\n",
    "    model_normal = nn.Linear(100, 100)\n",
    "    \n",
    "    nn.init.xavier_uniform_(model_xavier.weight)\n",
    "    nn.init.kaiming_uniform_(model_kaiming.weight, nonlinearity='relu')\n",
    "    nn.init.normal_(model_normal.weight, std=0.01)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    weights = {\n",
    "        'Xavier': model_xavier.weight.detach().numpy().flatten(),\n",
    "        'Kaiming': model_kaiming.weight.detach().numpy().flatten(),\n",
    "        'Normal (0.01)': model_normal.weight.detach().numpy().flatten()\n",
    "    }\n",
    "    \n",
    "    for i, (name, weight) in enumerate(weights.items()):\n",
    "        axes[i].hist(weight, bins=50, alpha=0.7, density=True)\n",
    "        axes[i].set_title(f'{name} Initialization')\n",
    "        axes[i].set_xlabel('Weight Value')\n",
    "        axes[i].set_ylabel('Density')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics\n",
    "        mean, std = weight.mean(), weight.std()\n",
    "        axes[i].axvline(mean, color='red', linestyle='--', label=f'Mean: {mean:.4f}')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_title(f'{name}\\nMean: {mean:.4f}, Std: {std:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insights:\")\n",
    "    print(\"â€¢ Xavier: Balanced for tanh/sigmoid activations\")\n",
    "    print(\"â€¢ Kaiming: Wider distribution for ReLU activations\")\n",
    "    print(\"â€¢ Normal (0.01): Very narrow, might cause vanishing gradients\")\n",
    "\n",
    "visualize_weight_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Advanced Module Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom module with learnable parameters\n",
    "class CustomLinear(nn.Module):\n",
    "    \"\"\"Custom linear layer to demonstrate parameter creation\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Create learnable parameters\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            # Register as None so it doesn't appear in parameters()\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Manual implementation of linear transformation\n",
    "        output = torch.matmul(x, self.weight.t())\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        # Custom string representation\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "# Test custom linear layer\n",
    "import math\n",
    "\n",
    "custom_linear = CustomLinear(10, 5)\n",
    "builtin_linear = nn.Linear(10, 5)\n",
    "\n",
    "print(\"Custom linear layer:\")\n",
    "print(custom_linear)\n",
    "print(f\"Parameters: {sum(p.numel() for p in custom_linear.parameters())}\")\n",
    "\n",
    "# Test they produce similar results\n",
    "x = torch.randn(3, 10)\n",
    "output_custom = custom_linear(x)\n",
    "output_builtin = builtin_linear(x)\n",
    "\n",
    "print(f\"\\nOutput shapes - Custom: {output_custom.shape}, Built-in: {output_builtin.shape}\")\n",
    "print(\"Custom and built-in linear layers work equivalently!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module with submodules and parameter sharing\n",
    "class ModularNet(nn.Module):\n",
    "    \"\"\"Demonstrate modular architecture and parameter sharing\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Shared transformation (parameter sharing)\n",
    "        self.shared_transform = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Layer-specific transformations\n",
    "        self.layer_transforms = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization for each layer\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = F.relu(self.input_proj(x))\n",
    "        \n",
    "        # Process through layers\n",
    "        for i in range(self.num_layers):\n",
    "            residual = x\n",
    "            \n",
    "            # Apply shared transformation (parameter sharing across layers)\n",
    "            x = self.shared_transform(x)\n",
    "            \n",
    "            # Apply layer-specific transformation\n",
    "            x = self.layer_transforms[i](x)\n",
    "            \n",
    "            # Residual connection and layer norm\n",
    "            x = self.layer_norms[i](x + residual)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)\n",
    "        return x\n",
    "\n",
    "# Create and analyze modular network\n",
    "modular_net = ModularNet(input_size=64, hidden_size=128, num_layers=4)\n",
    "\n",
    "print(\"Modular Network Architecture:\")\n",
    "print(modular_net)\n",
    "\n",
    "print(\"\\nParameter analysis:\")\n",
    "total_params = 0\n",
    "for name, param in modular_net.named_parameters():\n",
    "    print(f\"{name:30s}: {str(param.shape):20s} {param.numel():>8,d}\")\n",
    "    total_params += param.numel()\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Note: shared_transform parameters are used across all layers\n",
    "print(f\"\\nShared parameters (used {modular_net.num_layers} times): {modular_net.shared_transform.weight.numel() + modular_net.shared_transform.bias.numel():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(5, 64)\n",
    "output = modular_net(x)\n",
    "print(f\"\\nForward pass: {x.shape} -> {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hooks for monitoring activations and gradients\n",
    "class MonitoredNet(nn.Module):\n",
    "    \"\"\"Network with built-in monitoring capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Storage for activations and gradients\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "        \n",
    "        # Register hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        def save_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                self.activations[name] = output.detach()\n",
    "            return hook\n",
    "        \n",
    "        def save_gradient(name):\n",
    "            def hook(module, grad_input, grad_output):\n",
    "                if grad_output[0] is not None:\n",
    "                    self.gradients[name] = grad_output[0].detach()\n",
    "            return hook\n",
    "        \n",
    "        # Register forward and backward hooks\n",
    "        self.fc1.register_forward_hook(save_activation('fc1'))\n",
    "        self.fc2.register_forward_hook(save_activation('fc2'))\n",
    "        self.fc3.register_forward_hook(save_activation('fc3'))\n",
    "        \n",
    "        self.fc1.register_backward_hook(save_gradient('fc1'))\n",
    "        self.fc2.register_backward_hook(save_gradient('fc2'))\n",
    "        self.fc3.register_backward_hook(save_gradient('fc3'))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def get_activation_stats(self):\n",
    "        \"\"\"Get statistics about activations\"\"\"\n",
    "        stats = {}\n",
    "        for name, activation in self.activations.items():\n",
    "            stats[name] = {\n",
    "                'mean': activation.mean().item(),\n",
    "                'std': activation.std().item(),\n",
    "                'min': activation.min().item(),\n",
    "                'max': activation.max().item(),\n",
    "                'zeros': (activation == 0).float().mean().item()  # Sparsity for ReLU\n",
    "            }\n",
    "        return stats\n",
    "    \n",
    "    def get_gradient_stats(self):\n",
    "        \"\"\"Get statistics about gradients\"\"\"\n",
    "        stats = {}\n",
    "        for name, gradient in self.gradients.items():\n",
    "            stats[name] = {\n",
    "                'mean': gradient.mean().item(),\n",
    "                'std': gradient.std().item(),\n",
    "                'norm': gradient.norm().item()\n",
    "            }\n",
    "        return stats\n",
    "\n",
    "# Test monitored network\n",
    "monitored_net = MonitoredNet(20, 50, 5)\n",
    "optimizer = optim.SGD(monitored_net.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward and backward pass\n",
    "x = torch.randn(10, 20)\n",
    "y_true = torch.randn(10, 5)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = monitored_net(x)\n",
    "loss = criterion(y_pred, y_true)\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Analyze activations and gradients\n",
    "print(\"Activation Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "activation_stats = monitored_net.get_activation_stats()\n",
    "for layer, stats in activation_stats.items():\n",
    "    print(f\"{layer}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "    print(f\"  Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "    print(f\"  Sparsity (zeros): {stats['zeros']:.2%}\")\n",
    "\n",
    "print(\"\\nGradient Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "gradient_stats = monitored_net.get_gradient_stats()\n",
    "for layer, stats in gradient_stats.items():\n",
    "    print(f\"{layer}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}\")\n",
    "    print(f\"  Norm: {stats['norm']:.6f}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Module exploration completed!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"â€¢ Use nn.Module as base class for all models\")\n",
    "print(\"â€¢ Parameters are automatically tracked when using nn.Parameter\")\n",
    "print(\"â€¢ Proper initialization is crucial for training success\")\n",
    "print(\"â€¢ ModuleList and ModuleDict help organize complex architectures\")\n",
    "print(\"â€¢ Hooks enable monitoring and debugging of model internals\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}